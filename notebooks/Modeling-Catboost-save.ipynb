{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.model_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import model_selection , metrics   #Additional scklearn functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_strategy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/merged_data/train.pkl')\n",
    "test = pd.read_pickle('../data/merged_data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to try:\n",
    "* optimize aucroc with keras and batch methods\n",
    "* SVM / rank SVM (https://github.com/rdipietro/pyrvm)\n",
    "* try separate models on separate variables + stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features : \n",
    "* NLP on COMMENTAIRE_BI\n",
    "* Extract options from OPTION (pas sur que ce soit util, peut être juste check la qualité)\n",
    "* features from history\n",
    "\n",
    "\n",
    "\n",
    "* check for intervention in test without contracts and to handle them\n",
    "* add combinations of categorical features (see paribas example on catboost site)\n",
    "* select only best features (feature_importance ?)\n",
    "* try to put random nas in train for handling missing contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fails:\n",
    "* one model with selectKbest on modalities vs one model on dimensionality reduction with group rare modalities\n",
    "    * dimensionality reduction don't bring much value here (mb MCA)\n",
    "    * select k best fails with 5k categories, which is not enough to be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['INSTANCE_ID', #460k modalities, not usable as a feature\n",
    "        'INCIDENT_NUMBER']\n",
    "drop_atm = [#'AUTEUR_INCIDENT', # 2088 modalities\n",
    "            'TYPE_VOIE',\n",
    "#            'NATURE_CODE', # 313 modalities, need to be splitted in 5 modalities\n",
    "#            'MARQUE_LIB', # 167 modalities\n",
    "#            'OPTION', # 80 modalities, extract options\n",
    "            #'MODELE_CODE', # 10k modalities\n",
    "#            'COMMENTAIRE_BI', # NLP 400k modalities\n",
    "             #'RESOURCE_ID', # 4033 modalities\n",
    "            'CODE_POSTAL', # 5800 modalities (only get first 2 numbers ?)\n",
    "            'L2_ORGA_CODE_POSTAL', # 147 modalities (might be redondent with L2_ORGANISATION_ID)\n",
    "#            'L2_ORGANISATION_ID' #151 modalities\n",
    "            'L2_ORGA_VILLE', # 146, might be redondent with other organisation variables\n",
    "#            'RACHAT_CODE' # 312 modalities (try binarising ?)         \n",
    "#            'CODE_INSTALLATION' # 17 modalities\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.drop(drop + drop_atm + ['target'], axis=1, inplace=True)\n",
    "train.drop(drop + drop_atm , axis=1, inplace=True)\n",
    "test.drop(drop + drop_atm, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = list(train.columns[train.dtypes == 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### imputation of missing data\n",
    "\n",
    "TODO: try imputing test based on test values, not train <br>\n",
    "TODO: try diffrent strategy on imputing datas from contract since missing are present only in test set<BR>\n",
    "TODO: try creating data with missing contract in train sample and do not fill the missing in test<br>\n",
    "TODO: try imputing specific na value for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_selected_variables(df, test, categ, quanti, dates):\n",
    "    _df = df.copy()\n",
    "    _test = test.copy() if test is not None else None\n",
    "    \n",
    "    replace = _df[categ].mode()\n",
    "    replace_values = {k:v.iloc[0] for k,v in replace.items()}\n",
    "    _df.fillna(replace_values, inplace=True)\n",
    "\n",
    "    replace_quanti = _df[quanti].mean()\n",
    "    _df.fillna(replace_quanti, inplace=True)\n",
    "\n",
    "    _df[dates] = _df[dates].fillna(method='pad')\n",
    "    \n",
    "    if test is not None:\n",
    "        \n",
    "        _test.fillna(replace_values, inplace=True)\n",
    "        _test.fillna(replace_quanti, inplace=True)\n",
    "        _test[dates] = _df[dates].fillna(method='pad')\n",
    "    \n",
    "    return _df, _test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace = train[categoricals].mode()\n",
    "#replace_values = {k:v.iloc[0] for k,v in replace.items()}\n",
    "def impute_contract_variables(df):\n",
    "    _df = df.copy()\n",
    "    \n",
    "    for var in categ_contract:\n",
    "        try:\n",
    "            _df[var] = _df[var].fillna('NAN')\n",
    "        except ValueError as e:\n",
    "            _df[var] = _df[var].cat.add_categories(['NAN'])\n",
    "            _df[var] = _df[var].fillna('NAN') \n",
    "\n",
    "    _df[quanti_contract] = _df[quanti_contract].fillna(-9999)\n",
    "    _df[date_contract] = _df[date_contract].fillna(datetime.datetime(1970, 1, 1))\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = list(train.columns[train.dtypes == 'category'])\n",
    "quantitative = ['NB_PASSAGE', 'POINTS_FIDEL', 'CONTRAT_TARIF', 'PRIX_FACTURE']\n",
    "dates = list(train.columns[train.dtypes == 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_variables = [ 'UPD_DATE', 'DATE_DEBUT', 'DATE_FIN', 'STS_CODE', 'OPTION', 'FORMULE', 'CONTRAT_TARIF', 'PRIX_FACTURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute without contract\n",
    "categ_to_impute = list(set(categoricals) - set(contract_variables))\n",
    "quanti_to_impute = list(set(quantitative) - set(contract_variables))\n",
    "date_to_impute = list(set(dates) - set(contract_variables))\n",
    "\n",
    "#impute contract\n",
    "categ_contract = list(set(categoricals).intersection(set(contract_variables)))\n",
    "quanti_contract = list(set(quantitative).intersection(set(contract_variables)))\n",
    "date_contract = list(set(dates).intersection(set(contract_variables)))\n",
    "\n",
    "#train and test are filled with values taken from train\n",
    "#contract and other variables are imputed together\n",
    "if imput_strategy == 1:\n",
    "    train, test = impute_selected_variables(train, test, categoricals, quantitative, dates)\n",
    "\n",
    "#train and test are filled with values taken on their own values\n",
    "#contract and other variables are imputed together\n",
    "if imput_strategy == 2:\n",
    "    train, _ = impute_selected_variables(train, None, categoricals, quantitative, dates)\n",
    "    test, _ = impute_selected_variables(test, None, categoricals, quantitative, dates)\n",
    "\n",
    "#train and test are filled with values taken from train\n",
    "#contract and other variables are imputed separatly (need to import some NAN in train set)\n",
    "if imput_strategy == 3:\n",
    "    train, test = impute_selected_variables(train, test, categ_to_impute, quanti_to_impute, date_to_impute)\n",
    "    train = impute_contract_variables(train)\n",
    "    test = impute_contract_variables(test)\n",
    "\n",
    "# train and test are filled on their own values\n",
    "# contract and other variables are imputed separatly (need to import some NAN in train set)\n",
    "if imput_strategy == 4:\n",
    "    train, _ = impute_selected_variables(train, None, categ_to_impute, quanti_to_impute, date_to_impute)\n",
    "    test, _ = impute_selected_variables(test, None, categ_to_impute, quanti_to_impute, date_to_impute)\n",
    "    train = impute_contract_variables(train)\n",
    "    test = impute_contract_variables(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature ingineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to adapt\n",
    "def cleanText(s):\n",
    "    s = s.str.lower()                         # Convert to lowercase\n",
    "    s = s.str.replace(r'<.*?>', ' ')          # Remove HTML characters\n",
    "    s = s.str.replace('\\'', '')               # Remove single quotes ' \n",
    "    s = s.str.replace('-', '')                # Remove dashes -\n",
    "    s = s.str.replace(r'[^a-zA-Z]', ' ')      # Remove non alpha characters\n",
    "    s = s.map(lambda s: ' '.join(s.split()))  # Remove extra whitespae\n",
    "    s = s.str.strip()                         # Remove whitespace at start and end\n",
    "#    s = s.apply(lambda x: removeStopwords(x)) # Remove stopwords\n",
    "return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commentaire_bi(df):\n",
    "    _df = df.copy()\n",
    "    \n",
    "    _df.COMMENTAIRE_BI = _df.COMMENTAIRE_BI.str.upper()\n",
    "    COMMENTAIRE_BI_vc = _df.COMMENTAIRE_BI.value_counts()\n",
    "    common_commentaire_bi = COMMENTAIRE_BI_vc[COMMENTAIRE_BI_vc > 100].index\n",
    "    _df['COMMENTAIRE_BI_common'] = _df.COMMENTAIRE_BI.where(_df.COMMENTAIRE_BI.isin(common_commentaire_bi), \"Rare\")\n",
    "    \n",
    "    _df['nb_char_commentaire'] = [len(txt) for txt in _df.COMMENTAIRE_BI]\n",
    "    _df['nb_mots_commentaire'] = [len(txt.split()) for txt in _df.COMMENTAIRE_BI]\n",
    "    _df['has_number_commentaire'] = [any(char.isdigit() for char in txt) for txt in _df.COMMENTAIRE_BI]\n",
    "    _df['is_empty_commentaire'] = [(txt == '.') for txt in _df.COMMENTAIRE_BI]\n",
    "    _df.drop('COMMENTAIRE_BI', axis=1, inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nature_code_split(df):\n",
    "    _df = df.copy()\n",
    "    nature_code_splitted = [nc.split('-') for nc in df.NATURE_CODE]\n",
    "    nature_code_df = pd.DataFrame(nature_code_splitted, columns=['nc_1', 'nc_2', 'nc_3', 'nc_4', 'nc_5'])\n",
    "    nature_code_df.fillna('-1', inplace=True)\n",
    "    for nc_i in ['nc_1', 'nc_2', 'nc_3', 'nc_4', 'nc_5']:\n",
    "        nature_code_df[nc_i] = nature_code_df[nc_i].astype('category')\n",
    "    \n",
    "    #_df.drop('NATURE_CODE', axis=1, inplace=True)\n",
    "    _df = _df.merge(nature_code_df, left_index=True, right_index=True)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: use dt series accessor\n",
    "def add_dates_features(data):\n",
    "    data['age_installation'] = (data['CRE_DATE_GZL'] - data['INSTALL_DATE']).dt.days // 365\n",
    "    data['mois_appel'] = data['CRE_DATE_GZL'].map(lambda x: x.month)\n",
    "    data['joursemaine_appel'] = data['CRE_DATE_GZL'].map(lambda x: x.isoweekday()) #integer, might be considered categorical\n",
    "    data['jour_appel'] = data['CRE_DATE_GZL'].map(lambda x: x.day)\n",
    "    data['mois_intervention'] = data['SCHEDULED_START_DATE'].map(lambda x: x.month)\n",
    "    data['joursemaine_intervention'] = data['SCHEDULED_START_DATE'].map(lambda x: x.isoweekday()) #integer, might be considered categorical\n",
    "    data['jour_intervention'] = data['SCHEDULED_START_DATE'].map(lambda x: x.day)\n",
    "    data['duree_avant_intervention'] = (data['SCHEDULED_START_DATE'] - data['CRE_DATE_GZL']).dt.days\n",
    "    data['duree_prevue'] = (data['SCHEDULED_END_DATE'] - data['SCHEDULED_START_DATE']).dt.days\n",
    "    data['temps_depuis_debut_contrat'] = (data['CRE_DATE_GZL'] - data['DATE_DEBUT']).dt.days\n",
    "    data['temps_jusqua_fin_contrat'] = (data['CRE_DATE_GZL'] - data['DATE_FIN']).dt.days  #souvent nan ? (mettre 0)\n",
    "    data['temps_depuis_maj_contrat'] = (data['CRE_DATE_GZL'] - data['UPD_DATE']).dt.days \n",
    "\n",
    "    data.drop(['CRE_DATE_GZL', 'INSTALL_DATE', 'SCHEDULED_START_DATE', 'SCHEDULED_END_DATE', 'DATE_DEBUT', 'DATE_FIN', 'UPD_DATE'], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mots en particulier\n",
    "#créer un dictionnaire et compter (count et garder les must ? voir la diff entre les cas + et - ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# (history) temps depuis dernière visite (pas forcément dispo sur le test)\n",
    "# (history) déjà eu une casse sur ce matériel\n",
    "# (history) temps depuis dernière casse\n",
    "# (history) la dernière visite date de moins de 6 mois\n",
    "# (history) nb interventions faires par la ressource\n",
    "# (history) temps depuis la première intervention de la ressource\n",
    "# (contract history) nb de fois que le contrat a été mis à jour sur les X dernières années"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_from_file(df, csv_file):\n",
    "    features = pd.read_csv(csv_file)\n",
    "    features['canceled_pred'] = features['canceled_pred'].astype(bool)\n",
    "    df = df.join(features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = commentaire_bi(train)\n",
    "train = nature_code_split(train)\n",
    "train = add_dates_features(train)\n",
    "train = add_features_from_file(train, 'features_canceled_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = commentaire_bi(test)\n",
    "test = nature_code_split(test)\n",
    "test = add_dates_features(test)\n",
    "test = add_features_from_file(test, 'features_canceled_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop variables\n",
    "#drop = ['joursemaine_appel', 'USAGE_LOCAL', 'nc_4', 'is_empty_commentaire', 'duree_prevue', 'nc_1']\n",
    "\n",
    "#train.drop(drop, axis=1,inplace=True)\n",
    "#test.drop(drop, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get variables where test set has modalities which are not in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = list(train.columns[train.dtypes == 'category'])\n",
    "var_with_new_categ = []\n",
    "for cat in categoricals:\n",
    "    if len(set(test[cat]) - set(train[cat])) > 0:\n",
    "        var_with_new_categ.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop categories in test which are not in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categoricals:\n",
    "    train[cat] = train[cat].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoriesDroper(BaseEstimator, TransformerMixin):\n",
    "    '''Drop categories which are not in the train set'''\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.categories_dict = dict()\n",
    "        \n",
    "    def fit(self, df, y=None):\n",
    "        self.categories_dict = {column: df[column].cat.categories for column in self.columns}\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        _df = df.copy()\n",
    "        for column in self.columns:\n",
    "            _df[column] = _df[column].cat.set_categories(self.categories_dict[column])\n",
    "            try:\n",
    "                _df[column] = _df[column].fillna('NAN')\n",
    "            except ValueError as e:\n",
    "                _df[column] = _df[column].cat.add_categories(['NAN'])\n",
    "                _df[column] = _df[column].fillna('NAN') \n",
    "                print(e,'with ', column)\n",
    "        \n",
    "        return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill value must be in categories with  INCIDENT_TYPE_ID\n",
      "fill value must be in categories with  TYPE_BI\n",
      "fill value must be in categories with  MILLESIME\n",
      "fill value must be in categories with  PROBLEM_CODE\n",
      "fill value must be in categories with  AUTEUR_INCIDENT\n",
      "fill value must be in categories with  GRAVITE\n",
      "fill value must be in categories with  RESOURCE_ID\n",
      "fill value must be in categories with  TYPE_OCC\n",
      "fill value must be in categories with  RACHAT_CODE\n",
      "fill value must be in categories with  NATURE_CODE\n",
      "fill value must be in categories with  MARQUE_LIB\n",
      "fill value must be in categories with  MODELE_CODE\n",
      "fill value must be in categories with  PAYS\n",
      "fill value must be in categories with  STOP_PHONING\n",
      "fill value must be in categories with  CODE_GEN_EQUIPEMENT\n",
      "fill value must be in categories with  CODE_FONCTION\n",
      "fill value must be in categories with  CODE_ENERGIE\n",
      "fill value must be in categories with  CODE_INSTALLATION\n",
      "fill value must be in categories with  CODE_SPECIFICATION\n",
      "fill value must be in categories with  CODE_EAU_CHAUDE\n",
      "fill value must be in categories with  L1_ORGANISATION_ID\n",
      "fill value must be in categories with  L2_ORGANISATION_ID\n",
      "fill value must be in categories with  STS_CODE\n",
      "fill value must be in categories with  FORMULE\n",
      "fill value must be in categories with  OPTION\n",
      "fill value must be in categories with  nc_2\n",
      "fill value must be in categories with  nc_3\n",
      "fill value must be in categories with  nc_5\n"
     ]
    }
   ],
   "source": [
    "cd = CategoriesDroper(categoricals)\n",
    "cd.fit(train)\n",
    "test = cd.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in categoricals:\n",
    "#    train[item] = train[item].cat.codes +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train in train, cv (will be replaced by cross validation for parameters tuning)\n",
    "# stratify ?\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = sklearn.model_selection.train_test_split(train, y_train, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio =  sum(y_train==0) / sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinations_ctr for list of variables ?\n",
    "model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "#    one_hot_max_size=3,\n",
    "    learning_rate=0.16,\n",
    "#    depth=8,\n",
    "    eval_metric=\"AUC\",\n",
    "    random_seed=42,\n",
    "    od_type='Iter',\n",
    "    od_wait=40,\n",
    "    use_best_model=True\n",
    "#    scale_pos_weight=pos_neg_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [X_train_train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7048973\tbest: 0.7048973 (0)\ttotal: 3.18s\tremaining: 10m 32s\n",
      "1:\ttest: 0.7106869\tbest: 0.7106869 (1)\ttotal: 6.6s\tremaining: 10m 53s\n",
      "2:\ttest: 0.7126851\tbest: 0.7126851 (2)\ttotal: 9.72s\tremaining: 10m 38s\n",
      "3:\ttest: 0.7140945\tbest: 0.7140945 (3)\ttotal: 12.9s\tremaining: 10m 32s\n",
      "4:\ttest: 0.7145079\tbest: 0.7145079 (4)\ttotal: 16s\tremaining: 10m 25s\n",
      "5:\ttest: 0.7151936\tbest: 0.7151936 (5)\ttotal: 19.5s\tremaining: 10m 29s\n",
      "6:\ttest: 0.7163818\tbest: 0.7163818 (6)\ttotal: 22.5s\tremaining: 10m 19s\n",
      "7:\ttest: 0.7168471\tbest: 0.7168471 (7)\ttotal: 25.6s\tremaining: 10m 14s\n",
      "8:\ttest: 0.7173209\tbest: 0.7173209 (8)\ttotal: 28.7s\tremaining: 10m 9s\n",
      "9:\ttest: 0.7178285\tbest: 0.7178285 (9)\ttotal: 32.1s\tremaining: 10m 9s\n",
      "10:\ttest: 0.7188165\tbest: 0.7188165 (10)\ttotal: 35s\tremaining: 10m 1s\n",
      "11:\ttest: 0.7194177\tbest: 0.7194177 (11)\ttotal: 38.5s\tremaining: 10m 3s\n",
      "12:\ttest: 0.7198280\tbest: 0.7198280 (12)\ttotal: 42.1s\tremaining: 10m 5s\n",
      "13:\ttest: 0.7201591\tbest: 0.7201591 (13)\ttotal: 45.8s\tremaining: 10m 8s\n",
      "14:\ttest: 0.7205350\tbest: 0.7205350 (14)\ttotal: 49.4s\tremaining: 10m 9s\n",
      "15:\ttest: 0.7210099\tbest: 0.7210099 (15)\ttotal: 52.9s\tremaining: 10m 8s\n",
      "16:\ttest: 0.7213561\tbest: 0.7213561 (16)\ttotal: 56.7s\tremaining: 10m 10s\n",
      "17:\ttest: 0.7216439\tbest: 0.7216439 (17)\ttotal: 1m\tremaining: 10m 7s\n",
      "18:\ttest: 0.7229679\tbest: 0.7229679 (18)\ttotal: 1m 3s\tremaining: 10m 4s\n",
      "19:\ttest: 0.7231603\tbest: 0.7231603 (19)\ttotal: 1m 6s\tremaining: 10m 1s\n",
      "20:\ttest: 0.7243806\tbest: 0.7243806 (20)\ttotal: 1m 10s\tremaining: 10m 3s\n",
      "21:\ttest: 0.7251438\tbest: 0.7251438 (21)\ttotal: 1m 15s\tremaining: 10m 7s\n",
      "22:\ttest: 0.7252733\tbest: 0.7252733 (22)\ttotal: 1m 18s\tremaining: 10m 5s\n",
      "23:\ttest: 0.7253871\tbest: 0.7253871 (23)\ttotal: 1m 22s\tremaining: 10m 6s\n",
      "24:\ttest: 0.7263296\tbest: 0.7263296 (24)\ttotal: 1m 27s\tremaining: 10m 11s\n",
      "25:\ttest: 0.7264971\tbest: 0.7264971 (25)\ttotal: 1m 31s\tremaining: 10m 11s\n",
      "26:\ttest: 0.7267120\tbest: 0.7267120 (26)\ttotal: 1m 36s\tremaining: 10m 16s\n",
      "27:\ttest: 0.7267544\tbest: 0.7267544 (27)\ttotal: 1m 40s\tremaining: 10m 18s\n",
      "28:\ttest: 0.7279043\tbest: 0.7279043 (28)\ttotal: 1m 44s\tremaining: 10m 16s\n",
      "29:\ttest: 0.7280350\tbest: 0.7280350 (29)\ttotal: 1m 48s\tremaining: 10m 13s\n",
      "30:\ttest: 0.7281309\tbest: 0.7281309 (30)\ttotal: 1m 52s\tremaining: 10m 14s\n",
      "31:\ttest: 0.7289779\tbest: 0.7289779 (31)\ttotal: 1m 56s\tremaining: 10m 9s\n",
      "32:\ttest: 0.7296394\tbest: 0.7296394 (32)\ttotal: 1m 59s\tremaining: 10m 6s\n",
      "33:\ttest: 0.7297289\tbest: 0.7297289 (33)\ttotal: 2m 3s\tremaining: 10m 5s\n",
      "34:\ttest: 0.7298423\tbest: 0.7298423 (34)\ttotal: 2m 8s\tremaining: 10m 4s\n",
      "35:\ttest: 0.7299195\tbest: 0.7299195 (35)\ttotal: 2m 12s\tremaining: 10m 1s\n",
      "36:\ttest: 0.7300280\tbest: 0.7300280 (36)\ttotal: 2m 16s\tremaining: 9m 59s\n",
      "37:\ttest: 0.7301272\tbest: 0.7301272 (37)\ttotal: 2m 19s\tremaining: 9m 55s\n",
      "38:\ttest: 0.7302178\tbest: 0.7302178 (38)\ttotal: 2m 23s\tremaining: 9m 52s\n",
      "39:\ttest: 0.7302493\tbest: 0.7302493 (39)\ttotal: 2m 27s\tremaining: 9m 48s\n",
      "40:\ttest: 0.7303224\tbest: 0.7303224 (40)\ttotal: 2m 30s\tremaining: 9m 44s\n",
      "41:\ttest: 0.7306904\tbest: 0.7306904 (41)\ttotal: 2m 34s\tremaining: 9m 39s\n",
      "42:\ttest: 0.7307363\tbest: 0.7307363 (42)\ttotal: 2m 37s\tremaining: 9m 34s\n",
      "43:\ttest: 0.7312138\tbest: 0.7312138 (43)\ttotal: 2m 41s\tremaining: 9m 31s\n",
      "44:\ttest: 0.7312372\tbest: 0.7312372 (44)\ttotal: 2m 44s\tremaining: 9m 26s\n",
      "45:\ttest: 0.7312796\tbest: 0.7312796 (45)\ttotal: 2m 47s\tremaining: 9m 21s\n",
      "46:\ttest: 0.7314591\tbest: 0.7314591 (46)\ttotal: 2m 51s\tremaining: 9m 17s\n",
      "47:\ttest: 0.7314941\tbest: 0.7314941 (47)\ttotal: 2m 54s\tremaining: 9m 13s\n",
      "48:\ttest: 0.7315597\tbest: 0.7315597 (48)\ttotal: 2m 58s\tremaining: 9m 8s\n",
      "49:\ttest: 0.7315900\tbest: 0.7315900 (49)\ttotal: 3m 1s\tremaining: 9m 4s\n",
      "50:\ttest: 0.7320804\tbest: 0.7320804 (50)\ttotal: 3m 5s\tremaining: 9m 1s\n",
      "51:\ttest: 0.7321136\tbest: 0.7321136 (51)\ttotal: 3m 8s\tremaining: 8m 57s\n",
      "52:\ttest: 0.7321398\tbest: 0.7321398 (52)\ttotal: 3m 12s\tremaining: 8m 53s\n",
      "53:\ttest: 0.7321686\tbest: 0.7321686 (53)\ttotal: 3m 16s\tremaining: 8m 50s\n",
      "54:\ttest: 0.7322117\tbest: 0.7322117 (54)\ttotal: 3m 19s\tremaining: 8m 45s\n",
      "55:\ttest: 0.7322422\tbest: 0.7322422 (55)\ttotal: 3m 22s\tremaining: 8m 41s\n",
      "56:\ttest: 0.7322938\tbest: 0.7322938 (56)\ttotal: 3m 26s\tremaining: 8m 38s\n",
      "57:\ttest: 0.7323198\tbest: 0.7323198 (57)\ttotal: 3m 29s\tremaining: 8m 33s\n",
      "58:\ttest: 0.7323486\tbest: 0.7323486 (58)\ttotal: 3m 33s\tremaining: 8m 29s\n",
      "59:\ttest: 0.7323665\tbest: 0.7323665 (59)\ttotal: 3m 36s\tremaining: 8m 25s\n",
      "60:\ttest: 0.7324088\tbest: 0.7324088 (60)\ttotal: 3m 40s\tremaining: 8m 21s\n",
      "61:\ttest: 0.7324464\tbest: 0.7324464 (61)\ttotal: 3m 43s\tremaining: 8m 16s\n",
      "62:\ttest: 0.7324816\tbest: 0.7324816 (62)\ttotal: 3m 46s\tremaining: 8m 12s\n",
      "63:\ttest: 0.7324941\tbest: 0.7324941 (63)\ttotal: 3m 49s\tremaining: 8m 7s\n",
      "64:\ttest: 0.7324963\tbest: 0.7324963 (64)\ttotal: 3m 52s\tremaining: 8m 3s\n",
      "65:\ttest: 0.7328445\tbest: 0.7328445 (65)\ttotal: 3m 57s\tremaining: 8m 1s\n",
      "66:\ttest: 0.7328763\tbest: 0.7328763 (66)\ttotal: 4m\tremaining: 7m 57s\n",
      "67:\ttest: 0.7332138\tbest: 0.7332138 (67)\ttotal: 4m 4s\tremaining: 7m 54s\n",
      "68:\ttest: 0.7332358\tbest: 0.7332358 (68)\ttotal: 4m 7s\tremaining: 7m 50s\n",
      "69:\ttest: 0.7332782\tbest: 0.7332782 (69)\ttotal: 4m 11s\tremaining: 7m 47s\n",
      "70:\ttest: 0.7332927\tbest: 0.7332927 (70)\ttotal: 4m 14s\tremaining: 7m 43s\n",
      "71:\ttest: 0.7333441\tbest: 0.7333441 (71)\ttotal: 4m 18s\tremaining: 7m 39s\n",
      "72:\ttest: 0.7333666\tbest: 0.7333666 (72)\ttotal: 4m 22s\tremaining: 7m 36s\n",
      "73:\ttest: 0.7335151\tbest: 0.7335151 (73)\ttotal: 4m 26s\tremaining: 7m 34s\n",
      "74:\ttest: 0.7335214\tbest: 0.7335214 (74)\ttotal: 4m 30s\tremaining: 7m 31s\n",
      "75:\ttest: 0.7335349\tbest: 0.7335349 (75)\ttotal: 4m 39s\tremaining: 7m 35s\n",
      "76:\ttest: 0.7335522\tbest: 0.7335522 (76)\ttotal: 4m 50s\tremaining: 7m 43s\n",
      "77:\ttest: 0.7337642\tbest: 0.7337642 (77)\ttotal: 4m 57s\tremaining: 7m 44s\n",
      "78:\ttest: 0.7337803\tbest: 0.7337803 (78)\ttotal: 5m 1s\tremaining: 7m 41s\n",
      "79:\ttest: 0.7338501\tbest: 0.7338501 (79)\ttotal: 5m 5s\tremaining: 7m 38s\n",
      "80:\ttest: 0.7338670\tbest: 0.7338670 (80)\ttotal: 5m 9s\tremaining: 7m 34s\n",
      "81:\ttest: 0.7338856\tbest: 0.7338856 (81)\ttotal: 5m 13s\tremaining: 7m 30s\n",
      "82:\ttest: 0.7339172\tbest: 0.7339172 (82)\ttotal: 5m 17s\tremaining: 7m 27s\n",
      "83:\ttest: 0.7339332\tbest: 0.7339332 (83)\ttotal: 5m 21s\tremaining: 7m 24s\n",
      "84:\ttest: 0.7339922\tbest: 0.7339922 (84)\ttotal: 5m 26s\tremaining: 7m 21s\n",
      "85:\ttest: 0.7340424\tbest: 0.7340424 (85)\ttotal: 5m 30s\tremaining: 7m 18s\n",
      "86:\ttest: 0.7340753\tbest: 0.7340753 (86)\ttotal: 5m 35s\tremaining: 7m 15s\n",
      "87:\ttest: 0.7341171\tbest: 0.7341171 (87)\ttotal: 5m 40s\tremaining: 7m 13s\n",
      "88:\ttest: 0.7341756\tbest: 0.7341756 (88)\ttotal: 5m 45s\tremaining: 7m 11s\n",
      "89:\ttest: 0.7341760\tbest: 0.7341760 (89)\ttotal: 5m 49s\tremaining: 7m 7s\n",
      "90:\ttest: 0.7342146\tbest: 0.7342146 (90)\ttotal: 5m 54s\tremaining: 7m 4s\n",
      "91:\ttest: 0.7342255\tbest: 0.7342255 (91)\ttotal: 5m 58s\tremaining: 7m 1s\n",
      "92:\ttest: 0.7342684\tbest: 0.7342684 (92)\ttotal: 6m 3s\tremaining: 6m 58s\n",
      "93:\ttest: 0.7342879\tbest: 0.7342879 (93)\ttotal: 6m 8s\tremaining: 6m 55s\n",
      "94:\ttest: 0.7343172\tbest: 0.7343172 (94)\ttotal: 6m 12s\tremaining: 6m 52s\n",
      "95:\ttest: 0.7343247\tbest: 0.7343247 (95)\ttotal: 6m 18s\tremaining: 6m 49s\n",
      "96:\ttest: 0.7343980\tbest: 0.7343980 (96)\ttotal: 6m 23s\tremaining: 6m 46s\n",
      "97:\ttest: 0.7344093\tbest: 0.7344093 (97)\ttotal: 6m 27s\tremaining: 6m 43s\n",
      "98:\ttest: 0.7344317\tbest: 0.7344317 (98)\ttotal: 6m 32s\tremaining: 6m 40s\n",
      "99:\ttest: 0.7344497\tbest: 0.7344497 (99)\ttotal: 6m 36s\tremaining: 6m 36s\n",
      "100:\ttest: 0.7345899\tbest: 0.7345899 (100)\ttotal: 6m 41s\tremaining: 6m 33s\n",
      "101:\ttest: 0.7346010\tbest: 0.7346010 (101)\ttotal: 6m 46s\tremaining: 6m 30s\n",
      "102:\ttest: 0.7346152\tbest: 0.7346152 (102)\ttotal: 6m 51s\tremaining: 6m 27s\n",
      "103:\ttest: 0.7346371\tbest: 0.7346371 (103)\ttotal: 6m 55s\tremaining: 6m 23s\n",
      "104:\ttest: 0.7346397\tbest: 0.7346397 (104)\ttotal: 7m\tremaining: 6m 20s\n",
      "105:\ttest: 0.7346593\tbest: 0.7346593 (105)\ttotal: 7m 5s\tremaining: 6m 17s\n",
      "106:\ttest: 0.7346956\tbest: 0.7346956 (106)\ttotal: 7m 10s\tremaining: 6m 13s\n",
      "107:\ttest: 0.7347315\tbest: 0.7347315 (107)\ttotal: 7m 14s\tremaining: 6m 10s\n",
      "108:\ttest: 0.7347597\tbest: 0.7347597 (108)\ttotal: 7m 18s\tremaining: 6m 6s\n",
      "109:\ttest: 0.7347814\tbest: 0.7347814 (109)\ttotal: 7m 22s\tremaining: 6m 2s\n",
      "110:\ttest: 0.7347862\tbest: 0.7347862 (110)\ttotal: 7m 27s\tremaining: 5m 58s\n",
      "111:\ttest: 0.7348204\tbest: 0.7348204 (111)\ttotal: 7m 31s\tremaining: 5m 55s\n",
      "112:\ttest: 0.7348348\tbest: 0.7348348 (112)\ttotal: 7m 36s\tremaining: 5m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113:\ttest: 0.7348478\tbest: 0.7348478 (113)\ttotal: 7m 42s\tremaining: 5m 48s\n",
      "114:\ttest: 0.7348490\tbest: 0.7348490 (114)\ttotal: 7m 45s\tremaining: 5m 44s\n",
      "115:\ttest: 0.7348533\tbest: 0.7348533 (115)\ttotal: 7m 49s\tremaining: 5m 40s\n",
      "116:\ttest: 0.7348791\tbest: 0.7348791 (116)\ttotal: 7m 54s\tremaining: 5m 36s\n",
      "117:\ttest: 0.7349988\tbest: 0.7349988 (117)\ttotal: 7m 58s\tremaining: 5m 32s\n",
      "118:\ttest: 0.7350095\tbest: 0.7350095 (118)\ttotal: 8m 3s\tremaining: 5m 28s\n",
      "119:\ttest: 0.7350209\tbest: 0.7350209 (119)\ttotal: 8m 7s\tremaining: 5m 25s\n",
      "120:\ttest: 0.7350309\tbest: 0.7350309 (120)\ttotal: 8m 11s\tremaining: 5m 21s\n",
      "121:\ttest: 0.7350452\tbest: 0.7350452 (121)\ttotal: 8m 16s\tremaining: 5m 17s\n",
      "122:\ttest: 0.7350449\tbest: 0.7350452 (121)\ttotal: 8m 20s\tremaining: 5m 13s\n",
      "123:\ttest: 0.7350537\tbest: 0.7350537 (123)\ttotal: 8m 25s\tremaining: 5m 9s\n",
      "124:\ttest: 0.7350681\tbest: 0.7350681 (124)\ttotal: 8m 29s\tremaining: 5m 5s\n",
      "125:\ttest: 0.7350637\tbest: 0.7350681 (124)\ttotal: 8m 33s\tremaining: 5m 1s\n",
      "126:\ttest: 0.7350723\tbest: 0.7350723 (126)\ttotal: 8m 38s\tremaining: 4m 57s\n",
      "127:\ttest: 0.7350916\tbest: 0.7350916 (127)\ttotal: 8m 42s\tremaining: 4m 54s\n",
      "128:\ttest: 0.7351006\tbest: 0.7351006 (128)\ttotal: 8m 48s\tremaining: 4m 50s\n",
      "129:\ttest: 0.7351381\tbest: 0.7351381 (129)\ttotal: 8m 52s\tremaining: 4m 46s\n",
      "130:\ttest: 0.7351648\tbest: 0.7351648 (130)\ttotal: 8m 56s\tremaining: 4m 42s\n",
      "131:\ttest: 0.7351943\tbest: 0.7351943 (131)\ttotal: 9m\tremaining: 4m 38s\n",
      "132:\ttest: 0.7351972\tbest: 0.7351972 (132)\ttotal: 9m 5s\tremaining: 4m 34s\n",
      "133:\ttest: 0.7352246\tbest: 0.7352246 (133)\ttotal: 9m 9s\tremaining: 4m 30s\n",
      "134:\ttest: 0.7352418\tbest: 0.7352418 (134)\ttotal: 9m 13s\tremaining: 4m 26s\n",
      "135:\ttest: 0.7352407\tbest: 0.7352418 (134)\ttotal: 9m 17s\tremaining: 4m 22s\n",
      "136:\ttest: 0.7352451\tbest: 0.7352451 (136)\ttotal: 9m 22s\tremaining: 4m 18s\n",
      "137:\ttest: 0.7352402\tbest: 0.7352451 (136)\ttotal: 9m 26s\tremaining: 4m 14s\n",
      "138:\ttest: 0.7352734\tbest: 0.7352734 (138)\ttotal: 9m 30s\tremaining: 4m 10s\n",
      "139:\ttest: 0.7352822\tbest: 0.7352822 (139)\ttotal: 9m 35s\tremaining: 4m 6s\n",
      "140:\ttest: 0.7352827\tbest: 0.7352827 (140)\ttotal: 9m 39s\tremaining: 4m 2s\n",
      "141:\ttest: 0.7353820\tbest: 0.7353820 (141)\ttotal: 9m 43s\tremaining: 3m 58s\n",
      "142:\ttest: 0.7354012\tbest: 0.7354012 (142)\ttotal: 9m 47s\tremaining: 3m 54s\n",
      "143:\ttest: 0.7354389\tbest: 0.7354389 (143)\ttotal: 9m 52s\tremaining: 3m 50s\n",
      "144:\ttest: 0.7354414\tbest: 0.7354414 (144)\ttotal: 9m 56s\tremaining: 3m 46s\n",
      "145:\ttest: 0.7354469\tbest: 0.7354469 (145)\ttotal: 10m 1s\tremaining: 3m 42s\n",
      "146:\ttest: 0.7354527\tbest: 0.7354527 (146)\ttotal: 10m 6s\tremaining: 3m 38s\n",
      "147:\ttest: 0.7354696\tbest: 0.7354696 (147)\ttotal: 10m 10s\tremaining: 3m 34s\n",
      "148:\ttest: 0.7354676\tbest: 0.7354696 (147)\ttotal: 10m 14s\tremaining: 3m 30s\n",
      "149:\ttest: 0.7354816\tbest: 0.7354816 (149)\ttotal: 10m 18s\tremaining: 3m 26s\n",
      "150:\ttest: 0.7354957\tbest: 0.7354957 (150)\ttotal: 10m 23s\tremaining: 3m 22s\n",
      "151:\ttest: 0.7355094\tbest: 0.7355094 (151)\ttotal: 10m 27s\tremaining: 3m 18s\n",
      "152:\ttest: 0.7355058\tbest: 0.7355094 (151)\ttotal: 10m 31s\tremaining: 3m 13s\n",
      "153:\ttest: 0.7355083\tbest: 0.7355094 (151)\ttotal: 10m 35s\tremaining: 3m 9s\n",
      "154:\ttest: 0.7355085\tbest: 0.7355094 (151)\ttotal: 10m 40s\tremaining: 3m 5s\n",
      "155:\ttest: 0.7355226\tbest: 0.7355226 (155)\ttotal: 10m 44s\tremaining: 3m 1s\n",
      "156:\ttest: 0.7355344\tbest: 0.7355344 (156)\ttotal: 10m 49s\tremaining: 2m 57s\n",
      "157:\ttest: 0.7355376\tbest: 0.7355376 (157)\ttotal: 10m 53s\tremaining: 2m 53s\n",
      "158:\ttest: 0.7357733\tbest: 0.7357733 (158)\ttotal: 10m 58s\tremaining: 2m 49s\n",
      "159:\ttest: 0.7357728\tbest: 0.7357733 (158)\ttotal: 11m 2s\tremaining: 2m 45s\n",
      "160:\ttest: 0.7357718\tbest: 0.7357733 (158)\ttotal: 11m 7s\tremaining: 2m 41s\n",
      "161:\ttest: 0.7357724\tbest: 0.7357733 (158)\ttotal: 11m 12s\tremaining: 2m 37s\n",
      "162:\ttest: 0.7357853\tbest: 0.7357853 (162)\ttotal: 11m 17s\tremaining: 2m 33s\n",
      "163:\ttest: 0.7357921\tbest: 0.7357921 (163)\ttotal: 11m 21s\tremaining: 2m 29s\n",
      "164:\ttest: 0.7358101\tbest: 0.7358101 (164)\ttotal: 11m 26s\tremaining: 2m 25s\n",
      "165:\ttest: 0.7358141\tbest: 0.7358141 (165)\ttotal: 11m 30s\tremaining: 2m 21s\n",
      "166:\ttest: 0.7358409\tbest: 0.7358409 (166)\ttotal: 11m 35s\tremaining: 2m 17s\n",
      "167:\ttest: 0.7358473\tbest: 0.7358473 (167)\ttotal: 11m 39s\tremaining: 2m 13s\n",
      "168:\ttest: 0.7358620\tbest: 0.7358620 (168)\ttotal: 11m 43s\tremaining: 2m 9s\n",
      "169:\ttest: 0.7358620\tbest: 0.7358620 (169)\ttotal: 11m 48s\tremaining: 2m 5s\n",
      "170:\ttest: 0.7358600\tbest: 0.7358620 (169)\ttotal: 11m 53s\tremaining: 2m\n",
      "171:\ttest: 0.7358676\tbest: 0.7358676 (171)\ttotal: 11m 57s\tremaining: 1m 56s\n",
      "172:\ttest: 0.7358781\tbest: 0.7358781 (172)\ttotal: 12m 2s\tremaining: 1m 52s\n",
      "173:\ttest: 0.7361723\tbest: 0.7361723 (173)\ttotal: 12m 6s\tremaining: 1m 48s\n",
      "174:\ttest: 0.7361740\tbest: 0.7361740 (174)\ttotal: 12m 12s\tremaining: 1m 44s\n",
      "175:\ttest: 0.7361739\tbest: 0.7361740 (174)\ttotal: 12m 17s\tremaining: 1m 40s\n",
      "176:\ttest: 0.7362998\tbest: 0.7362998 (176)\ttotal: 12m 21s\tremaining: 1m 36s\n",
      "177:\ttest: 0.7363168\tbest: 0.7363168 (177)\ttotal: 12m 25s\tremaining: 1m 32s\n",
      "178:\ttest: 0.7363293\tbest: 0.7363293 (178)\ttotal: 12m 30s\tremaining: 1m 28s\n",
      "179:\ttest: 0.7363422\tbest: 0.7363422 (179)\ttotal: 12m 34s\tremaining: 1m 23s\n",
      "180:\ttest: 0.7363454\tbest: 0.7363454 (180)\ttotal: 12m 39s\tremaining: 1m 19s\n",
      "181:\ttest: 0.7363536\tbest: 0.7363536 (181)\ttotal: 12m 43s\tremaining: 1m 15s\n",
      "182:\ttest: 0.7363615\tbest: 0.7363615 (182)\ttotal: 12m 47s\tremaining: 1m 11s\n",
      "183:\ttest: 0.7363582\tbest: 0.7363615 (182)\ttotal: 12m 51s\tremaining: 1m 7s\n",
      "184:\ttest: 0.7363626\tbest: 0.7363626 (184)\ttotal: 12m 55s\tremaining: 1m 2s\n",
      "185:\ttest: 0.7363607\tbest: 0.7363626 (184)\ttotal: 13m\tremaining: 58.7s\n",
      "186:\ttest: 0.7363733\tbest: 0.7363733 (186)\ttotal: 13m 5s\tremaining: 54.6s\n",
      "187:\ttest: 0.7363728\tbest: 0.7363733 (186)\ttotal: 13m 9s\tremaining: 50.4s\n",
      "188:\ttest: 0.7363971\tbest: 0.7363971 (188)\ttotal: 13m 15s\tremaining: 46.3s\n",
      "189:\ttest: 0.7364046\tbest: 0.7364046 (189)\ttotal: 13m 19s\tremaining: 42.1s\n",
      "190:\ttest: 0.7364095\tbest: 0.7364095 (190)\ttotal: 13m 24s\tremaining: 37.9s\n",
      "191:\ttest: 0.7364167\tbest: 0.7364167 (191)\ttotal: 13m 29s\tremaining: 33.7s\n",
      "192:\ttest: 0.7364290\tbest: 0.7364290 (192)\ttotal: 13m 34s\tremaining: 29.5s\n",
      "193:\ttest: 0.7364400\tbest: 0.7364400 (193)\ttotal: 13m 39s\tremaining: 25.3s\n",
      "194:\ttest: 0.7364538\tbest: 0.7364538 (194)\ttotal: 13m 44s\tremaining: 21.1s\n",
      "195:\ttest: 0.7364629\tbest: 0.7364629 (195)\ttotal: 13m 49s\tremaining: 16.9s\n",
      "196:\ttest: 0.7364666\tbest: 0.7364666 (196)\ttotal: 13m 53s\tremaining: 12.7s\n",
      "197:\ttest: 0.7364737\tbest: 0.7364737 (197)\ttotal: 13m 58s\tremaining: 8.47s\n",
      "198:\ttest: 0.7364727\tbest: 0.7364737 (197)\ttotal: 14m 3s\tremaining: 4.24s\n",
      "199:\ttest: 0.7364833\tbest: 0.7364833 (199)\ttotal: 14m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7364833229\n",
      "bestIteration = 199\n",
      "\n",
      "Shrink model to first 200 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f8376bcc9b0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_train, y_train_train,\n",
    "    cat_features=categorical_features_indices,\n",
    "    eval_set=(X_train_val, y_train_val),\n",
    "    logging_level='Verbose'  # you can uncomment this for text output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "199:\ttest: 0.7686368\tbest: 0.7686409 (198)\ttotal: 14m 41s\tremaining: 0us (nlp, lr=0.31)\n",
    "199:\ttest: 0.7686109\tbest: 0.7686109 (199)\ttotal: 14m 17s\tremaining: 0us (nlp, lr=0.31, 9 variables dropped)\n",
    "199:\ttest: 0.7729298\tbest: 0.7730353 (190)\ttotal: 22m 13s\tremaining: 0us (nlp, lr=0.31, 6 variables dropped, depth=8)\n",
    "199:\ttest: 0.7576144\tbest: 0.7576257 (197)\ttotal: 15m 42s\tremaining: 0us(nlp, lr=0.31, weighted, one_hot=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances=model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(model.feature_importances_)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(train.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, train.columns[indices[f]], model.feature_importances_[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(train.shape[1]), indices)\n",
    "plt.xlim([-1, train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what to do with cat features\n",
    "params = {'depth': [4, 7, 10],\n",
    "          'learning_rate' : [0.20, 0.31, 0.40],\n",
    "         'l2_leaf_reg': [1, 4, 9],\n",
    "         'iterations': [300]}\n",
    "cb = CatBoostClassifier()\n",
    "cb_model = GridSearchCV(cb, params, scoring=\"roc_auc\")\n",
    "cb_model.fit(train, y_train, cb__cat_features=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_objective(params):\n",
    "    model = CatBoostClassifier(\n",
    "        l2_leaf_reg=int(params['l2_leaf_reg']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        iterations=int(params['iterations']),\n",
    "        border_count=int(params['border_count']),\n",
    "        eval_metric='AUC',\n",
    "        random_seed=42,\n",
    "        logging_level='Silent'\n",
    "        #od_type='Iter',\n",
    "        #od_wait=40\n",
    "    )\n",
    "    \n",
    "    cv_data = cv(\n",
    "        Pool(train, y_train, cat_features=categorical_features_indices),\n",
    "        model.get_params(),\n",
    "        stratified=True,\n",
    "        fold_count=5\n",
    "    )\n",
    "    best_AUC = np.max(cv_data['test-AUC-mean'])\n",
    "    print(params, 'best_AUC: ', best_AUC)\n",
    "    \n",
    "    return 1 - best_AUC # as hyperopt minimises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param space example for xgboost\n",
    "#    space = {\n",
    "#             'n_estimators' : hp.quniform('n_estimators', 100, 1000, 1),\n",
    "#             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "#             'max_depth' : hp.quniform('max_depth', 1, 13, 1),\n",
    "#             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "#             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "#             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "#             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "#             'num_class' : 9,\n",
    "#             'eval_metric': 'mlogloss',\n",
    "#             'objective': 'multi:softprob',\n",
    "#             'nthread' : 6,\n",
    "#             'silent' : 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "params_space = {\n",
    "    #'l2_leaf_reg': hyperopt.hp.loguniform('l2_leaf_reg', -1, np.log(50)),\n",
    "    'l2_leaf_reg': hyperopt.hp.choice('l2_leaf_reg', [11.47871028241772]),\n",
    "    'learning_rate': hyperopt.hp.uniform('learning_rate', 1e-1, 8e-1),\n",
    "     #'learning_rate': hyperopt.hp.choice('learning_rate', [0.31]),\n",
    "    'iterations': hyperopt.hp.quniform('iterations', 250, 1000, 1),\n",
    "    #'iterations': hyperopt.hp.choice('iterations', [250]),\n",
    "    #'depth': hyperopt.hp.quniform('depth', 3, 9, 1),\n",
    "    'depth': hyperopt.hp.choice('depth', [7]),\n",
    "#    'ctr_border_count': hyperopt.hp.quniform('ctr_border_count', 32, 255, 1),\n",
    "    #'border_count': hyperopt.hp.quniform('border_count', 16, 255, 1)\n",
    "    'border_count': hyperopt.hp.choice('border_count', [213])\n",
    "}\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(\n",
    "    hyperopt_objective,\n",
    "    space=params_space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=8,\n",
    "    trials=trials,\n",
    "    rstate=RandomState(123)\n",
    "    \n",
    ")\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#round 1\n",
    "{'depth': 7.0, 'l2_leaf_reg': 6.687638927479829} best_AUC:  0.764339203919555\n",
    "{'depth': 8.0, 'l2_leaf_reg': 2.238434810070511} best_AUC:  0.7641940842160905\n",
    "{'depth': 9.0, 'l2_leaf_reg': 15.61615505112418} best_AUC:  0.7656179177534072\n",
    "{'depth': 7.0, 'l2_leaf_reg': 11.47871028241772} best_AUC:  0.766724797438951\n",
    "{'depth': 6.0, 'l2_leaf_reg': 2.0946141383866816} best_AUC:  0.7641940842160905\n",
    "{'depth': 5.0, 'l2_leaf_reg': 3.409700409045487} best_AUC:  0.7656396435014416\n",
    "{'l2_leaf_reg': 11.47871028241772, 'learning_rate': 7.0}\n",
    "\n",
    "#round 2\n",
    "{'border_count': 199.0, 'depth': 7, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7654693707749893\n",
    "{'border_count': 213.0, 'depth': 7, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7662068818039395\n",
    "{'border_count': 105.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7639026943327258\n",
    "{'border_count': 238.0, 'depth': 7, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.764574925126635\n",
    "{'border_count': 22.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7643064612539859\n",
    "{'border_count': 103.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7632638136706521\n",
    "{'border_count': 225.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7653002122083675\n",
    "{'border_count': 212.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7649391813119792\n",
    "{'border_count': 85.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7649657537494025\n",
    "{'border_count': 251.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7649243873489558\n",
    "{'border_count': 213.0, 'depth': 0, 'iterations': 0, 'l2_leaf_reg': 0, 'learning_rate': 0}\n",
    "\n",
    "#round 3\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 693.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.6096210638782021} best_AUC:  0.767539056284941\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 526.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.701085638831484} best_AUC:  0.767228088670764\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 822.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.7947626751796161} best_AUC:  0.7647414699770261\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 775.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.571434077994127} best_AUC:  0.7686875566115616\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 516.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.4938304425560929} best_AUC:  0.7680238623609622\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check quality to expect\n",
    "model = CatBoostClassifier(\n",
    "    l2_leaf_reg=int(best['l2_leaf_reg']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    iterations=500,\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    logging_level='Silent'\n",
    ")\n",
    "cv_data = cv(Pool(X, y, cat_features=categorical_features_indices), model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precise validation AUC score: {}'.format(np.max(cv_data['test-AUC-mean'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['canceled_pred', 'canceled_proba_pred'], axis=1)\n",
    "test = test.drop(['canceled_pred', 'canceled_proba_pred'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio =  sum(y_train==0) / sum(y_train==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "#          'border_count': 128, \n",
    "#          'one_hot_max_size': 5,\n",
    "#          'depth': 7,\n",
    "#          'depth': 5,\n",
    "          'iterations': 50.0,\n",
    "#          'l2_leaf_reg': 11.47871028241772, \n",
    "          'learning_rate': 0.15,\n",
    "#          'scale_pos_weight': pos_neg_ratio\n",
    "          'thread_count': 3\n",
    "         }\n",
    "\n",
    "#random_strength (default 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 5.32s\tremaining: 4m 20s\n",
      "1:\ttotal: 10.3s\tremaining: 4m 8s\n",
      "2:\ttotal: 15.3s\tremaining: 3m 59s\n",
      "3:\ttotal: 19.9s\tremaining: 3m 48s\n",
      "4:\ttotal: 24.6s\tremaining: 3m 41s\n",
      "5:\ttotal: 28.9s\tremaining: 3m 31s\n",
      "6:\ttotal: 33.5s\tremaining: 3m 25s\n",
      "7:\ttotal: 37.8s\tremaining: 3m 18s\n",
      "8:\ttotal: 42.6s\tremaining: 3m 14s\n",
      "9:\ttotal: 47.3s\tremaining: 3m 9s\n",
      "10:\ttotal: 52.7s\tremaining: 3m 6s\n",
      "11:\ttotal: 57.7s\tremaining: 3m 2s\n",
      "12:\ttotal: 1m 2s\tremaining: 2m 58s\n",
      "13:\ttotal: 1m 8s\tremaining: 2m 55s\n",
      "14:\ttotal: 1m 13s\tremaining: 2m 50s\n",
      "15:\ttotal: 1m 17s\tremaining: 2m 45s\n",
      "16:\ttotal: 1m 23s\tremaining: 2m 41s\n",
      "17:\ttotal: 1m 29s\tremaining: 2m 38s\n",
      "18:\ttotal: 1m 33s\tremaining: 2m 33s\n",
      "19:\ttotal: 1m 39s\tremaining: 2m 29s\n",
      "20:\ttotal: 1m 46s\tremaining: 2m 27s\n",
      "21:\ttotal: 1m 53s\tremaining: 2m 24s\n",
      "22:\ttotal: 1m 58s\tremaining: 2m 19s\n",
      "23:\ttotal: 2m 4s\tremaining: 2m 14s\n",
      "24:\ttotal: 2m 9s\tremaining: 2m 9s\n",
      "25:\ttotal: 2m 15s\tremaining: 2m 5s\n",
      "26:\ttotal: 2m 21s\tremaining: 2m\n",
      "27:\ttotal: 2m 27s\tremaining: 1m 56s\n",
      "28:\ttotal: 2m 34s\tremaining: 1m 51s\n",
      "29:\ttotal: 2m 40s\tremaining: 1m 46s\n",
      "30:\ttotal: 2m 59s\tremaining: 1m 50s\n",
      "31:\ttotal: 3m 8s\tremaining: 1m 45s\n",
      "32:\ttotal: 3m 14s\tremaining: 1m 40s\n",
      "33:\ttotal: 3m 19s\tremaining: 1m 33s\n",
      "34:\ttotal: 3m 26s\tremaining: 1m 28s\n",
      "35:\ttotal: 3m 34s\tremaining: 1m 23s\n",
      "36:\ttotal: 3m 40s\tremaining: 1m 17s\n",
      "37:\ttotal: 3m 48s\tremaining: 1m 12s\n",
      "38:\ttotal: 3m 58s\tremaining: 1m 7s\n",
      "39:\ttotal: 4m 4s\tremaining: 1m 1s\n",
      "40:\ttotal: 4m 11s\tremaining: 55.2s\n",
      "41:\ttotal: 4m 16s\tremaining: 48.9s\n",
      "42:\ttotal: 4m 24s\tremaining: 43s\n",
      "43:\ttotal: 4m 30s\tremaining: 36.9s\n",
      "44:\ttotal: 4m 37s\tremaining: 30.8s\n",
      "45:\ttotal: 4m 43s\tremaining: 24.6s\n",
      "46:\ttotal: 4m 48s\tremaining: 18.4s\n",
      "47:\ttotal: 4m 54s\tremaining: 12.3s\n",
      "48:\ttotal: 5m\tremaining: 6.13s\n",
      "49:\ttotal: 5m 6s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f82fcaec2e8>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(\n",
    "    **params,\n",
    "    eval_metric=\"AUC\",\n",
    "    od_type='Iter',\n",
    "    od_wait=40\n",
    ")\n",
    "\n",
    "model.fit(train, y_train, cat_features=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to disk\n",
    "#model.save_model('catboost_model.dump')\n",
    "#load\n",
    "#model = CatBoostClassifier()\n",
    "#model.load_model('catboost_model.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisstion = model.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = str(datetime.datetime.now())[:-7] + '_submission.csv'\n",
    "sub = pd.Series(submisstion, name='target')\n",
    "sub.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all parameters free and 200 iterations : 0.67\n",
    "all parameters free and 200 iterations and delete ressource_id : 0.67\n",
    "200 iterations + learning rate = 0.31 : 0.694\n",
    "50 iterations + learning rate = 0.15 : 0.7057\n",
    "200 iterations + learning rate = 0.15 : 0.695\n",
    "'' + deletion of a all id like variables : 0.684\n",
    "'' +  add cancellation results : 0.698 (vs 0.73 in val)\n",
    "200 iterations + learning rate = 0.15 + l2 = 11.5 :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests on cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio =  sum(y_train==0) / sum(y_train==1)\n",
    "\n",
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    border_count=128, \n",
    "    one_hot_max_size= 5,\n",
    "    depth= 5,\n",
    "    iterations= 200.0, \n",
    "    l2_leaf_reg= 11.478, \n",
    "    scale_pos_weight= pos_neg_ratio,\n",
    "    eval_metric='AUC'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = cv(\n",
    "    Pool(train, y_train, cat_features=categorical_features_indices),\n",
    "    model.get_params(),\n",
    "    stratified=True,\n",
    "    fold_count=5\n",
    ")\n",
    "\n",
    "#0.707 on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.plot(y=['test-AUC-mean', 'train-AUC-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.plot(y=['test-Logloss-mean', 'train-Logloss-mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with some randomly added na for contract variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_with_new_categ_not_in_contract = [var for var in var_with_new_categ if var not in categ_contract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanti_contract = ['PRIX_FACTURE', \n",
    "                   'CONTRAT_TARIF',\n",
    "                   'temps_depuis_debut_contrat',\n",
    "                   'temps_jusqua_fin_contrat', \n",
    "                   'temps_depuis_maj_contrat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in categ_contract + var_with_new_categ_not_in_contract:\n",
    "    try:\n",
    "        train.loc[:, var] = train[var].cat.add_categories(['NAN'])\n",
    "    except ValueError as e:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb86920812fd41d4b523ecf1ee541c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "#np.random.seed(42)\n",
    "for i in tqdm.tqdm_notebook(range(10)):\n",
    "    X_train = train.copy()\n",
    "    \n",
    "    #randomly put missing contract in train data\n",
    "    _idx = np.random.choice(X_train.index, size=X_train.shape[0]//20, replace=False)\n",
    "    X_train.loc[_idx, categ_contract] = 'NAN'\n",
    "    X_train.loc[_idx, quanti_contract] = -9999\n",
    "    \n",
    "    #randomly put missing in variables with new categories in test\n",
    "    for var in var_with_new_categ_not_in_contract:\n",
    "        _idx = np.random.choice(X_train.index, size=X_train.shape[0]//20, replace=False)\n",
    "        X_train.loc[_idx, var] = 'NAN'\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=8,\n",
    "        border_count=128,\n",
    "        learning_rate=0.30,\n",
    "#        l2_leaf_reg= 11.478,\n",
    "        eval_metric='AUC',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "    models.append(model.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a27a9b123e04c78ae5d992b8ae8a407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for _model in tqdm.tqdm_notebook(models):\n",
    "    predictions.append(_model.predict_proba(test)[:,1])\n",
    "    \n",
    "submisstion = np.vstack(predictions).T.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = str(datetime.datetime.now())[:-7] + '_submission.csv'\n",
    "sub = pd.Series(submisstion, name='target')\n",
    "sub.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119a09c719964b72814b3381f53df113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7638860824056132\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for _model in tqdm.tqdm_notebook(models):\n",
    "    predictions.append(_model.predict_proba(X_train_val)[:,1])\n",
    "    \n",
    "predictions = np.vstack(predictions).T.mean(axis=1)\n",
    "\n",
    "# 150 arbres : 0.7624871381817937\n",
    "# 200 arbres : .7638860824056132\n",
    "print(sklearn.metrics.roc_auc_score(y_train_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = np.hstack([predictions, predictions2]).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbm = lightgbm.LGBMClassifier(\n",
    "    seed=np.random.randint(10**10),\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    num_leaves=200,\n",
    "    min_data_in_leaf=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    #is_unbalance=True,\n",
    "    objective='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbm.fit(X_train_train, y_train_train, \n",
    "              eval_set=[(X_train_val, y_train_val)], \n",
    "              eval_metric='auc', \n",
    "              categorical_feature=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O random forest\n",
    "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#grid-search-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: assembling categorical features (see netstack tutorial)\n",
    "TODO: compare LB to val score\n",
    "TODO: remove cancelled binary\n",
    "TODO: hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_161\"; Java(TM) SE Runtime Environment (build 1.8.0_161-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)\n",
      "  Starting server from /home/julien/anaconda3/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp9qoibaju\n",
      "  JVM stdout: /tmp/tmp9qoibaju/h2o_julien_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp9qoibaju/h2o_julien_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Paris</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>24 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_julien_rk9xb4</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>7.111 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.6 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Europe/Paris\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.5\n",
       "H2O cluster version age:    24 days\n",
       "H2O cluster name:           H2O_from_python_julien_rk9xb4\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    7.111 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  2\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.6 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(nthreads=2, max_mem_size = \"8G\")             #specify max number of bytes. uses all cores by default.\n",
    "h2o.remove_all() #clean slate, in case cluster was already running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class H2OGradientBoostingEstimator in module h2o.estimators.gbm:\n",
      "\n",
      "class H2OGradientBoostingEstimator(h2o.estimators.estimator_base.H2OEstimator)\n",
      " |  Gradient Boosting Machine\n",
      " |  \n",
      " |  Builds gradient boosted trees on a parsed data set, for regression or classification.\n",
      " |  The default distribution function will guess the model type based on the response column type.\n",
      " |  Otherwise, the response column must be an enum for \"bernoulli\" or \"multinomial\", and numeric\n",
      " |  for all other distributions.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      H2OGradientBoostingEstimator\n",
      " |      h2o.estimators.estimator_base.H2OEstimator\n",
      " |      h2o.model.model_base.ModelBase\n",
      " |      h2o.utils.backward_compatibility.BackwardsCompatibleBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |      Construct a new model instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  balance_classes\n",
      " |      Balance training data class counts via over/under-sampling (for imbalanced data).\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  build_tree_one_node\n",
      " |      Run on one node only; no network overhead but fewer cpus used.  Suitable for small datasets.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  calibrate_model\n",
      " |      Use Platt Scaling to calculate calibrated class probabilities. Calibration can provide more accurate estimates\n",
      " |      of class probabilities.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  calibration_frame\n",
      " |      Calibration frame for Platt Scaling\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |  \n",
      " |  categorical_encoding\n",
      " |      Encoding scheme for categorical features\n",
      " |      \n",
      " |      One of: ``\"auto\"``, ``\"enum\"``, ``\"one_hot_internal\"``, ``\"one_hot_explicit\"``, ``\"binary\"``, ``\"eigen\"``,\n",
      " |      ``\"label_encoder\"``, ``\"sort_by_response\"``, ``\"enum_limited\"``  (default: ``\"auto\"``).\n",
      " |  \n",
      " |  checkpoint\n",
      " |      Model checkpoint to resume training with.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  class_sampling_factors\n",
      " |      Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will\n",
      " |      be automatically computed to obtain class balance during training. Requires balance_classes.\n",
      " |      \n",
      " |      Type: ``List[float]``.\n",
      " |  \n",
      " |  col_sample_rate\n",
      " |      Column sample rate (from 0.0 to 1.0)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1``).\n",
      " |  \n",
      " |  col_sample_rate_change_per_level\n",
      " |      Relative change of the column sampling rate for every level (must be > 0.0 and <= 2.0)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1``).\n",
      " |  \n",
      " |  col_sample_rate_per_tree\n",
      " |      Column sample rate per tree (from 0.0 to 1.0)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1``).\n",
      " |  \n",
      " |  custom_metric_func\n",
      " |      Reference to custom evaluation function, format: `language:keyName=funcName`\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  distribution\n",
      " |      Distribution function\n",
      " |      \n",
      " |      One of: ``\"auto\"``, ``\"bernoulli\"``, ``\"quasibinomial\"``, ``\"multinomial\"``, ``\"gaussian\"``, ``\"poisson\"``,\n",
      " |      ``\"gamma\"``, ``\"tweedie\"``, ``\"laplace\"``, ``\"quantile\"``, ``\"huber\"``  (default: ``\"auto\"``).\n",
      " |  \n",
      " |  fold_assignment\n",
      " |      Cross-validation fold assignment scheme, if fold_column is not specified. The 'Stratified' option will stratify\n",
      " |      the folds based on the response variable, for classification problems.\n",
      " |      \n",
      " |      One of: ``\"auto\"``, ``\"random\"``, ``\"modulo\"``, ``\"stratified\"``  (default: ``\"auto\"``).\n",
      " |  \n",
      " |  fold_column\n",
      " |      Column with cross-validation fold index assignment per observation.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  histogram_type\n",
      " |      What type of histogram to use for finding optimal split points\n",
      " |      \n",
      " |      One of: ``\"auto\"``, ``\"uniform_adaptive\"``, ``\"random\"``, ``\"quantiles_global\"``, ``\"round_robin\"``  (default:\n",
      " |      ``\"auto\"``).\n",
      " |  \n",
      " |  huber_alpha\n",
      " |      Desired quantile for Huber/M-regression (threshold between quadratic and linear loss, must be between 0 and 1).\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0.9``).\n",
      " |  \n",
      " |  ignore_const_cols\n",
      " |      Ignore constant columns.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``True``).\n",
      " |  \n",
      " |  ignored_columns\n",
      " |      Names of columns to ignore for training.\n",
      " |      \n",
      " |      Type: ``List[str]``.\n",
      " |  \n",
      " |  keep_cross_validation_fold_assignment\n",
      " |      Whether to keep the cross-validation fold assignment.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  keep_cross_validation_predictions\n",
      " |      Whether to keep the predictions of the cross-validation models.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  learn_rate\n",
      " |      Learning rate (from 0.0 to 1.0)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0.1``).\n",
      " |  \n",
      " |  learn_rate_annealing\n",
      " |      Scale the learning rate by this factor after each tree (e.g., 0.99 or 0.999)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1``).\n",
      " |  \n",
      " |  max_abs_leafnode_pred\n",
      " |      Maximum absolute value of a leaf node prediction\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1.797693135e+308``).\n",
      " |  \n",
      " |  max_after_balance_size\n",
      " |      Maximum relative size of the training data after balancing class counts (can be less than 1.0). Requires\n",
      " |      balance_classes.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``5``).\n",
      " |  \n",
      " |  max_confusion_matrix_size\n",
      " |      [Deprecated] Maximum size (# classes) for confusion matrices to be printed in the Logs\n",
      " |      \n",
      " |      Type: ``int``  (default: ``20``).\n",
      " |  \n",
      " |  max_depth\n",
      " |      Maximum tree depth.\n",
      " |      \n",
      " |      Type: ``int``  (default: ``5``).\n",
      " |  \n",
      " |  max_hit_ratio_k\n",
      " |      Max. number (top K) of predictions to use for hit ratio computation (for multi-class only, 0 to disable)\n",
      " |      \n",
      " |      Type: ``int``  (default: ``0``).\n",
      " |  \n",
      " |  max_runtime_secs\n",
      " |      Maximum allowed runtime in seconds for model training. Use 0 to disable.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0``).\n",
      " |  \n",
      " |  min_rows\n",
      " |      Fewest allowed (weighted) observations in a leaf.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``10``).\n",
      " |  \n",
      " |  min_split_improvement\n",
      " |      Minimum relative improvement in squared error reduction for a split to happen\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1e-05``).\n",
      " |  \n",
      " |  nbins\n",
      " |      For numerical columns (real/int), build a histogram of (at least) this many bins, then split at the best point\n",
      " |      \n",
      " |      Type: ``int``  (default: ``20``).\n",
      " |  \n",
      " |  nbins_cats\n",
      " |      For categorical columns (factors), build a histogram of this many bins, then split at the best point. Higher\n",
      " |      values can lead to more overfitting.\n",
      " |      \n",
      " |      Type: ``int``  (default: ``1024``).\n",
      " |  \n",
      " |  nbins_top_level\n",
      " |      For numerical columns (real/int), build a histogram of (at most) this many bins at the root level, then decrease\n",
      " |      by factor of two per level\n",
      " |      \n",
      " |      Type: ``int``  (default: ``1024``).\n",
      " |  \n",
      " |  nfolds\n",
      " |      Number of folds for K-fold cross-validation (0 to disable or >= 2).\n",
      " |      \n",
      " |      Type: ``int``  (default: ``0``).\n",
      " |  \n",
      " |  ntrees\n",
      " |      Number of trees.\n",
      " |      \n",
      " |      Type: ``int``  (default: ``50``).\n",
      " |  \n",
      " |  offset_column\n",
      " |      Offset column. This will be added to the combination of columns before applying the link function.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  pred_noise_bandwidth\n",
      " |      Bandwidth (sigma) of Gaussian multiplicative noise ~N(1,sigma) for tree node predictions\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0``).\n",
      " |  \n",
      " |  quantile_alpha\n",
      " |      Desired quantile for Quantile regression, must be between 0 and 1.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0.5``).\n",
      " |  \n",
      " |  r2_stopping\n",
      " |      r2_stopping is no longer supported and will be ignored if set - please use stopping_rounds, stopping_metric and\n",
      " |      stopping_tolerance instead. Previous version of H2O would stop making trees when the R^2 metric equals or\n",
      " |      exceeds this\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1.797693135e+308``).\n",
      " |  \n",
      " |  response_column\n",
      " |      Response variable column.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  sample_rate\n",
      " |      Row sample rate per tree (from 0.0 to 1.0)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1``).\n",
      " |  \n",
      " |  sample_rate_per_class\n",
      " |      A list of row sample rates per class (relative fraction for each class, from 0.0 to 1.0), for each tree\n",
      " |      \n",
      " |      Type: ``List[float]``.\n",
      " |  \n",
      " |  score_each_iteration\n",
      " |      Whether to score during each iteration of model training.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  score_tree_interval\n",
      " |      Score the model after every so many trees. Disabled if set to 0.\n",
      " |      \n",
      " |      Type: ``int``  (default: ``0``).\n",
      " |  \n",
      " |  seed\n",
      " |      Seed for pseudo random number generator (if applicable)\n",
      " |      \n",
      " |      Type: ``int``  (default: ``-1``).\n",
      " |  \n",
      " |  stopping_metric\n",
      " |      Metric to use for early stopping (AUTO: logloss for classification, deviance for regression)\n",
      " |      \n",
      " |      One of: ``\"auto\"``, ``\"deviance\"``, ``\"logloss\"``, ``\"mse\"``, ``\"rmse\"``, ``\"mae\"``, ``\"rmsle\"``, ``\"auc\"``,\n",
      " |      ``\"lift_top_group\"``, ``\"misclassification\"``, ``\"mean_per_class_error\"``  (default: ``\"auto\"``).\n",
      " |  \n",
      " |  stopping_rounds\n",
      " |      Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the\n",
      " |      stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable)\n",
      " |      \n",
      " |      Type: ``int``  (default: ``0``).\n",
      " |  \n",
      " |  stopping_tolerance\n",
      " |      Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0.001``).\n",
      " |  \n",
      " |  training_frame\n",
      " |      Id of the training data frame.\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |  \n",
      " |  tweedie_power\n",
      " |      Tweedie power for Tweedie regression, must be between 1 and 2.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1.5``).\n",
      " |  \n",
      " |  validation_frame\n",
      " |      Id of the validation data frame.\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |  \n",
      " |  weights_column\n",
      " |      Column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from the\n",
      " |      dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative\n",
      " |      weights are not allowed. Note: Weights are per-row observation weights and do not increase the size of the data\n",
      " |      frame. This is typically the number of times a row is repeated, but non-integer values are supported as well.\n",
      " |      During training, rows with higher weights matter more, due to the larger loss function pre-factor.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  algo = 'gbm'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  fit(self, x, y=None, **params)\n",
      " |      Fit an H2O model as part of a scikit-learn pipeline or grid search.\n",
      " |      \n",
      " |      A warning will be issued if a caller other than sklearn attempts to use this method.\n",
      " |      \n",
      " |      :param H2OFrame x: An H2OFrame consisting of the predictor variables.\n",
      " |      :param H2OFrame y: An H2OFrame consisting of the response variable.\n",
      " |      :param params: Extra arguments.\n",
      " |      :returns: The current instance of H2OEstimator for method chaining.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Obtain parameters for this estimator.\n",
      " |      \n",
      " |      Used primarily for sklearn Pipelines and sklearn grid search.\n",
      " |      \n",
      " |      :param deep: If True, return parameters of all sub-objects that are estimators.\n",
      " |      \n",
      " |      :returns: A dict of parameters\n",
      " |  \n",
      " |  join(self)\n",
      " |      Wait until job's completion.\n",
      " |  \n",
      " |  set_params(self, **parms)\n",
      " |      Used by sklearn for updating parameters during grid search.\n",
      " |      \n",
      " |      :param parms: A dictionary of parameters that will be set on this model.\n",
      " |      :returns: self, the current estimator object with the parameters all set as desired.\n",
      " |  \n",
      " |  start(self, x, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, **params)\n",
      " |      Train the model asynchronously (to block for results call :meth:`join`).\n",
      " |      \n",
      " |      :param x: A list of column names or indices indicating the predictor columns.\n",
      " |      :param y: An index or a column name indicating the response column.\n",
      " |      :param H2OFrame training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |      :param offset_column: The name or index of the column in training_frame that holds the offsets.\n",
      " |      :param fold_column: The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |      :param weights_column: The name or index of the column in training_frame that holds the per-row weights.\n",
      " |      :param validation_frame: H2OFrame with validation data to be scored on while training.\n",
      " |  \n",
      " |  train(self, x=None, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, max_runtime_secs=None, ignored_columns=None, model_id=None, verbose=False)\n",
      " |      Train the H2O model.\n",
      " |      \n",
      " |      :param x: A list of column names or indices indicating the predictor columns.\n",
      " |      :param y: An index or a column name indicating the response column.\n",
      " |      :param H2OFrame training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |      :param offset_column: The name or index of the column in training_frame that holds the offsets.\n",
      " |      :param fold_column: The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |      :param weights_column: The name or index of the column in training_frame that holds the per-row weights.\n",
      " |      :param validation_frame: H2OFrame with validation data to be scored on while training.\n",
      " |      :param float max_runtime_secs: Maximum allowed runtime in seconds for model training. Use 0 to disable.\n",
      " |      :param bool verbose: Print scoring history to stdout. Defaults to False.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  mixin(obj, cls)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aic(self, train=False, valid=False, xval=False)\n",
      " |      Get the AIC (Akaike Information Criterium).\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the AIC value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the AIC value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the AIC value for the validation data.\n",
      " |      \n",
      " |      :returns: The AIC.\n",
      " |  \n",
      " |  auc(self, train=False, valid=False, xval=False)\n",
      " |      Get the AUC (Area Under Curve).\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the AUC value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the AUC value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the AUC value for the validation data.\n",
      " |      \n",
      " |      :returns: The AUC.\n",
      " |  \n",
      " |  biases(self, vector_id=0)\n",
      " |      Return the frame for the respective bias vector.\n",
      " |      \n",
      " |      :param: vector_id: an integer, ranging from 0 to number of layers, that specifies the bias vector to return.\n",
      " |      \n",
      " |      :returns: an H2OFrame which represents the bias vector identified by vector_id\n",
      " |  \n",
      " |  catoffsets(self)\n",
      " |      Categorical offsets for one-hot encoding.\n",
      " |  \n",
      " |  coef(self)\n",
      " |      Return the coefficients which can be applied to the non-standardized data.\n",
      " |      \n",
      " |      Note: standardize = True by default, if set to False then coef() return the coefficients which are fit directly.\n",
      " |  \n",
      " |  coef_norm(self)\n",
      " |      Return coefficients fitted on the standardized data (requires standardize = True, which is on by default).\n",
      " |      \n",
      " |      These coefficients can be used to evaluate variable importance.\n",
      " |  \n",
      " |  cross_validation_fold_assignment(self)\n",
      " |      Obtain the cross-validation fold assignment for all rows in the training data.\n",
      " |      \n",
      " |      :returns: H2OFrame\n",
      " |  \n",
      " |  cross_validation_holdout_predictions(self)\n",
      " |      Obtain the (out-of-sample) holdout predictions of all cross-validation models on the training data.\n",
      " |      \n",
      " |      This is equivalent to summing up all H2OFrames returned by cross_validation_predictions.\n",
      " |      \n",
      " |      :returns: H2OFrame\n",
      " |  \n",
      " |  cross_validation_metrics_summary(self)\n",
      " |      Retrieve Cross-Validation Metrics Summary.\n",
      " |      \n",
      " |      :returns: The cross-validation metrics summary as an H2OTwoDimTable\n",
      " |  \n",
      " |  cross_validation_models(self)\n",
      " |      Obtain a list of cross-validation models.\n",
      " |      \n",
      " |      :returns: list of H2OModel objects.\n",
      " |  \n",
      " |  cross_validation_predictions(self)\n",
      " |      Obtain the (out-of-sample) holdout predictions of all cross-validation models on their holdout data.\n",
      " |      \n",
      " |      Note that the predictions are expanded to the full number of rows of the training data, with 0 fill-in.\n",
      " |      \n",
      " |      :returns: list of H2OFrame objects.\n",
      " |  \n",
      " |  deepfeatures(self, test_data, layer)\n",
      " |      Return hidden layer details.\n",
      " |      \n",
      " |      :param test_data: Data to create a feature space on\n",
      " |      :param layer: 0 index hidden layer\n",
      " |  \n",
      " |  download_mojo(self, path='.', get_genmodel_jar=False, genmodel_name='')\n",
      " |      Download the model in MOJO format.\n",
      " |      \n",
      " |      :param path: the path where MOJO file should be saved.\n",
      " |      :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.\n",
      " |      :param genmodel_name Custom name of genmodel jar\n",
      " |      :returns: name of the MOJO file written.\n",
      " |  \n",
      " |  download_pojo(self, path='', get_genmodel_jar=False, genmodel_name='')\n",
      " |      Download the POJO for this model to the directory specified by path.\n",
      " |      \n",
      " |      If path is an empty string, then dump the output to screen.\n",
      " |      \n",
      " |      :param path:  An absolute path to the directory where POJO should be saved.\n",
      " |      :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.\n",
      " |      :param genmodel_name Custom name of genmodel jar\n",
      " |      :returns: name of the POJO file written.\n",
      " |  \n",
      " |  get_xval_models(self, key=None)\n",
      " |      Return a Model object.\n",
      " |      \n",
      " |      :param key: If None, return all cross-validated models; otherwise return the model that key points to.\n",
      " |      \n",
      " |      :returns: A model or list of models.\n",
      " |  \n",
      " |  gini(self, train=False, valid=False, xval=False)\n",
      " |      Get the Gini coefficient.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\"\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the Gini Coefficient value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the Gini Coefficient value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the Gini Coefficient value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The Gini Coefficient for this binomial model.\n",
      " |  \n",
      " |  is_cross_validated(self)\n",
      " |      Return True if the model was cross-validated.\n",
      " |  \n",
      " |  levelone_frame_id(self)\n",
      " |      Fetch the levelone_frame_id for the model, if any.  Currently only used by H2OStackedEnsembleEstimator.\n",
      " |  \n",
      " |  logloss(self, train=False, valid=False, xval=False)\n",
      " |      Get the Log Loss.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the log loss value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the log loss value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the log loss value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The log loss for this regression model.\n",
      " |  \n",
      " |  mae(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Absolute Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the MAE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the MAE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the MAE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The MAE for this regression model.\n",
      " |  \n",
      " |  mean_residual_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Residual Deviances.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the Mean Residual Deviance value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the Mean Residual Deviance value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the Mean Residual Deviance value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The Mean Residual Deviance for this regression model.\n",
      " |  \n",
      " |  metalearner(self)\n",
      " |      Print the metalearner for the model, if any.  Currently only used by H2OStackedEnsembleEstimator.\n",
      " |  \n",
      " |  model_performance(self, test_data=None, train=False, valid=False, xval=False)\n",
      " |      Generate model metrics for this model on test_data.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data set for which model metrics shall be computed against. All three of train,\n",
      " |          valid and xval arguments are ignored if test_data is not None.\n",
      " |      :param bool train: Report the training metrics for the model.\n",
      " |      :param bool valid: Report the validation metrics for the model.\n",
      " |      :param bool xval: Report the cross-validation metrics for the model. If train and valid are True, then it\n",
      " |          defaults to True.\n",
      " |      \n",
      " |      :returns: An object of class H2OModelMetrics.\n",
      " |  \n",
      " |  mse(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Square Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the MSE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the MSE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the MSE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The MSE for this regression model.\n",
      " |  \n",
      " |  normmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric predictors.\n",
      " |  \n",
      " |  normsub(self)\n",
      " |      Normalization/Standardization offsets for numeric predictors.\n",
      " |  \n",
      " |  null_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the null dof for the training set. If both train and valid are False, then train is\n",
      " |          selected by default.\n",
      " |      :param bool valid: Get the null dof for the validation set. If both train and valid are True, then train is\n",
      " |          selected by default.\n",
      " |      \n",
      " |      :returns: Return the null dof, or None if it is not present.\n",
      " |  \n",
      " |  null_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the null deviance for the training set. If both train and valid are False, then train\n",
      " |          is selected by default.\n",
      " |      :param bool valid: Get the null deviance for the validation set. If both train and valid are True, then train\n",
      " |          is selected by default.\n",
      " |      \n",
      " |      :returns: Return the null deviance, or None if it is not present.\n",
      " |  \n",
      " |  partial_plot(self, data, cols, destination_key=None, nbins=20, plot=True, plot_stddev=True, figsize=(7, 10), server=False)\n",
      " |      Create partial dependence plot which gives a graphical depiction of the marginal effect of a variable on the\n",
      " |      response. The effect of a variable is measured in change in the mean response.\n",
      " |      \n",
      " |      :param H2OFrame data: An H2OFrame object used for scoring and constructing the plot.\n",
      " |      :param cols: Feature(s) for which partial dependence will be calculated.\n",
      " |      :param destination_key: An key reference to the created partial dependence tables in H2O.\n",
      " |      :param nbins: Number of bins used. For categorical columns make sure the number of bins exceed the level count.\n",
      " |      :param plot: A boolean specifying whether to plot partial dependence table.\n",
      " |      :param plot_stddev: A boolean specifying whether to add std err to partial dependence plot.\n",
      " |      :param figsize: Dimension/size of the returning plots, adjust to fit your output cells.\n",
      " |      :param server: ?\n",
      " |      :returns: Plot and list of calculated mean response tables for each feature requested.\n",
      " |  \n",
      " |  pprint_coef(self)\n",
      " |      Pretty print the coefficents table (includes normalized coefficients).\n",
      " |  \n",
      " |  predict(self, test_data, custom_metric=None, custom_metric_func=None)\n",
      " |      Predict on a dataset.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      :param custom_metric:  custom evaluation function defined as class reference, the class get uploaded\n",
      " |      into cluster\n",
      " |      :param custom_metric_func: custom evaluation function reference, e.g, result of upload_custom_metric\n",
      " |      \n",
      " |      :returns: A new H2OFrame of predictions.\n",
      " |  \n",
      " |  predict_leaf_node_assignment(self, test_data)\n",
      " |      Predict on a dataset and return the leaf node assignment (only for tree-based models).\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      \n",
      " |      :returns: A new H2OFrame of predictions.\n",
      " |  \n",
      " |  r2(self, train=False, valid=False, xval=False)\n",
      " |      Return the R squared for this regression model.\n",
      " |      \n",
      " |      Will return R^2 for GLM Models and will return NaN otherwise.\n",
      " |      \n",
      " |      The R^2 value is defined to be 1 - MSE/var, where var is computed as sigma*sigma.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the R^2 value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the R^2 value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the R^2 value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The R squared for this regression model.\n",
      " |  \n",
      " |  residual_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the residual degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the residual dof for the training set. If both train and valid are False, then train\n",
      " |          is selected by default.\n",
      " |      :param bool valid: Get the residual dof for the validation set. If both train and valid are True, then train\n",
      " |          is selected by default.\n",
      " |      \n",
      " |      :returns: Return the residual dof, or None if it is not present.\n",
      " |  \n",
      " |  residual_deviance(self, train=False, valid=False, xval=None)\n",
      " |      Retreive the residual deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the residual deviance for the training set. If both train and valid are False, then\n",
      " |          train is selected by default.\n",
      " |      :param bool valid: Get the residual deviance for the validation set. If both train and valid are True, then\n",
      " |          train is selected by default.\n",
      " |      \n",
      " |      :returns: Return the residual deviance, or None if it is not present.\n",
      " |  \n",
      " |  respmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric response.\n",
      " |  \n",
      " |  respsub(self)\n",
      " |      Normalization/Standardization offsets for numeric response.\n",
      " |  \n",
      " |  rmse(self, train=False, valid=False, xval=False)\n",
      " |      Get the Root Mean Square Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the RMSE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the RMSE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the RMSE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The RMSE for this regression model.\n",
      " |  \n",
      " |  rmsle(self, train=False, valid=False, xval=False)\n",
      " |      Get the Root Mean Squared Logarithmic Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the RMSLE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the RMSLE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the RMSLE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The RMSLE for this regression model.\n",
      " |  \n",
      " |  rotation(self)\n",
      " |      Obtain the rotations (eigenvectors) for a PCA model\n",
      " |      \n",
      " |      :return: H2OFrame\n",
      " |  \n",
      " |  save_model_details(self, path='', force=False)\n",
      " |      Save Model Details of an H2O Model in JSON Format to disk.\n",
      " |      \n",
      " |      :param model: The model object to save.\n",
      " |      :param path: a path to save the model details at (hdfs, s3, local)\n",
      " |      :param force: if True overwrite destination directory in case it exists, or throw exception if set to False.\n",
      " |      \n",
      " |      :returns str: the path of the saved model details\n",
      " |  \n",
      " |  save_mojo(self, path='', force=False)\n",
      " |      Save an H2O Model as MOJO (Model Object, Optimized) to disk.\n",
      " |      \n",
      " |      :param model: The model object to save.\n",
      " |      :param path: a path to save the model at (hdfs, s3, local)\n",
      " |      :param force: if True overwrite destination directory in case it exists, or throw exception if set to False.\n",
      " |      \n",
      " |      :returns str: the path of the saved model\n",
      " |  \n",
      " |  score_history(self)\n",
      " |      DEPRECATED. Use :meth:`scoring_history` instead.\n",
      " |  \n",
      " |  scoring_history(self)\n",
      " |      Retrieve Model Score History.\n",
      " |      \n",
      " |      :returns: The score history as an H2OTwoDimTable or a Pandas DataFrame.\n",
      " |  \n",
      " |  show(self)\n",
      " |      Print innards of model, without regards to type.\n",
      " |  \n",
      " |  std_coef_plot(self, num_of_features=None, server=False)\n",
      " |      Plot a GLM model\"s standardized coefficient magnitudes.\n",
      " |      \n",
      " |      :param num_of_features: the number of features shown in the plot.\n",
      " |      :param server: ?\n",
      " |      \n",
      " |      :returns: None.\n",
      " |  \n",
      " |  summary(self)\n",
      " |      Print a detailed summary of the model.\n",
      " |  \n",
      " |  varimp(self, use_pandas=False)\n",
      " |      Pretty print the variable importances, or return them in a list.\n",
      " |      \n",
      " |      :param use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n",
      " |      \n",
      " |      :returns: A list or Pandas DataFrame.\n",
      " |  \n",
      " |  varimp_plot(self, num_of_features=None, server=False)\n",
      " |      Plot the variable importance for a trained model.\n",
      " |      \n",
      " |      :param num_of_features: the number of features shown in the plot (default is 10 or all if less than 10).\n",
      " |      :param server: ?\n",
      " |      \n",
      " |      :returns: None.\n",
      " |  \n",
      " |  weights(self, matrix_id=0)\n",
      " |      Return the frame for the respective weight matrix.\n",
      " |      \n",
      " |      :param: matrix_id: an integer, ranging from 0 to number of layers, that specifies the weight matrix to return.\n",
      " |      \n",
      " |      :returns: an H2OFrame which represents the weight matrix identified by matrix_id\n",
      " |  \n",
      " |  xval_keys(self)\n",
      " |      Return model keys for the cross-validated model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  actual_params\n",
      " |      Dictionary of actual parameters of the model.\n",
      " |  \n",
      " |  default_params\n",
      " |      Dictionary of the default parameters of the model.\n",
      " |  \n",
      " |  full_parameters\n",
      " |      Dictionary of the full specification of all parameters.\n",
      " |  \n",
      " |  have_mojo\n",
      " |      True, if export to MOJO is possible\n",
      " |  \n",
      " |  have_pojo\n",
      " |      True, if export to POJO is possible\n",
      " |  \n",
      " |  model_id\n",
      " |      Model identifier.\n",
      " |  \n",
      " |  params\n",
      " |      Get the parameters and the actual/default values only.\n",
      " |      \n",
      " |      :returns: A dictionary of parameters used to build this model.\n",
      " |  \n",
      " |  type\n",
      " |      The type of model built: ``\"classifier\"`` or ``\"regressor\"`` or ``\"unsupervised\"``\n",
      " |  \n",
      " |  xvals\n",
      " |      Return a list of the cross-validated models.\n",
      " |      \n",
      " |      :returns: A list of models.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.utils.backward_compatibility.BackwardsCompatibleBase:\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h2o.utils.backward_compatibility.BackwardsCompatibleBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Help on function import_file in module h2o.h2o:\n",
      "\n",
      "import_file(path=None, destination_frame=None, parse=True, header=0, sep=None, col_names=None, col_types=None, na_strings=None, pattern=None)\n",
      "    Import a dataset that is already on the cluster.\n",
      "    \n",
      "    The path to the data must be a valid path for each node in the H2O cluster. If some node in the H2O cluster\n",
      "    cannot see the file, then an exception will be thrown by the H2O cluster. Does a parallel/distributed\n",
      "    multi-threaded pull of the data. The main difference between this method and :func:`upload_file` is that\n",
      "    the latter works with local files, whereas this method imports remote files (i.e. files local to the server).\n",
      "    If you running H2O server on your own maching, then both methods behave the same.\n",
      "    \n",
      "    :param path: path(s) specifying the location of the data to import or a path to a directory of files to import\n",
      "    :param destination_frame: The unique hex key assigned to the imported file. If none is given, a key will be\n",
      "        automatically generated.\n",
      "    :param parse: If True, the file should be parsed after import. If False, then a list is returned containing the file path.\n",
      "    :param header: -1 means the first line is data, 0 means guess, 1 means first line is header.\n",
      "    :param sep: The field separator character. Values on each line of the file are separated by\n",
      "        this character. If not provided, the parser will automatically detect the separator.\n",
      "    :param col_names: A list of column names for the file.\n",
      "    :param col_types: A list of types or a dictionary of column names to types to specify whether columns\n",
      "        should be forced to a certain type upon import parsing. If a list, the types for elements that are\n",
      "        one will be guessed. The possible types a column may have are:\n",
      "    \n",
      "        - \"unknown\" - this will force the column to be parsed as all NA\n",
      "        - \"uuid\"    - the values in the column must be true UUID or will be parsed as NA\n",
      "        - \"string\"  - force the column to be parsed as a string\n",
      "        - \"numeric\" - force the column to be parsed as numeric. H2O will handle the compression of the numeric\n",
      "          data in the optimal manner.\n",
      "        - \"enum\"    - force the column to be parsed as a categorical column.\n",
      "        - \"time\"    - force the column to be parsed as a time column. H2O will attempt to parse the following\n",
      "          list of date time formats: (date) \"yyyy-MM-dd\", \"yyyy MM dd\", \"dd-MMM-yy\", \"dd MMM yy\", (time)\n",
      "          \"HH:mm:ss\", \"HH:mm:ss:SSS\", \"HH:mm:ss:SSSnnnnnn\", \"HH.mm.ss\" \"HH.mm.ss.SSS\", \"HH.mm.ss.SSSnnnnnn\".\n",
      "          Times can also contain \"AM\" or \"PM\".\n",
      "    :param na_strings: A list of strings, or a list of lists of strings (one list per column), or a dictionary\n",
      "        of column names to strings which are to be interpreted as missing values.\n",
      "    :param pattern: Character string containing a regular expression to match file(s) in the folder if `path` is a\n",
      "        directory.\n",
      "    \n",
      "    :returns: a new :class:`H2OFrame` instance.\n",
      "    \n",
      "    :examples:\n",
      "        >>> # Single file import\n",
      "        >>> iris = import_file(\"h2o-3/smalldata/iris.csv\")\n",
      "        >>> # Return all files in the folder iris/ matching the regex r\"iris_.*\\.csv\"\n",
      "        >>> iris_pattern = h2o.import_file(path = \"h2o-3/smalldata/iris\",\n",
      "        ...                                pattern = \"iris_.*\\.csv\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "help(H2OGradientBoostingEstimator)\n",
    "help(h2o.import_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target = train.target.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/anaconda3/lib/python3.6/site-packages/h2o/utils/shared_utils.py:177: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "#covtype_df = h2o.import_file(os.path.realpath(\"../data/covtype.full.csv\"))\n",
    "#convert df to H2O df type\n",
    "covtype_df = h2o.H2OFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data as described above\n",
    "X_train, X_valid, X_test = covtype_df.split_frame([0.60, 0.20], seed=1234)\n",
    "\n",
    "#Prepare predictors and response columns\n",
    "\n",
    "covtype_X = covtype_df.col_names     #last column is Cover_Type, our desired response variable \n",
    "covtype_X.remove('target')\n",
    "covtype_y = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_v1 = H2ORandomForestEstimator(\n",
    "    model_id=\"rf_v1\",\n",
    "    ntrees=50,\n",
    "    stopping_rounds=2,\n",
    "    score_each_iteration=True,\n",
    "    seed=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "rf_v1.train(covtype_X, covtype_y, training_frame=X_train, validation_frame=X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7171321991348156"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(X_test['target'].as_data_frame(), rf_v1.predict(X_test[covtype_X])[2].as_data_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_v2 = H2ORandomForestEstimator(\n",
    "    model_id=\"rf_covType_v2\",\n",
    "    ntrees=200,\n",
    "    max_depth=30,\n",
    "    stopping_rounds=2,\n",
    "    stopping_tolerance=0.01,\n",
    "    score_each_iteration=True,\n",
    "    seed=3000000)\n",
    "rf_v2.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_v2.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOE + LR\n",
    "https://github.com/patrick201/information_value/blob/master/src/information_value.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
