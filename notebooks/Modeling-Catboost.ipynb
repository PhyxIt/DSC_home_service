{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.model_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import model_selection , metrics   #Additional scklearn functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_strategy = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/merged_data/train.pkl')\n",
    "test = pd.read_pickle('../data/merged_data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to try:\n",
    "* optimize aucroc with keras and batch methods\n",
    "* SVM / rank SVM (https://github.com/rdipietro/pyrvm)\n",
    "* try separate models on separate variables + stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features : \n",
    "* NLP on COMMENTAIRE_BI\n",
    "* Extract options from OPTION (pas sur que ce soit util, peut être juste check la qualité)\n",
    "* features from history\n",
    "\n",
    "\n",
    "\n",
    "* check for intervention in test without contracts and to handle them\n",
    "* add combinations of categorical features (see paribas example on catboost site)\n",
    "* select only best features (feature_importance ?)\n",
    "* try to put random nas in train for handling missing contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fails:\n",
    "* one model with selectKbest on modalities vs one model on dimensionality reduction with group rare modalities\n",
    "    * dimensionality reduction don't bring much value here (mb MCA)\n",
    "    * select k best fails with 5k categories, which is not enough to be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['INSTANCE_ID', #460k modalities, not usable as a feature\n",
    "        'INCIDENT_NUMBER']\n",
    "drop_atm = [#'AUTEUR_INCIDENT', # 2088 modalities\n",
    "            'TYPE_VOIE',\n",
    "#            'NATURE_CODE', # 313 modalities, need to be splitted in 5 modalities\n",
    "#            'MARQUE_LIB', # 167 modalities\n",
    "#            'OPTION', # 80 modalities, extract options\n",
    "#            'MODELE_CODE', # 10k modalities\n",
    "#            'COMMENTAIRE_BI', # NLP 400k modalities\n",
    "#             'RESOURCE_ID', # 4033 modalities\n",
    "            'CODE_POSTAL', # 5800 modalities (only get first 2 numbers ?)\n",
    "            'L2_ORGA_CODE_POSTAL', # 147 modalities (might be redondent with L2_ORGANISATION_ID)\n",
    "#            'L2_ORGANISATION_ID' #151 modalities\n",
    "            'L2_ORGA_VILLE', # 146, might be redondent with other organisation variables\n",
    "#            'RACHAT_CODE' # 312 modalities (try binarising ?)         \n",
    "#            'CODE_INSTALLATION' # 17 modalities\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(drop + drop_atm + ['target'], axis=1, inplace=True)\n",
    "test.drop(drop + drop_atm, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = list(train.columns[train.dtypes == 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### imputation of missing data\n",
    "\n",
    "TODO: try imputing test based on test values, not train <br>\n",
    "TODO: try diffrent strategy on imputing datas from contract since missing are present only in test set<BR>\n",
    "TODO: try creating data with missing contract in train sample and do not fill the missing in test<br>\n",
    "TODO: try imputing specific na value for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_selected_variables(df, test, categ, quanti, dates):\n",
    "    _df = df.copy()\n",
    "    _test = test.copy() if test is not None else None\n",
    "    \n",
    "    replace = _df[categ].mode()\n",
    "    replace_values = {k:v.iloc[0] for k,v in replace.items()}\n",
    "    _df.fillna(replace_values, inplace=True)\n",
    "\n",
    "    replace_quanti = _df[quanti].mean()\n",
    "    _df.fillna(replace_quanti, inplace=True)\n",
    "\n",
    "    _df[dates] = _df[dates].fillna(method='pad')\n",
    "    \n",
    "    if test is not None:\n",
    "        \n",
    "        _test.fillna(replace_values, inplace=True)\n",
    "        _test.fillna(replace_quanti, inplace=True)\n",
    "        _test[dates] = _df[dates].fillna(method='pad')\n",
    "    \n",
    "    return _df, _test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace = train[categoricals].mode()\n",
    "#replace_values = {k:v.iloc[0] for k,v in replace.items()}\n",
    "def impute_contract_variables(df):\n",
    "    _df = df.copy()\n",
    "    \n",
    "    for var in categ_contract:\n",
    "        _df[var] = _df[var].cat.add_categories(['NAN'])\n",
    "        _df[var].fillna('NAN', inplace=True)\n",
    "\n",
    "    _df[quanti_contract] = _df[quanti_contract].fillna(-9999)\n",
    "    _df[date_contract] = _df[date_contract].fillna(datetime.datetime(1970, 1, 1))\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = list(train.columns[train.dtypes == 'category'])\n",
    "quantitative = ['NB_PASSAGE', 'POINTS_FIDEL', 'CONTRAT_TARIF', 'PRIX_FACTURE']\n",
    "dates = list(train.columns[train.dtypes == 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_variables = [ 'UPD_DATE', 'DATE_DEBUT', 'DATE_FIN', 'STS_CODE', 'OPTION', 'FORMULE', 'CONTRAT_TARIF', 'PRIX_FACTURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute without contract\n",
    "categ_to_impute = list(set(categoricals) - set(contract_variables))\n",
    "quanti_to_impute = list(set(quantitative) - set(contract_variables))\n",
    "date_to_impute = list(set(dates) - set(contract_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute contract\n",
    "categ_contract = list(set(categoricals).intersection(set(contract_variables)))\n",
    "quanti_contract = list(set(quantitative).intersection(set(contract_variables)))\n",
    "date_contract = list(set(dates).intersection(set(contract_variables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test are filled with values taken from train\n",
    "#contract and other variables are imputed together\n",
    "if imput_strategy == 1:\n",
    "    train, test = impute_selected_variables(train, test, categoricals, quantitative, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test are filled with values taken on their own values\n",
    "#contract and other variables are imputed together\n",
    "if imput_strategy == 2:\n",
    "    train, _ = impute_selected_variables(train, None, categoricals, quantitative, dates)\n",
    "    test, _ = impute_selected_variables(test, None, categoricals, quantitative, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test are filled with values taken from train\n",
    "#contract and other variables are imputed separatly (need to import some NAN in train set)\n",
    "if imput_strategy == 3:\n",
    "    train, test = impute_selected_variables(train, test, categ_to_impute, quanti_to_impute, date_to_impute)\n",
    "    train = impute_contract_variables(train)\n",
    "    test = impute_contract_variables(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test are filled on their own values\n",
    "# contract and other variables are imputed separatly (need to import some NAN in train set)\n",
    "if imput_strategy == 4:\n",
    "    train, _ = impute_selected_variables(train, None, categ_to_impute, quanti_to_impute, date_to_impute)\n",
    "    test, _ = impute_selected_variables(test, None, categ_to_impute, quanti_to_impute, date_to_impute)\n",
    "    train = impute_contract_variables(train)\n",
    "    test = impute_contract_variables(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature ingineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commentaire_bi(df):\n",
    "    _df = df.copy()\n",
    "    \n",
    "    _df.COMMENTAIRE_BI = _df.COMMENTAIRE_BI.str.upper()\n",
    "    COMMENTAIRE_BI_vc = _df.COMMENTAIRE_BI.value_counts()\n",
    "    common_commentaire_bi = COMMENTAIRE_BI_vc[COMMENTAIRE_BI_vc > 100].index\n",
    "    _df['COMMENTAIRE_BI_common'] = _df.COMMENTAIRE_BI.where(_df.COMMENTAIRE_BI.isin(common_commentaire_bi), \"Rare\")\n",
    "    \n",
    "    _df['nb_char_commentaire'] = [len(txt) for txt in _df.COMMENTAIRE_BI]\n",
    "    _df['nb_mots_commentaire'] = [len(txt.split()) for txt in _df.COMMENTAIRE_BI]\n",
    "    _df['has_number_commentaire'] = [any(char.isdigit() for char in txt) for txt in _df.COMMENTAIRE_BI]\n",
    "    _df['is_empty_commentaire'] = [(txt == '.') for txt in _df.COMMENTAIRE_BI]\n",
    "    _df.drop('COMMENTAIRE_BI', axis=1, inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nature_code_split(df):\n",
    "    _df = df.copy()\n",
    "    nature_code_splitted = [nc.split('-') for nc in df.NATURE_CODE]\n",
    "    nature_code_df = pd.DataFrame(nature_code_splitted, columns=['nc_1', 'nc_2', 'nc_3', 'nc_4', 'nc_5'])\n",
    "    nature_code_df.fillna('-1', inplace=True)\n",
    "    for nc_i in ['nc_1', 'nc_2', 'nc_3', 'nc_4', 'nc_5']:\n",
    "        nature_code_df[nc_i] = nature_code_df[nc_i].astype('category')\n",
    "    \n",
    "    #_df.drop('NATURE_CODE', axis=1, inplace=True)\n",
    "    _df = _df.merge(nature_code_df, left_index=True, right_index=True)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: use dt series accessor\n",
    "def add_dates_features(data):\n",
    "    data['age_installation'] = (data['CRE_DATE_GZL'] - data['INSTALL_DATE']).dt.days // 365\n",
    "    data['mois_appel'] = data['CRE_DATE_GZL'].map(lambda x: x.month)\n",
    "    data['joursemaine_appel'] = data['CRE_DATE_GZL'].map(lambda x: x.isoweekday()) #integer, might be considered categorical\n",
    "    data['jour_appel'] = data['CRE_DATE_GZL'].map(lambda x: x.day)\n",
    "    data['mois_intervention'] = data['SCHEDULED_START_DATE'].map(lambda x: x.month)\n",
    "    data['joursemaine_intervention'] = data['SCHEDULED_START_DATE'].map(lambda x: x.isoweekday()) #integer, might be considered categorical\n",
    "    data['jour_intervention'] = data['SCHEDULED_START_DATE'].map(lambda x: x.day)\n",
    "    data['duree_avant_intervention'] = (data['SCHEDULED_START_DATE'] - data['CRE_DATE_GZL']).dt.days\n",
    "    data['duree_prevue'] = (data['SCHEDULED_END_DATE'] - data['SCHEDULED_START_DATE']).dt.days\n",
    "    data['temps_depuis_debut_contrat'] = (data['CRE_DATE_GZL'] - data['DATE_DEBUT']).dt.days\n",
    "    data['temps_jusqua_fin_contrat'] = (data['CRE_DATE_GZL'] - data['DATE_FIN']).dt.days  #souvent nan ? (mettre 0)\n",
    "    data['temps_depuis_maj_contrat'] = (data['CRE_DATE_GZL'] - data['UPD_DATE']).dt.days \n",
    "\n",
    "    data.drop(['CRE_DATE_GZL', 'INSTALL_DATE', 'SCHEDULED_START_DATE', 'SCHEDULED_END_DATE', 'DATE_DEBUT', 'DATE_FIN', 'UPD_DATE'], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mots en particulier\n",
    "#créer un dictionnaire et compter (count et garder les must ? voir la diff entre les cas + et - ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# (history) temps depuis dernière visite (pas forcément dispo sur le test)\n",
    "# (history) déjà eu une casse sur ce matériel\n",
    "# (history) temps depuis dernière casse\n",
    "# (history) la dernière visite date de moins de 6 mois\n",
    "# (history) nb interventions faires par la ressource\n",
    "# (history) temps depuis la première intervention de la ressource\n",
    "# (contract history) nb de fois que le contrat a été mis à jour sur les X dernières années"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = commentaire_bi(train)\n",
    "train = nature_code_split(train)\n",
    "train = add_dates_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = commentaire_bi(test)\n",
    "test = nature_code_split(test)\n",
    "test = add_dates_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop variables\n",
    "drop = ['joursemaine_appel', \n",
    "'USAGE_LOCAL', \n",
    "'nc_4', \n",
    "'is_empty_commentaire', \n",
    "'duree_prevue', \n",
    "'nc_1']\n",
    "\n",
    "train.drop(drop, axis=1,inplace=True)\n",
    "test.drop(drop, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get variables where test set has modalities which are not in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = list(train.columns[train.dtypes == 'category'])\n",
    "var_with_new_categ = []\n",
    "for cat in categoricals:\n",
    "    if len(set(test[cat]) - set(train[cat])) > 0:\n",
    "        var_with_new_categ.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop categories in test which are not in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-22064db2a5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategoricals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_unused_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4370\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[1;32m   4371\u001b[0m                 name in self._accessors):\n\u001b[0;32m-> 4372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2377\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2378\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2386\u001b[0;31m             raise AttributeError(\"Can only use .cat accessor with a \"\n\u001b[0m\u001b[1;32m   2387\u001b[0m                                  \"'category' dtype\")\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "for cat in categoricals:\n",
    "    train[cat] = train[cat].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoriesDroper(BaseEstimator, TransformerMixin):\n",
    "    '''Drop categories which are not in the train set'''\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.categories_dict = dict()\n",
    "        \n",
    "    def fit(self, df, y=None):\n",
    "        self.categories_dict = {column: df[column].cat.categories for column in self.columns}\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        _df = df.copy()\n",
    "        for column in self.columns:\n",
    "            _df[column] = _df[column].cat.set_categories(self.categories_dict[column])\n",
    "            try:\n",
    "                _df[column] = _df[column].fillna('NAN')\n",
    "            except ValueError as e:\n",
    "                _df[column] = _df[column].cat.add_categories(['NAN'])\n",
    "                _df[column] = _df[column].fillna('NAN') \n",
    "                print(e,'with ', column)\n",
    "        \n",
    "        return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill value must be in categories in  INCIDENT_TYPE_ID\n",
      "fill value must be in categories in  TYPE_BI\n",
      "fill value must be in categories in  MILLESIME\n",
      "fill value must be in categories in  PROBLEM_CODE\n",
      "fill value must be in categories in  AUTEUR_INCIDENT\n",
      "fill value must be in categories in  GRAVITE\n",
      "fill value must be in categories in  RESOURCE_ID\n",
      "fill value must be in categories in  TYPE_OCC\n",
      "fill value must be in categories in  RACHAT_CODE\n",
      "fill value must be in categories in  NATURE_CODE\n",
      "fill value must be in categories in  MARQUE_LIB\n",
      "fill value must be in categories in  MODELE_CODE\n",
      "fill value must be in categories in  PAYS\n",
      "fill value must be in categories in  STOP_PHONING\n",
      "fill value must be in categories in  CODE_GEN_EQUIPEMENT\n",
      "fill value must be in categories in  CODE_FONCTION\n",
      "fill value must be in categories in  CODE_ENERGIE\n",
      "fill value must be in categories in  CODE_INSTALLATION\n",
      "fill value must be in categories in  CODE_SPECIFICATION\n",
      "fill value must be in categories in  CODE_EAU_CHAUDE\n",
      "fill value must be in categories in  L1_ORGANISATION_ID\n",
      "fill value must be in categories in  L2_ORGANISATION_ID\n",
      "fill value must be in categories in  STS_CODE\n",
      "fill value must be in categories in  FORMULE\n",
      "fill value must be in categories in  OPTION\n",
      "fill value must be in categories in  nc_2\n",
      "fill value must be in categories in  nc_3\n",
      "fill value must be in categories in  nc_5\n"
     ]
    }
   ],
   "source": [
    "cd = CategoriesDroper(categoricals)\n",
    "cd.fit(train)\n",
    "test = cd.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation\n",
    "\n",
    "TODO:\n",
    "* one hot encoding low modalities: keep for grid search (no ideal value found right now, try 3, 5, ... 9)\n",
    "* group rare modalities: doesn't seem to improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in categoricals:\n",
    "#    train[item] = train[item].cat.codes +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train in train, cv (will be replaced by cross validation for parameters tuning)\n",
    "# stratify ?\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = sklearn.model_selection.train_test_split(train, y_train, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio =  sum(y_train==0) / sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinations_ctr for list of variables ?\n",
    "model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "#    one_hot_max_size=3,\n",
    "#    learning_rate=0.16,\n",
    "    depth=8,\n",
    "    eval_metric=\"AUC\",\n",
    "    random_seed=42,\n",
    "    od_type='Iter',\n",
    "    od_wait=40,\n",
    "    use_best_model=True\n",
    "#    scale_pos_weight=pos_neg_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [X_train_train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_train, y_train_train,\n",
    "    cat_features=categorical_features_indices,\n",
    "    eval_set=(X_train_val, y_train_val),\n",
    "    logging_level='Verbose'  # you can uncomment this for text output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "199:\ttest: 0.7686368\tbest: 0.7686409 (198)\ttotal: 14m 41s\tremaining: 0us (nlp, lr=0.31)\n",
    "199:\ttest: 0.7686109\tbest: 0.7686109 (199)\ttotal: 14m 17s\tremaining: 0us (nlp, lr=0.31, 9 variables dropped)\n",
    "199:\ttest: 0.7729298\tbest: 0.7730353 (190)\ttotal: 22m 13s\tremaining: 0us (nlp, lr=0.31, 6 variables dropped, depth=8)\n",
    "199:\ttest: 0.7576144\tbest: 0.7576257 (197)\ttotal: 15m 42s\tremaining: 0us(nlp, lr=0.31, weighted, one_hot=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances=model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(model.feature_importances_)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(train.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, train.columns[indices[f]], model.feature_importances_[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(train.shape[1]), indices)\n",
    "plt.xlim([-1, train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what to do with cat features\n",
    "params = {'depth': [4, 7, 10],\n",
    "          'learning_rate' : [0.20, 0.31, 0.40],\n",
    "         'l2_leaf_reg': [1, 4, 9],\n",
    "         'iterations': [300]}\n",
    "cb = CatBoostClassifier()\n",
    "cb_model = GridSearchCV(cb, params, scoring=\"roc_auc\")\n",
    "cb_model.fit(train, y_train, cb__cat_features=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_objective(params):\n",
    "    model = CatBoostClassifier(\n",
    "        l2_leaf_reg=int(params['l2_leaf_reg']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        iterations=int(params['iterations']),\n",
    "        border_count=int(params['border_count']),\n",
    "        eval_metric='AUC',\n",
    "        random_seed=42,\n",
    "        logging_level='Silent'\n",
    "        #od_type='Iter',\n",
    "        #od_wait=40\n",
    "    )\n",
    "    \n",
    "    cv_data = cv(\n",
    "        Pool(train, y_train, cat_features=categorical_features_indices),\n",
    "        model.get_params(),\n",
    "        stratified=True,\n",
    "        fold_count=5\n",
    "    )\n",
    "    best_AUC = np.max(cv_data['test-AUC-mean'])\n",
    "    print(params, 'best_AUC: ', best_AUC)\n",
    "    \n",
    "    return 1 - best_AUC # as hyperopt minimises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param space example for xgboost\n",
    "#    space = {\n",
    "#             'n_estimators' : hp.quniform('n_estimators', 100, 1000, 1),\n",
    "#             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "#             'max_depth' : hp.quniform('max_depth', 1, 13, 1),\n",
    "#             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "#             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "#             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "#             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "#             'num_class' : 9,\n",
    "#             'eval_metric': 'mlogloss',\n",
    "#             'objective': 'multi:softprob',\n",
    "#             'nthread' : 6,\n",
    "#             'silent' : 1\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "params_space = {\n",
    "    #'l2_leaf_reg': hyperopt.hp.loguniform('l2_leaf_reg', -1, np.log(50)),\n",
    "    'l2_leaf_reg': hyperopt.hp.choice('l2_leaf_reg', [11.47871028241772]),\n",
    "    'learning_rate': hyperopt.hp.uniform('learning_rate', 1e-1, 8e-1),\n",
    "     #'learning_rate': hyperopt.hp.choice('learning_rate', [0.31]),\n",
    "    'iterations': hyperopt.hp.quniform('iterations', 250, 1000, 1),\n",
    "    #'iterations': hyperopt.hp.choice('iterations', [250]),\n",
    "    #'depth': hyperopt.hp.quniform('depth', 3, 9, 1),\n",
    "    'depth': hyperopt.hp.choice('depth', [7]),\n",
    "#    'ctr_border_count': hyperopt.hp.quniform('ctr_border_count', 32, 255, 1),\n",
    "    #'border_count': hyperopt.hp.quniform('border_count', 16, 255, 1)\n",
    "    'border_count': hyperopt.hp.choice('border_count', [213])\n",
    "}\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(\n",
    "    hyperopt_objective,\n",
    "    space=params_space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=8,\n",
    "    trials=trials,\n",
    "    rstate=RandomState(123)\n",
    "    \n",
    ")\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#round 1\n",
    "{'depth': 7.0, 'l2_leaf_reg': 6.687638927479829} best_AUC:  0.764339203919555\n",
    "{'depth': 8.0, 'l2_leaf_reg': 2.238434810070511} best_AUC:  0.7641940842160905\n",
    "{'depth': 9.0, 'l2_leaf_reg': 15.61615505112418} best_AUC:  0.7656179177534072\n",
    "{'depth': 7.0, 'l2_leaf_reg': 11.47871028241772} best_AUC:  0.766724797438951\n",
    "{'depth': 6.0, 'l2_leaf_reg': 2.0946141383866816} best_AUC:  0.7641940842160905\n",
    "{'depth': 5.0, 'l2_leaf_reg': 3.409700409045487} best_AUC:  0.7656396435014416\n",
    "{'l2_leaf_reg': 11.47871028241772, 'learning_rate': 7.0}\n",
    "\n",
    "#round 2\n",
    "{'border_count': 199.0, 'depth': 7, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7654693707749893\n",
    "{'border_count': 213.0, 'depth': 7, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7662068818039395\n",
    "{'border_count': 105.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7639026943327258\n",
    "{'border_count': 238.0, 'depth': 7, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.764574925126635\n",
    "{'border_count': 22.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7643064612539859\n",
    "{'border_count': 103.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7632638136706521\n",
    "{'border_count': 225.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7653002122083675\n",
    "{'border_count': 212.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7649391813119792\n",
    "{'border_count': 85.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7649657537494025\n",
    "{'border_count': 251.0, 'depth': 8, 'iterations': 250, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.31} best_AUC:  0.7649243873489558\n",
    "{'border_count': 213.0, 'depth': 0, 'iterations': 0, 'l2_leaf_reg': 0, 'learning_rate': 0}\n",
    "\n",
    "#round 3\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 693.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.6096210638782021} best_AUC:  0.767539056284941\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 526.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.701085638831484} best_AUC:  0.767228088670764\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 822.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.7947626751796161} best_AUC:  0.7647414699770261\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 775.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.571434077994127} best_AUC:  0.7686875566115616\n",
    "{'border_count': 213, 'depth': 7, 'iterations': 516.0, 'l2_leaf_reg': 11.47871028241772, 'learning_rate': 0.4938304425560929} best_AUC:  0.7680238623609622\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check quality to expect\n",
    "model = CatBoostClassifier(\n",
    "    l2_leaf_reg=int(best['l2_leaf_reg']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    iterations=500,\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    logging_level='Silent'\n",
    ")\n",
    "cv_data = cv(Pool(X, y, cat_features=categorical_features_indices), model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precise validation AUC score: {}'.format(np.max(cv_data['test-AUC-mean'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio =  sum(y_train==0) / sum(y_train==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'border_count': 213, \n",
    "          'one_hot_max_size': 5,\n",
    "#          'depth': 7,\n",
    "          'depth': 5,\n",
    "          'iterations': 200.0, \n",
    "          'l2_leaf_reg': 11.47871028241772, \n",
    "#          'learning_rate': 0.4938304425560929,\n",
    "          'scale_pos_weight': pos_neg_ratio\n",
    "         }\n",
    "\n",
    "#random_strength (default 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    **params,\n",
    "    eval_metric=\"AUC\",\n",
    "    od_type='Iter',\n",
    "    od_wait=40\n",
    ")\n",
    "\n",
    "model.fit(train, y_train, cat_features=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to disk\n",
    "#model.save_model('catboost_model.dump')\n",
    "#load\n",
    "#model = CatBoostClassifier()\n",
    "#model.load_model('catboost_model.dump');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisstion = model.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = str(datetime.datetime.now())[:-7] + '_submission.csv'\n",
    "sub = pd.Series(submisstion, name='target')\n",
    "sub.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests on cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio =  sum(y_train==0) / sum(y_train==1)\n",
    "\n",
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    border_count=128, \n",
    "    one_hot_max_size= 5,\n",
    "    depth= 5,\n",
    "    iterations= 200.0, \n",
    "    l2_leaf_reg= 11.478, \n",
    "    scale_pos_weight= pos_neg_ratio,\n",
    "    eval_metric='AUC'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = cv(\n",
    "    Pool(train, y_train, cat_features=categorical_features_indices),\n",
    "    model.get_params(),\n",
    "    stratified=True,\n",
    "    fold_count=5\n",
    ")\n",
    "\n",
    "#0.707 on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.plot(y=['test-AUC-mean', 'train-AUC-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.plot(y=['test-Logloss-mean', 'train-Logloss-mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with some randomly added na for contract variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_with_new_categ_not_in_contract = [var for var in var_with_new_categ if var not in categ_contract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanti_contract = ['PRIX_FACTURE', \n",
    "                   'CONTRAT_TARIF',\n",
    "                   'temps_depuis_debut_contrat',\n",
    "                   'temps_jusqua_fin_contrat', \n",
    "                   'temps_depuis_maj_contrat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in categ_contract + var_with_new_categ_not_in_contract:\n",
    "    try:\n",
    "        train[var] = train[var].cat.add_categories(['NAN'])\n",
    "    except ValueError as e:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "categorical_features_indices = [train.columns.get_loc(cat) for cat in categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c190f9dc43254747993705416edf4527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 3.39s\tremaining: 8m 25s\n",
      "1:\ttotal: 6.62s\tremaining: 8m 9s\n",
      "2:\ttotal: 10s\tremaining: 8m 11s\n",
      "3:\ttotal: 13.2s\tremaining: 8m 3s\n",
      "4:\ttotal: 16.5s\tremaining: 7m 59s\n",
      "5:\ttotal: 19.5s\tremaining: 7m 48s\n",
      "6:\ttotal: 22.5s\tremaining: 7m 39s\n",
      "7:\ttotal: 25.9s\tremaining: 7m 40s\n",
      "8:\ttotal: 30.2s\tremaining: 7m 53s\n",
      "9:\ttotal: 34.5s\tremaining: 8m 2s\n",
      "10:\ttotal: 38.2s\tremaining: 8m 2s\n",
      "11:\ttotal: 41.7s\tremaining: 7m 59s\n",
      "12:\ttotal: 46s\tremaining: 8m 4s\n",
      "13:\ttotal: 49.1s\tremaining: 7m 56s\n",
      "14:\ttotal: 52s\tremaining: 7m 48s\n",
      "15:\ttotal: 55.1s\tremaining: 7m 41s\n",
      "16:\ttotal: 58.2s\tremaining: 7m 35s\n",
      "17:\ttotal: 1m 1s\tremaining: 7m 30s\n",
      "18:\ttotal: 1m 4s\tremaining: 7m 26s\n",
      "19:\ttotal: 1m 7s\tremaining: 7m 21s\n",
      "20:\ttotal: 1m 11s\tremaining: 7m 19s\n",
      "21:\ttotal: 1m 15s\tremaining: 7m 19s\n",
      "22:\ttotal: 1m 18s\tremaining: 7m 14s\n",
      "23:\ttotal: 1m 21s\tremaining: 7m 8s\n",
      "24:\ttotal: 1m 24s\tremaining: 7m 2s\n",
      "25:\ttotal: 1m 27s\tremaining: 6m 58s\n",
      "26:\ttotal: 1m 30s\tremaining: 6m 53s\n",
      "27:\ttotal: 1m 35s\tremaining: 6m 58s\n",
      "28:\ttotal: 1m 40s\tremaining: 6m 57s\n",
      "29:\ttotal: 1m 43s\tremaining: 6m 52s\n",
      "30:\ttotal: 1m 47s\tremaining: 6m 50s\n",
      "31:\ttotal: 1m 50s\tremaining: 6m 48s\n",
      "32:\ttotal: 1m 54s\tremaining: 6m 44s\n",
      "33:\ttotal: 1m 57s\tremaining: 6m 41s\n",
      "34:\ttotal: 2m\tremaining: 6m 36s\n",
      "35:\ttotal: 2m 3s\tremaining: 6m 31s\n",
      "36:\ttotal: 2m 6s\tremaining: 6m 26s\n",
      "37:\ttotal: 2m 9s\tremaining: 6m 22s\n",
      "38:\ttotal: 2m 13s\tremaining: 6m 18s\n",
      "39:\ttotal: 2m 15s\tremaining: 6m 13s\n",
      "40:\ttotal: 2m 20s\tremaining: 6m 13s\n",
      "41:\ttotal: 2m 23s\tremaining: 6m 9s\n",
      "42:\ttotal: 2m 27s\tremaining: 6m 7s\n",
      "43:\ttotal: 2m 31s\tremaining: 6m 5s\n",
      "44:\ttotal: 2m 37s\tremaining: 6m 7s\n",
      "45:\ttotal: 2m 40s\tremaining: 6m 3s\n",
      "46:\ttotal: 2m 44s\tremaining: 5m 59s\n",
      "47:\ttotal: 2m 48s\tremaining: 5m 58s\n",
      "48:\ttotal: 2m 51s\tremaining: 5m 53s\n",
      "49:\ttotal: 2m 55s\tremaining: 5m 50s\n",
      "50:\ttotal: 2m 58s\tremaining: 5m 46s\n",
      "51:\ttotal: 3m 1s\tremaining: 5m 42s\n",
      "52:\ttotal: 3m 6s\tremaining: 5m 40s\n",
      "53:\ttotal: 3m 9s\tremaining: 5m 37s\n",
      "54:\ttotal: 3m 13s\tremaining: 5m 34s\n",
      "55:\ttotal: 3m 17s\tremaining: 5m 30s\n",
      "56:\ttotal: 3m 22s\tremaining: 5m 30s\n",
      "57:\ttotal: 3m 26s\tremaining: 5m 27s\n",
      "58:\ttotal: 3m 31s\tremaining: 5m 25s\n",
      "59:\ttotal: 3m 38s\tremaining: 5m 27s\n",
      "60:\ttotal: 3m 47s\tremaining: 5m 31s\n",
      "61:\ttotal: 3m 59s\tremaining: 5m 39s\n",
      "62:\ttotal: 4m 2s\tremaining: 5m 34s\n",
      "63:\ttotal: 4m 7s\tremaining: 5m 32s\n",
      "64:\ttotal: 4m 10s\tremaining: 5m 27s\n",
      "65:\ttotal: 4m 14s\tremaining: 5m 24s\n",
      "66:\ttotal: 4m 17s\tremaining: 5m 19s\n",
      "67:\ttotal: 4m 20s\tremaining: 5m 14s\n",
      "68:\ttotal: 4m 25s\tremaining: 5m 11s\n",
      "69:\ttotal: 4m 29s\tremaining: 5m 7s\n",
      "70:\ttotal: 4m 32s\tremaining: 5m 3s\n",
      "71:\ttotal: 4m 36s\tremaining: 4m 59s\n",
      "72:\ttotal: 4m 39s\tremaining: 4m 55s\n",
      "73:\ttotal: 4m 42s\tremaining: 4m 50s\n",
      "74:\ttotal: 4m 46s\tremaining: 4m 46s\n",
      "75:\ttotal: 4m 51s\tremaining: 4m 43s\n",
      "76:\ttotal: 4m 55s\tremaining: 4m 40s\n",
      "77:\ttotal: 4m 58s\tremaining: 4m 35s\n",
      "78:\ttotal: 5m 2s\tremaining: 4m 32s\n",
      "79:\ttotal: 5m 6s\tremaining: 4m 28s\n",
      "80:\ttotal: 5m 9s\tremaining: 4m 23s\n",
      "81:\ttotal: 5m 12s\tremaining: 4m 19s\n",
      "82:\ttotal: 5m 16s\tremaining: 4m 15s\n",
      "83:\ttotal: 5m 20s\tremaining: 4m 11s\n",
      "84:\ttotal: 5m 24s\tremaining: 4m 8s\n",
      "85:\ttotal: 5m 27s\tremaining: 4m 3s\n",
      "86:\ttotal: 5m 32s\tremaining: 4m\n",
      "87:\ttotal: 5m 35s\tremaining: 3m 56s\n",
      "88:\ttotal: 5m 39s\tremaining: 3m 52s\n",
      "89:\ttotal: 5m 43s\tremaining: 3m 48s\n",
      "90:\ttotal: 5m 48s\tremaining: 3m 45s\n",
      "91:\ttotal: 5m 53s\tremaining: 3m 42s\n",
      "92:\ttotal: 5m 57s\tremaining: 3m 38s\n",
      "93:\ttotal: 6m 1s\tremaining: 3m 35s\n",
      "94:\ttotal: 6m 5s\tremaining: 3m 31s\n",
      "95:\ttotal: 6m 9s\tremaining: 3m 27s\n",
      "96:\ttotal: 6m 13s\tremaining: 3m 23s\n",
      "97:\ttotal: 6m 16s\tremaining: 3m 19s\n",
      "98:\ttotal: 6m 19s\tremaining: 3m 15s\n",
      "99:\ttotal: 6m 23s\tremaining: 3m 11s\n",
      "100:\ttotal: 6m 26s\tremaining: 3m 7s\n",
      "101:\ttotal: 6m 30s\tremaining: 3m 3s\n",
      "102:\ttotal: 6m 33s\tremaining: 2m 59s\n",
      "103:\ttotal: 6m 36s\tremaining: 2m 55s\n",
      "104:\ttotal: 6m 39s\tremaining: 2m 51s\n",
      "105:\ttotal: 6m 44s\tremaining: 2m 47s\n",
      "106:\ttotal: 6m 47s\tremaining: 2m 43s\n",
      "107:\ttotal: 6m 50s\tremaining: 2m 39s\n",
      "108:\ttotal: 6m 53s\tremaining: 2m 35s\n",
      "109:\ttotal: 6m 56s\tremaining: 2m 31s\n",
      "110:\ttotal: 7m\tremaining: 2m 27s\n",
      "111:\ttotal: 7m 4s\tremaining: 2m 23s\n",
      "112:\ttotal: 7m 7s\tremaining: 2m 19s\n",
      "113:\ttotal: 7m 10s\tremaining: 2m 15s\n",
      "114:\ttotal: 7m 14s\tremaining: 2m 12s\n",
      "115:\ttotal: 7m 18s\tremaining: 2m 8s\n",
      "116:\ttotal: 7m 22s\tremaining: 2m 4s\n",
      "117:\ttotal: 7m 26s\tremaining: 2m 1s\n",
      "118:\ttotal: 7m 29s\tremaining: 1m 57s\n",
      "119:\ttotal: 7m 33s\tremaining: 1m 53s\n",
      "120:\ttotal: 7m 38s\tremaining: 1m 49s\n",
      "121:\ttotal: 7m 42s\tremaining: 1m 46s\n",
      "122:\ttotal: 7m 45s\tremaining: 1m 42s\n",
      "123:\ttotal: 7m 48s\tremaining: 1m 38s\n",
      "124:\ttotal: 7m 50s\tremaining: 1m 34s\n",
      "125:\ttotal: 7m 54s\tremaining: 1m 30s\n",
      "126:\ttotal: 7m 59s\tremaining: 1m 26s\n",
      "127:\ttotal: 8m 2s\tremaining: 1m 22s\n",
      "128:\ttotal: 8m 7s\tremaining: 1m 19s\n",
      "129:\ttotal: 8m 11s\tremaining: 1m 15s\n",
      "130:\ttotal: 8m 14s\tremaining: 1m 11s\n",
      "131:\ttotal: 8m 17s\tremaining: 1m 7s\n",
      "132:\ttotal: 8m 21s\tremaining: 1m 4s\n",
      "133:\ttotal: 8m 25s\tremaining: 1m\n",
      "134:\ttotal: 8m 29s\tremaining: 56.6s\n",
      "135:\ttotal: 8m 33s\tremaining: 52.9s\n",
      "136:\ttotal: 8m 36s\tremaining: 49.1s\n",
      "137:\ttotal: 8m 40s\tremaining: 45.2s\n",
      "138:\ttotal: 8m 44s\tremaining: 41.5s\n",
      "139:\ttotal: 8m 48s\tremaining: 37.7s\n",
      "140:\ttotal: 8m 51s\tremaining: 33.9s\n",
      "141:\ttotal: 8m 54s\tremaining: 30.1s\n",
      "142:\ttotal: 8m 58s\tremaining: 26.3s\n",
      "143:\ttotal: 9m 1s\tremaining: 22.6s\n",
      "144:\ttotal: 9m 5s\tremaining: 18.8s\n",
      "145:\ttotal: 9m 8s\tremaining: 15s\n",
      "146:\ttotal: 9m 12s\tremaining: 11.3s\n",
      "147:\ttotal: 9m 15s\tremaining: 7.5s\n",
      "148:\ttotal: 9m 19s\tremaining: 3.75s\n",
      "149:\ttotal: 9m 22s\tremaining: 0us\n",
      "0:\ttotal: 3.48s\tremaining: 8m 38s\n",
      "1:\ttotal: 6.81s\tremaining: 8m 24s\n",
      "2:\ttotal: 10s\tremaining: 8m 11s\n",
      "3:\ttotal: 13.5s\tremaining: 8m 11s\n",
      "4:\ttotal: 16.7s\tremaining: 8m 4s\n",
      "5:\ttotal: 20.4s\tremaining: 8m 10s\n",
      "6:\ttotal: 24.4s\tremaining: 8m 18s\n",
      "7:\ttotal: 27.7s\tremaining: 8m 10s\n",
      "8:\ttotal: 31s\tremaining: 8m 5s\n",
      "9:\ttotal: 34.6s\tremaining: 8m 4s\n",
      "10:\ttotal: 37.6s\tremaining: 7m 55s\n",
      "11:\ttotal: 42.8s\tremaining: 8m 12s\n",
      "12:\ttotal: 46.5s\tremaining: 8m 9s\n",
      "13:\ttotal: 50.3s\tremaining: 8m 8s\n",
      "14:\ttotal: 54.4s\tremaining: 8m 9s\n",
      "15:\ttotal: 57.6s\tremaining: 8m 2s\n",
      "16:\ttotal: 1m 1s\tremaining: 7m 57s\n",
      "17:\ttotal: 1m 5s\tremaining: 8m 1s\n",
      "18:\ttotal: 1m 9s\tremaining: 7m 56s\n",
      "19:\ttotal: 1m 15s\tremaining: 8m 10s\n",
      "20:\ttotal: 1m 19s\tremaining: 8m 10s\n",
      "21:\ttotal: 1m 24s\tremaining: 8m 10s\n",
      "22:\ttotal: 1m 27s\tremaining: 8m 5s\n",
      "23:\ttotal: 1m 31s\tremaining: 8m 2s\n",
      "24:\ttotal: 1m 35s\tremaining: 7m 57s\n",
      "25:\ttotal: 1m 38s\tremaining: 7m 51s\n",
      "26:\ttotal: 1m 42s\tremaining: 7m 45s\n",
      "27:\ttotal: 1m 46s\tremaining: 7m 42s\n",
      "28:\ttotal: 1m 49s\tremaining: 7m 35s\n",
      "29:\ttotal: 1m 56s\tremaining: 7m 47s\n",
      "30:\ttotal: 2m\tremaining: 7m 44s\n",
      "31:\ttotal: 2m 4s\tremaining: 7m 40s\n",
      "32:\ttotal: 2m 8s\tremaining: 7m 36s\n",
      "33:\ttotal: 2m 12s\tremaining: 7m 30s\n",
      "34:\ttotal: 2m 15s\tremaining: 7m 25s\n",
      "35:\ttotal: 2m 19s\tremaining: 7m 22s\n",
      "36:\ttotal: 2m 24s\tremaining: 7m 20s\n",
      "37:\ttotal: 2m 30s\tremaining: 7m 23s\n",
      "38:\ttotal: 2m 36s\tremaining: 7m 24s\n",
      "39:\ttotal: 2m 41s\tremaining: 7m 23s\n",
      "40:\ttotal: 2m 46s\tremaining: 7m 22s\n",
      "41:\ttotal: 2m 51s\tremaining: 7m 20s\n",
      "42:\ttotal: 2m 54s\tremaining: 7m 15s\n",
      "43:\ttotal: 2m 57s\tremaining: 7m 8s\n",
      "44:\ttotal: 3m 2s\tremaining: 7m 6s\n",
      "45:\ttotal: 3m 7s\tremaining: 7m 2s\n",
      "46:\ttotal: 3m 11s\tremaining: 6m 58s\n",
      "47:\ttotal: 3m 14s\tremaining: 6m 53s\n",
      "48:\ttotal: 3m 17s\tremaining: 6m 47s\n",
      "49:\ttotal: 3m 21s\tremaining: 6m 43s\n",
      "50:\ttotal: 3m 25s\tremaining: 6m 38s\n",
      "51:\ttotal: 3m 29s\tremaining: 6m 35s\n",
      "52:\ttotal: 3m 32s\tremaining: 6m 29s\n",
      "53:\ttotal: 3m 38s\tremaining: 6m 29s\n",
      "54:\ttotal: 3m 42s\tremaining: 6m 24s\n",
      "55:\ttotal: 3m 46s\tremaining: 6m 19s\n",
      "56:\ttotal: 3m 51s\tremaining: 6m 17s\n",
      "57:\ttotal: 3m 55s\tremaining: 6m 13s\n",
      "58:\ttotal: 3m 58s\tremaining: 6m 7s\n",
      "59:\ttotal: 4m 2s\tremaining: 6m 3s\n",
      "60:\ttotal: 4m 6s\tremaining: 5m 59s\n",
      "61:\ttotal: 4m 10s\tremaining: 5m 55s\n",
      "62:\ttotal: 4m 13s\tremaining: 5m 50s\n",
      "63:\ttotal: 4m 17s\tremaining: 5m 45s\n",
      "64:\ttotal: 4m 20s\tremaining: 5m 40s\n",
      "65:\ttotal: 4m 23s\tremaining: 5m 34s\n",
      "66:\ttotal: 4m 26s\tremaining: 5m 29s\n",
      "67:\ttotal: 4m 29s\tremaining: 5m 25s\n",
      "68:\ttotal: 4m 33s\tremaining: 5m 20s\n",
      "69:\ttotal: 4m 36s\tremaining: 5m 16s\n",
      "70:\ttotal: 4m 40s\tremaining: 5m 12s\n",
      "71:\ttotal: 4m 44s\tremaining: 5m 7s\n",
      "72:\ttotal: 4m 47s\tremaining: 5m 2s\n",
      "73:\ttotal: 4m 51s\tremaining: 4m 59s\n",
      "74:\ttotal: 4m 55s\tremaining: 4m 55s\n",
      "75:\ttotal: 5m\tremaining: 4m 52s\n",
      "76:\ttotal: 5m 4s\tremaining: 4m 48s\n",
      "77:\ttotal: 5m 9s\tremaining: 4m 45s\n",
      "78:\ttotal: 5m 13s\tremaining: 4m 41s\n",
      "79:\ttotal: 5m 22s\tremaining: 4m 41s\n",
      "80:\ttotal: 5m 25s\tremaining: 4m 37s\n",
      "81:\ttotal: 5m 29s\tremaining: 4m 33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82:\ttotal: 5m 32s\tremaining: 4m 28s\n",
      "83:\ttotal: 5m 36s\tremaining: 4m 24s\n",
      "84:\ttotal: 5m 39s\tremaining: 4m 19s\n",
      "85:\ttotal: 5m 43s\tremaining: 4m 15s\n",
      "86:\ttotal: 5m 46s\tremaining: 4m 10s\n",
      "87:\ttotal: 5m 49s\tremaining: 4m 6s\n",
      "88:\ttotal: 5m 53s\tremaining: 4m 2s\n",
      "89:\ttotal: 5m 57s\tremaining: 3m 58s\n",
      "90:\ttotal: 6m\tremaining: 3m 53s\n",
      "91:\ttotal: 6m 4s\tremaining: 3m 49s\n",
      "92:\ttotal: 6m 7s\tremaining: 3m 45s\n",
      "93:\ttotal: 6m 12s\tremaining: 3m 41s\n",
      "94:\ttotal: 6m 17s\tremaining: 3m 38s\n",
      "95:\ttotal: 6m 22s\tremaining: 3m 34s\n",
      "96:\ttotal: 6m 25s\tremaining: 3m 30s\n",
      "97:\ttotal: 6m 29s\tremaining: 3m 26s\n",
      "98:\ttotal: 6m 32s\tremaining: 3m 22s\n",
      "99:\ttotal: 6m 36s\tremaining: 3m 18s\n",
      "100:\ttotal: 6m 41s\tremaining: 3m 14s\n",
      "101:\ttotal: 6m 46s\tremaining: 3m 11s\n",
      "102:\ttotal: 6m 50s\tremaining: 3m 7s\n",
      "103:\ttotal: 6m 54s\tremaining: 3m 3s\n",
      "104:\ttotal: 6m 57s\tremaining: 2m 58s\n",
      "105:\ttotal: 7m\tremaining: 2m 54s\n",
      "106:\ttotal: 7m 3s\tremaining: 2m 50s\n",
      "107:\ttotal: 7m 8s\tremaining: 2m 46s\n",
      "108:\ttotal: 7m 11s\tremaining: 2m 42s\n",
      "109:\ttotal: 7m 14s\tremaining: 2m 37s\n",
      "110:\ttotal: 7m 18s\tremaining: 2m 33s\n",
      "111:\ttotal: 7m 22s\tremaining: 2m 30s\n",
      "112:\ttotal: 7m 26s\tremaining: 2m 26s\n",
      "113:\ttotal: 7m 29s\tremaining: 2m 21s\n",
      "114:\ttotal: 7m 33s\tremaining: 2m 17s\n",
      "115:\ttotal: 7m 36s\tremaining: 2m 13s\n",
      "116:\ttotal: 7m 43s\tremaining: 2m 10s\n",
      "117:\ttotal: 7m 48s\tremaining: 2m 6s\n",
      "118:\ttotal: 7m 51s\tremaining: 2m 2s\n",
      "119:\ttotal: 7m 55s\tremaining: 1m 58s\n",
      "120:\ttotal: 8m\tremaining: 1m 55s\n",
      "121:\ttotal: 8m 5s\tremaining: 1m 51s\n",
      "122:\ttotal: 8m 8s\tremaining: 1m 47s\n",
      "123:\ttotal: 8m 12s\tremaining: 1m 43s\n",
      "124:\ttotal: 8m 16s\tremaining: 1m 39s\n",
      "125:\ttotal: 8m 20s\tremaining: 1m 35s\n",
      "126:\ttotal: 8m 23s\tremaining: 1m 31s\n",
      "127:\ttotal: 8m 26s\tremaining: 1m 27s\n",
      "128:\ttotal: 8m 30s\tremaining: 1m 23s\n",
      "129:\ttotal: 8m 33s\tremaining: 1m 19s\n",
      "130:\ttotal: 8m 37s\tremaining: 1m 15s\n",
      "131:\ttotal: 8m 40s\tremaining: 1m 10s\n",
      "132:\ttotal: 8m 44s\tremaining: 1m 7s\n",
      "133:\ttotal: 8m 47s\tremaining: 1m 3s\n",
      "134:\ttotal: 8m 51s\tremaining: 59s\n",
      "135:\ttotal: 8m 56s\tremaining: 55.2s\n",
      "136:\ttotal: 9m 2s\tremaining: 51.4s\n",
      "137:\ttotal: 9m 5s\tremaining: 47.4s\n",
      "138:\ttotal: 9m 9s\tremaining: 43.5s\n",
      "139:\ttotal: 9m 12s\tremaining: 39.5s\n",
      "140:\ttotal: 9m 16s\tremaining: 35.5s\n",
      "141:\ttotal: 9m 19s\tremaining: 31.5s\n",
      "142:\ttotal: 9m 23s\tremaining: 27.6s\n",
      "143:\ttotal: 9m 26s\tremaining: 23.6s\n",
      "144:\ttotal: 9m 29s\tremaining: 19.7s\n",
      "145:\ttotal: 9m 33s\tremaining: 15.7s\n",
      "146:\ttotal: 9m 37s\tremaining: 11.8s\n",
      "147:\ttotal: 9m 40s\tremaining: 7.85s\n",
      "148:\ttotal: 9m 44s\tremaining: 3.92s\n",
      "149:\ttotal: 9m 47s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "#np.random.seed(42)\n",
    "for i in tqdm.tqdm_notebook(range(2)):\n",
    "    X_train = train.copy()\n",
    "    \n",
    "    #randomly put missing contract in train data\n",
    "    _idx = np.random.choice(X_train.index, size=X_train.shape[0]//20, replace=False)\n",
    "    X_train.loc[_idx, categ_contract] = 'NAN'\n",
    "    X_train.loc[_idx, quanti_contract] = -9999\n",
    "    \n",
    "    #randomly put missing in variables with new categories in test\n",
    "    for var in var_with_new_categ_not_in_contract:\n",
    "        _idx = np.random.choice(X_train.index, size=X_train.shape[0]//20, replace=False)\n",
    "        X_train.loc[_idx, var] = 'NAN'\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=150,\n",
    "        depth=6,\n",
    "        border_count=128,\n",
    "        learning_rate=0.30,\n",
    "        eval_metric='AUC'\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "    models.append(model.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33a09f90e2348b780e52d40b4a3eda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "for _model in tqdm.tqdm_notebook(models):\n",
    "    predictions.append(_model.predict_proba(test)[:,1])\n",
    "    \n",
    "predictions = np.vstack(predictions).T.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = np.hstack([predictions, predictions2]).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid1 = {'max_depth':[10],\n",
    "               'min_child_weight':[2]}\n",
    "\n",
    "model_xgb = XGBClassifier(learning_rate=0.1, \n",
    "                      n_estimators=200, \n",
    "                      max_depth=10,\n",
    "                      min_child_weight=1, \n",
    "                      gamma=0, \n",
    "                      subsample=0.8, \n",
    "                      colsample_bytree=0.8,\n",
    "                      objective= 'binary:logistic', \n",
    "                      nthread=7, \n",
    "                      scale_pos_weight=1, \n",
    "                      eval_metric='auc',\n",
    "                      seed=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(X_train_train, y_train_train, eval_set=[(X_train_val, y_train_val)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbm = lightgbm.LGBMClassifier(\n",
    "    seed=np.random.randint(10**10),\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    num_leaves=200,\n",
    "    min_data_in_leaf=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    #is_unbalance=True,\n",
    "    objective='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbm.fit(X_train_train, y_train_train, \n",
    "              eval_set=[(X_train_val, y_train_val)], \n",
    "              eval_metric='auc', \n",
    "              categorical_feature=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsc_hs]",
   "language": "python",
   "name": "conda-env-dsc_hs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
