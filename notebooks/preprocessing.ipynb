{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created 22/4/2017\n",
    "@author: Marios Michailidis\n",
    "Script that prepares 2 data sets to get a top 11 position in the Amazon Classification Challenge:\n",
    "Link:https://www.kaggle.com/c/amazon-employee-access-challenge\n",
    "First source of data is via selecting the best up for up to 4-way interractions fo all\n",
    "categorical variables ysing a linear model. Then the results are printes as sparse files\n",
    "The scource are counts and likelihood features created per fold for up to 3way interractions\n",
    "( no feature selection ) .This produces 5 pairs of train/cv files that will be used to do\n",
    "the stacking semi-manually . E.g. in the first level no kfolding will take place.\n",
    "\"\"\"\n",
    "\n",
    "#amazon helper functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csr_matrix,csc_matrix\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Converts a dataset to weights of evidence (actuall computations) :\n",
    "Good explanation here :http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/\n",
    "These are likelihood type of features\n",
    "\"\"\"\n",
    "\n",
    "def convert_dataset_to_woe(xc,yc,xt, rounding=2,cols=None):\n",
    "    xc=xc.tolist()\n",
    "    xt=xt.tolist()\n",
    "    yc=yc.tolist()\n",
    "    if cols==None:\n",
    "        cols=[k for k in range(0,len(xc[0]))]\n",
    "    woe=[ [0.0 for k in range(0,len(cols))] for g in range(0,len(xt))]\n",
    "    good=[]\n",
    "    bads=[]\n",
    "    for col in cols:\n",
    "        dictsgoouds=defaultdict(int)\n",
    "        dictsbads=defaultdict(int)\n",
    "        good.append(dictsgoouds)\n",
    "        bads.append(dictsbads)\n",
    "    total_goods=0\n",
    "    total_bads =0\n",
    "\n",
    "    for a in range (0,len(xc)):\n",
    "        target=yc[a]\n",
    "        if target>0.0:\n",
    "            total_goods+=1.0\n",
    "        else :\n",
    "            total_bads+=1.0\n",
    "        for j in range(0,len(cols)):\n",
    "            col=cols[j]\n",
    "            if target>0:\n",
    "                good[j][xc[a][col]]+=1.0\n",
    "            else :\n",
    "                bads[j][xc[a][col]]+=1.0\n",
    "    #print(total_goods,total_bads)\n",
    "\n",
    "    for a in range (0,len(xt)):\n",
    "        for j in range(0,len(cols)):\n",
    "            col=cols[j]\n",
    "            tempgood=0.0\n",
    "            tempbad=0.0\n",
    "            if xt[a][col] in good[j]:\n",
    "                tempgood=float(good[j][xt[a][col]])\n",
    "            if xt[a][col] in bads[j]:\n",
    "                tempbad=float(bads[j][xt[a][col]])\n",
    "            if tempgood>0.0 and tempbad>0.0:\n",
    "                #print(tempgood,tempbad)\n",
    "                woe[a][j]=round(np.log((tempgood/total_goods) /(tempbad/total_bads)) , rounding)\n",
    "            elif tempgood>0.0 :\n",
    "                 woe[a][j]=3.0\n",
    "            elif tempbad>0.0:\n",
    "                woe[a][j]=-3.0\n",
    "            else :\n",
    "                 woe[a][j]=round(np.log(0.9421/0.0579))\n",
    "    return woe\n",
    "\n",
    "\"\"\"\n",
    "Converts a dataset to weights of evidence (general):\n",
    "Good explanation here :http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/\n",
    "These are likelihood type of features\n",
    "\"\"\"\n",
    "def convert_to_woe(X,y, Xt, seed=1, cvals=5, roundings=2, columns=None):\n",
    "\n",
    "    if columns==None:\n",
    "        columns=[k for k in range(0,(X.shape[1]))]\n",
    "\n",
    "    X=X.tolist()\n",
    "    Xt=Xt.tolist()\n",
    "    woetrain=[ [0.0 for k in range(0,len(X[0]))] for g in range(0,len(X))]\n",
    "    woetest=[ [0.0 for k in range(0,len(X[0]))] for g in range(0,len(Xt))]\n",
    "\n",
    "    kfolder=StratifiedKFold(y, n_folds=cvals,shuffle=True, random_state=seed)\n",
    "    for train_index, test_index in kfolder:\n",
    "        # creaning and validation sets\n",
    "        X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train = np.array(y)[train_index]\n",
    "\n",
    "        woecv=convert_dataset_to_woe(X_train,y_train,X_cv, rounding=roundings,cols=columns)\n",
    "        X_cv=X_cv.tolist()\n",
    "        no=0\n",
    "        for real_index in test_index:\n",
    "            for j in range(0,len(X_cv[0])):\n",
    "                woetrain[real_index][j]=X_cv[no][j]\n",
    "            no+=1\n",
    "        no=0\n",
    "        for real_index in test_index:\n",
    "            for j in range(0,len(columns)):\n",
    "                col=columns[j]\n",
    "                woetrain[real_index][col]=woecv[no][j]\n",
    "            no+=1\n",
    "    woefinal=convert_dataset_to_woe(np.array(X),np.array(y),np.array(Xt), rounding=roundings,cols=columns)\n",
    "\n",
    "    for real_index in range(0,len(Xt)):\n",
    "        for j in range(0,len(Xt[0])):\n",
    "            woetest[real_index][j]=Xt[real_index][j]\n",
    "\n",
    "    for real_index in range(0,len(Xt)):\n",
    "        for j in range(0,len(columns)):\n",
    "            col=columns[j]\n",
    "            woetest[real_index][col]=woefinal[real_index][j]\n",
    "\n",
    "    return np.array(woetrain), np.array(woetest)\n",
    "\n",
    "\"\"\"\n",
    "converts sparse data to StackNet format\n",
    "Better use this one than standard svmlight.\n",
    "\"\"\"\n",
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):\n",
    "    zsparse=csr_matrix(csc_matrix(array))\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))\n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "\n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget is not None:\n",
    "             f.write(str(ytarget[b]) + deli1)\n",
    "\n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))\n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:\n",
    "            print(\" row : %d \" % (counter_row))\n",
    "    f.close()\n",
    "\n",
    "\"\"\"\n",
    "Load training and test data. Then create in a brute force way to cerate all possible 4-way\n",
    "categorical interractions and test whether auc improves when adding them.\n",
    "Once it finds the best interractions, it prints them as sparse data\n",
    "as:\n",
    "    train.sparse\n",
    "    test.sparse\n",
    "\"\"\"\n",
    "\n",
    "def create_4way_interractions(path=\"\"):\n",
    "\n",
    "    train_df=pd.read_pickle(path + \"train_preproc_1_.pickle\")\n",
    "    test_df=pd.read_pickle(path + \"test_preproc_1_.pickle\")\n",
    "#    train_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "\n",
    "    y=np.array(train_df['target'])\n",
    "    train_df.drop(\"target\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    cat_columns = list(train_df.select_dtypes(include=['int8', 'int16', 'int32']).columns)\n",
    "    for quanti_int in ['CONTRAT_TARIF', 'PRIX_FACTURE']:\n",
    "        cat_columns.remove(quanti_int)\n",
    "\n",
    "    cat_columns_idx = [train_df.columns.get_loc(cat) for cat in cat_columns]\n",
    "    cat_columns = [cat_columns[k] for k in range(0,len(cat_columns))] # ??\n",
    "\n",
    "    kfolder = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=1)\n",
    "\n",
    "    grand_auc = 0\n",
    "\n",
    "    X = np.array(train_df)\n",
    "    i=0 # iterator counter\n",
    "    model = SGDClassifier(loss='log', penalty='l2', alpha=0.0000225, max_iter=50, random_state=1)\n",
    "\n",
    "    for train_index, test_index in kfolder:\n",
    "            X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "            y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
    "            one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "            one.fit(X_train)\n",
    "            X_train=one.transform(X_train)\n",
    "            X_cv=one.transform(X_cv)\n",
    "            model.fit(X_train,y_train)\n",
    "            preds=model.predict_proba(X_cv)[:,1]\n",
    "            auc=roc_auc_score(y_cv,preds)\n",
    "            print (\" fold %d/%d auc %f \" % (i+1, 5, auc))\n",
    "            grand_auc+=auc\n",
    "            i+=1\n",
    "    grand_auc/=5\n",
    "    print (\"grand AUC is %f \" % (grand_auc))\n",
    "\n",
    "#    cat_columns = train_df.select_dtypes(include=['category', 'object']).columns\n",
    "#    cat_columns_idx = [train_df.columns.get_loc(cat) for cat in cat_columns]\n",
    "#    cat_columns = [cat_columns[k] for k in range(0, len(cat_columns))] # ??\n",
    "\n",
    "    cols=[k for k in cat_columns] #??\n",
    "    newcols=cols[:]\n",
    "    print(cols)\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "                name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]\n",
    "                cols.append(name1)\n",
    "\n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x))\n",
    "                cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "\n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "                mean_auc=0\n",
    "                X=np.array(train_df)\n",
    "                i=0 # iterator counter\n",
    "                for train_index, test_index in kfolder:\n",
    "                        X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "                        y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
    "                        one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "                        one.fit(X_train)\n",
    "                        X_train=one.transform(X_train)\n",
    "                        X_cv=one.transform(X_cv)\n",
    "                        model.fit(X_train, y_train)\n",
    "                        preds=model.predict_proba(X_cv)[:, 1]\n",
    "                        auc=roc_auc_score(y_cv,preds)\n",
    "                        print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "                        mean_auc+=auc\n",
    "                        i+=1\n",
    "                mean_auc/=5\n",
    "                if (mean_auc > grand_auc + 0.00001):\n",
    "                    print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "                    grand_auc=mean_auc\n",
    "                    newcols.append(name1)\n",
    "                else :\n",
    "                    print( \"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "                    train_df.drop(name1, inplace=True,axis=1)\n",
    "                    test_df.drop(name1, inplace=True,axis=1)\n",
    "                    cat_columns_idx = cat_columns_idx[:-1]\n",
    "\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):\n",
    "                name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]\n",
    "                cols.append(name1)\n",
    "\n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x))\n",
    "                cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "\n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "                mean_auc=0\n",
    "                X=np.array(train_df)\n",
    "                i=0 # iterator counter\n",
    "                for train_index, test_index in kfolder:\n",
    "                        X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "                        y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
    "                        one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "                        one.fit(X_train)\n",
    "                        X_train=one.transform(X_train)\n",
    "                        X_cv=one.transform(X_cv)\n",
    "                        model.fit(X_train,y_train)\n",
    "                        preds=model.predict_proba(X_cv)[:,1]\n",
    "                        auc=roc_auc_score(y_cv,preds)\n",
    "                        print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "                        mean_auc+=auc\n",
    "                        i+=1\n",
    "                mean_auc/=5\n",
    "                if (mean_auc>grand_auc+0.00001):\n",
    "                    print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "                    grand_auc=mean_auc\n",
    "                    newcols.append(name1)\n",
    "                else :\n",
    "                    print(\"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "                    train_df.drop(name1, inplace=True,axis=1)\n",
    "                    test_df.drop(name1, inplace=True,axis=1)\n",
    "                    cat_columns_idx = cat_columns_idx[:-1]\n",
    "    #\n",
    "    # for j1 in range(0,len(cat_columns)):\n",
    "    #     for j2 in range(j1+1,len(cat_columns)):\n",
    "    #         for j3 in range(j2+1,len(cat_columns)):\n",
    "    #             for j4 in range(j3+1,len(cat_columns)):\n",
    "    #                 name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]+ \"_plus_\" + cat_columns[j4]\n",
    "    #                 cols.append(name1)\n",
    "    #\n",
    "    #                 train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j4]].apply(lambda x:str(x))\n",
    "    #                 test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j4]].apply(lambda x:str(x))\n",
    "    #                 cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "    #\n",
    "    #                 lbl = LabelEncoder()\n",
    "    #                 lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    #                 train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    #                 test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "    #\n",
    "    #                 mean_auc=0\n",
    "    #                 X=np.array(train_df)\n",
    "    #                 i=0 # iterator counter\n",
    "    #                 for train_index, test_index in kfolder:\n",
    "    #                         X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    #                         y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    #                         one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "    #                         one.fit(X_train)\n",
    "    #                         X_train=one.transform(X_train)\n",
    "    #                         X_cv=one.transform(X_cv)\n",
    "    #                         model.fit(X_train,y_train)\n",
    "    #                         preds=model.predict_proba(X_cv)[:,1]\n",
    "    #                         auc=roc_auc_score(y_cv,preds)\n",
    "    #                         print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "    #                         mean_auc+=auc\n",
    "    #                         i+=1\n",
    "    #                 mean_auc/=5\n",
    "    #                 if (mean_auc>grand_auc+0.00001):\n",
    "    #                     print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "    #                     grand_auc=mean_auc\n",
    "    #                     newcols.append(name1)\n",
    "    #                 else :\n",
    "    #                     print( \"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "    #                     train_df.drop(name1, inplace=True,axis=1)\n",
    "    #                     test_df.drop(name1, inplace=True,axis=1)\n",
    "    #                     cat_columns_idx = cat_columns_idx[:-1]\n",
    "    #\n",
    "    # for j1 in range(0,len(cat_columns)):\n",
    "    #     for j2 in range(j1+1,len(cat_columns)):\n",
    "    #         for j3 in range(j2+1,len(cat_columns)):\n",
    "    #             for j4 in range(j3+1,len(cat_columns)):\n",
    "    #                 for j5 in range(j4+1,len(cat_columns)):\n",
    "    #                     name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]+ \"_plus_\" + cat_columns[j4]+ \"_plus_\" + cat_columns[j5]\n",
    "    #                     cols.append(name1)\n",
    "    #\n",
    "    #                     train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j4]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j5]].apply(lambda x:str(x))\n",
    "    #                     test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j4]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j5]].apply(lambda x:str(x))\n",
    "    #                     cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "    #\n",
    "    #                     lbl = LabelEncoder()\n",
    "    #                     lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    #                     train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    #                     test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "    #\n",
    "    #                     mean_auc=0\n",
    "    #                     X=np.array(train_df)\n",
    "    #                     i=0 # iterator counter\n",
    "    #                     for train_index, test_index in kfolder:\n",
    "    #                             X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    #                             y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    #                             one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "    #                             one.fit(X_train)\n",
    "    #                             X_train=one.transform(X_train)\n",
    "    #                             X_cv=one.transform(X_cv)\n",
    "    #                             model.fit(X_train,y_train)\n",
    "    #                             preds=model.predict_proba(X_cv)[:,1]\n",
    "    #                             auc=roc_auc_score(y_cv,preds)\n",
    "    #                             print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "    #                             mean_auc+=auc\n",
    "    #                             i+=1\n",
    "    #                     mean_auc/=5\n",
    "    #                     if (mean_auc>grand_auc+0.00001):\n",
    "    #                         print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "    #                         grand_auc=mean_auc\n",
    "    #                         newcols.append(name1)\n",
    "    #                     else :\n",
    "    #                         print( \"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "    #                         train_df.drop(name1, inplace=True,axis=1)\n",
    "    #                         test_df.drop(name1, inplace=True,axis=1)\n",
    "    #                         cat_columns_idx = cat_columns_idx[:-1]\n",
    "\n",
    "    #train_df.to_csv(\"trainid.csv\",index=False)\n",
    "    #test_df.to_csv(\"testid.csv\",index=False)\n",
    "    train_df.to_pickle(\"trainid.pickle\")\n",
    "    test_df.to_pickle(\"testid.pickle\")\n",
    "\n",
    "    print (\"one hot encoding\")\n",
    "    train=np.array(train_df)\n",
    "    test=np.array(test_df)\n",
    "\n",
    "    for j in range(0,train.shape[1]):\n",
    "        dicter=defaultdict(lambda:0)\n",
    "        for i in range(0,train.shape[0]):\n",
    "            dicter[str(train[i,j])]+=1\n",
    "        for i in range(0,test.shape[0]):\n",
    "            dicter[str(test[i,j])]+=1\n",
    "        for i in range(0,train.shape[0]):\n",
    "            train[i,j]=9999999 if dicter[str(train[i,j])]<=1 else  train[i,j]\n",
    "        for i in range(0,test.shape[0]):\n",
    "            test[i,j]=9999999 if dicter[str(test[i,j])]<=1 else  test[i,j]\n",
    "\n",
    "    one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx, sparse=True)\n",
    "    test=one.fit_transform(test)\n",
    "    train=one.transform(train)\n",
    "    test=csr_matrix(test)\n",
    "    train=csr_matrix(train)\n",
    "    fromsparsetofile(path + \"train.sparse\", train, deli1=\" \", deli2=\":\",ytarget=y)\n",
    "    fromsparsetofile(path + \"test.sparse\", test, deli1=\" \", deli2=\":\",ytarget=None)\n",
    "    print (train.shape)\n",
    "    print (test.shape)\n",
    "\n",
    "\n",
    "    print (\"counts\")\n",
    "    result = pd.concat([train_df,test_df])\n",
    "    for f in newcols:\n",
    "                cases=defaultdict(int)\n",
    "                temp=np.array(result[f]).tolist()\n",
    "                for k in temp:\n",
    "                    cases[k]+=1\n",
    "                print (f, len(cases))\n",
    "\n",
    "                train_df[f]=train_df[f].apply(lambda x: cases[x])\n",
    "                test_df[f]=test_df[f].apply(lambda x: cases[x])\n",
    "\n",
    "    train_df.to_pickle(\"traincount.pickle\")\n",
    "    test_df.to_pickle(\"testcount.pickle\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Computes all possible 3-way interractions\n",
    "and finds the counts of each category\n",
    "Then it perfoms k-fold and produces likelihood (woe)\n",
    "values for all features and stacks them next to the counts. Then\n",
    "it prints them in dense format as  :\n",
    "    amazon_counts_train\" + str(fold_number) + \".txt\"\n",
    "    amazon_counts_cv\" + str(fold_number) + \".txt\"\n",
    "It also produces an amazon_counts_train.txt and amazon_counts_test.txt file too.\n",
    "(so 12 in total - 5 pairs of train/cv and a final train and test file)\n",
    "The aim is to prepare StackNet to run stacking with our own folds.\n",
    "The data is also standardized.\n",
    "\"\"\"\n",
    "\n",
    "def create_likelihoods_with_counts(path=\"\"):\n",
    "\n",
    "    number_of_folds=5\n",
    "    SEED=15\n",
    "#    train_df=pd.read_csv(path + \"train.csv\")\n",
    "#    test_df=pd.read_csv(path + \"test.csv\")\n",
    "#    train_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "\n",
    "#    y=np.array(train_df['ACTION'])\n",
    "\n",
    "#    train_df.drop(\"ACTION\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "    train_df=pd.read_pickle(path + \"train_preproc_1_.pickle\")\n",
    "    test_df=pd.read_pickle(path + \"test_preproc_1_.pickle\")\n",
    "\n",
    "    y=np.array(train_df['target'])\n",
    "    train_df.drop(\"target\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    cat_columns = list(train_df.select_dtypes(include=['int8', 'int16', 'int32']).columns)\n",
    "    for quanti_int in ['CONTRAT_TARIF', 'PRIX_FACTURE']:\n",
    "        cat_columns.remove(quanti_int)\n",
    "    cat_columns_idx = [train_df.columns.get_loc(cat) for cat in cat_columns]\n",
    "    cat_columns = [cat_columns[k] for k in range(0,len(cat_columns))] # ??\n",
    "\n",
    "#    columns=train_df.columns.values\n",
    "#    columns=[columns[k] for k in range(0,len(columns))] # we exclude the first column\n",
    "    cols=[k for k in cat_columns]\n",
    "    print(cols)\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "                name1=columns[j1] + \"_plus_\" + columns[j2]\n",
    "                cols.append(name1)\n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x))\n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):\n",
    "                name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]\n",
    "                cols.append(name1)\n",
    "\n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x))\n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):\n",
    "                for j4 in range(j3+1,len(cat_columns)):\n",
    "                    name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]+ \"_plus_\" + cat_columns[j4]\n",
    "                    cols.append(name1)\n",
    "\n",
    "                    train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j4]].apply(lambda x:str(x))\n",
    "                    test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j4]].apply(lambda x:str(x))\n",
    "                    lbl = LabelEncoder()\n",
    "                    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    X=np.array(train_df)\n",
    "    X_Test=np.array(test_df)\n",
    "\n",
    "    print (\"counts\")\n",
    "    result = pd.concat([train_df,test_df])\n",
    "    for f in cols:\n",
    "                cases=defaultdict(int)\n",
    "                temp=np.array(result[f]).tolist()\n",
    "                for k in temp:\n",
    "                    cases[k]+=1\n",
    "                print (f, len(cases))\n",
    "\n",
    "                train_df[f]=train_df[f].apply(lambda x: cases[x])\n",
    "                test_df[f]=test_df[f].apply(lambda x: cases[x])\n",
    "\n",
    "    X_count=np.array(train_df)\n",
    "    X_Test_count=np.array(test_df)\n",
    "\n",
    "    X_count[X_count<=1]=0\n",
    "    X_Test_count[X_Test_count<=1]=0\n",
    "\n",
    "\n",
    "    bigy=None\n",
    "    print(\" creating likelihoods \")\n",
    "    kfolder=StratifiedKFold(y, n_folds=number_of_folds,shuffle=True, random_state=SEED)\n",
    "    #number_of_folds=0\n",
    "    #X,y=shuffle(X,y, random_state=SEED) # Shuffle since the data is ordered by time\n",
    "    i=0 # iterator counter\n",
    "    print (\"printing files for %d kfolds \" % (number_of_folds))\n",
    "    if number_of_folds>0:\n",
    "        for train_index, test_index in kfolder:\n",
    "            X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "            X_train_count, X_cv_count = np.array(X_count)[train_index], np.array(X_count)[test_index]\n",
    "            y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "            if bigy==None      :\n",
    "                bigy=y_cv\n",
    "            else :\n",
    "                bigy=np.concatenate((bigy,y_cv))\n",
    "\n",
    "\n",
    "            X_train,X_cv= convert_to_woe(X_train,y_train , X_cv, seed=1, cvals=5, roundings=2)\n",
    "\n",
    "\n",
    "\n",
    "            temp_array_train=X_train\n",
    "            temp_array_cv=X_cv\n",
    "            temp_array_train=np.column_stack((temp_array_train,X_train_count))\n",
    "            temp_array_cv=np.column_stack((temp_array_cv,X_cv_count))\n",
    "\n",
    "            for a in range(0,temp_array_train.shape[0]):\n",
    "                for b in range(0,temp_array_train.shape[1]):\n",
    "                    if temp_array_train[a,b]>0:\n",
    "                        temp_array_train[a,b]=np.log1p(temp_array_train[a,b])\n",
    "                    else :\n",
    "                        temp_array_train[a,b]=-np.log1p(-temp_array_train[a,b])\n",
    "\n",
    "            for a in range(0,temp_array_cv.shape[0]):\n",
    "                for b in range(0,temp_array_cv.shape[1]):\n",
    "                    if temp_array_cv[a,b]>0:\n",
    "                        temp_array_cv[a,b]=np.log1p(temp_array_cv[a,b])\n",
    "                    else :\n",
    "                        temp_array_cv[a,b]= -np.log1p(-temp_array_cv[a,b])\n",
    "\n",
    "            stda=StandardScaler()\n",
    "            stda.fit(temp_array_train)\n",
    "            temp_array_train=stda.transform(temp_array_train)\n",
    "            temp_array_cv=stda.transform(temp_array_cv)\n",
    "\n",
    "            #temp_array_train=csr_matrix(temp_array_train)\n",
    "            #temp_array_cv=csr_matrix(temp_array_cv)\n",
    "            np.savetxt (\"amazon_counts_train\" + str(i) + \".txt\",np.column_stack((y_train,temp_array_train)),delimiter=\",\")\n",
    "            np.savetxt (\"amazon_counts_cv\" + str(i) + \".txt\",np.column_stack((y_cv,temp_array_cv)),delimiter=\",\")\n",
    "\n",
    "            #fromsparsetofile(\"amazon_count_train\" + str(i) + \".txt\", temp_array_train, deli1=\" \", deli2=\":\",ytarget=y_train)\n",
    "            #fromsparsetofile(\"amazon_count_cv\" + str(i) + \".txt\", temp_array_cv,deli1=\" \", deli2=\":\",ytarget=y_cv)\n",
    "\n",
    "            i+=1\n",
    "\n",
    "    np.savetxt(\"labcv.txt\",bigy)\n",
    "\n",
    "    X,X_Test= convert_to_woe(X,y , X_Test, seed=1, cvals=5, roundings=2)\n",
    "    temp_array_train=X\n",
    "    temp_array_cv=X_Test\n",
    "\n",
    "    temp_array_train=np.column_stack((temp_array_train,X_count))\n",
    "    temp_array_cv=np.column_stack((temp_array_cv,X_Test_count))\n",
    "\n",
    "\n",
    "    for a in range(0,temp_array_train.shape[0]):\n",
    "        for b in range(0,temp_array_train.shape[1]):\n",
    "            if temp_array_train[a,b]>0:\n",
    "                temp_array_train[a,b]=np.log1p(temp_array_train[a,b])\n",
    "            else :\n",
    "                temp_array_train[a,b]=-np.log1p(-temp_array_train[a,b])\n",
    "\n",
    "    for a in range(0,temp_array_cv.shape[0]):\n",
    "        for b in range(0,temp_array_cv.shape[1]):\n",
    "            if temp_array_cv[a,b]>0:\n",
    "                temp_array_cv[a,b]=np.log1p(temp_array_cv[a,b])\n",
    "            else :\n",
    "                temp_array_cv[a,b]= -np.log1p(-temp_array_cv[a,b])\n",
    "\n",
    "    stda=StandardScaler()\n",
    "    stda.fit(temp_array_train)\n",
    "    temp_array_train=stda.transform(temp_array_train)\n",
    "    temp_array_cv=stda.transform(temp_array_cv)\n",
    "\n",
    "    #temp_array_train=csr_matrix(temp_array_train)\n",
    "    #temp_array_cv=csr_matrix(temp_array_cv)\n",
    "\n",
    "    np.savetxt (\"amazon_counts_train.txt\",np.column_stack((y,temp_array_train)),delimiter=\",\")\n",
    "    np.savetxt (\"amazon_counts_test.txt\",temp_array_cv,delimiter=\",\")\n",
    "\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "def create_interractions(train, test, path=\"\"):\n",
    "\n",
    "    train_df=pd.read_pickle(path + train)\n",
    "    test_df=pd.read_pickle(path + test)\n",
    "#    train_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "\n",
    "    y=np.array(train_df['target'])\n",
    "#    train_df.drop(\"target\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    cat_columns = list(train_df.select_dtypes(include=['category', 'bool']).columns)\n",
    "    cat_columns.remove('target')\n",
    "\n",
    "    cat_columns = [cat_columns[k] for k in range(0,len(cat_columns))] # ??\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "                name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]\n",
    "\n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x))\n",
    "\n",
    "                train_df[name1] = train_df[name1].astype('category')\n",
    "                test_df[name1] = test_df[name1].astype('category')\n",
    "                test_df[name1] = test_df[name1].cat.set_categories(train_df[name1].categories)\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):\n",
    "                name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]\n",
    "\n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x))\n",
    "\n",
    "                train_df[name1] = train_df[name1].astype('category')\n",
    "                test_df[name1] = test_df[name1].astype('category')\n",
    "                test_df[name1] = test_df[name1].cat.set_categories(train_df[name1].categories)\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):\n",
    "                for j4 in range(j3+1,len(cat_columns)):\n",
    "                     name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]+ \"_plus_\" + cat_columns[j4]\n",
    "\n",
    "                     train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j4]].apply(lambda x:str(x))\n",
    "                     test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j4]].apply(lambda x:str(x))\n",
    "\n",
    "                     train_df[name1] = train_df[name1].astype('category')\n",
    "                     test_df[name1] = test_df[name1].astype('category')\n",
    "                     test_df[name1] = test_df[name1].cat.set_categories(train_df[name1].categories)\n",
    "    #\n",
    "    # for j1 in range(0,len(cat_columns)):\n",
    "    #     for j2 in range(j1+1,len(cat_columns)):\n",
    "    #         for j3 in range(j2+1,len(cat_columns)):\n",
    "    #             for j4 in range(j3+1,len(cat_columns)):\n",
    "    #                 for j5 in range(j4+1,len(cat_columns)):\n",
    "    #                     name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]+ \"_plus_\" + cat_columns[j4]+ \"_plus_\" + cat_columns[j5]\n",
    "    #                     cols.append(name1)\n",
    "    #\n",
    "    #                     train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j4]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j5]].apply(lambda x:str(x))\n",
    "    #                     test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j4]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j5]].apply(lambda x:str(x))\n",
    "    #                     cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "    #\n",
    "    #                     lbl = LabelEncoder()\n",
    "    #                     lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    #                     train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    #                     test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "    #\n",
    "    #                     mean_auc=0\n",
    "    #                     X=np.array(train_df)\n",
    "    #                     i=0 # iterator counter\n",
    "    #                     for train_index, test_index in kfolder:\n",
    "    #                             X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    #                             y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    #                             one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "    #                             one.fit(X_train)\n",
    "    #                             X_train=one.transform(X_train)\n",
    "    #                             X_cv=one.transform(X_cv)\n",
    "    #                             model.fit(X_train,y_train)\n",
    "    #                             preds=model.predict_proba(X_cv)[:,1]\n",
    "    #                             auc=roc_auc_score(y_cv,preds)\n",
    "    #                             print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "    #                             mean_auc+=auc\n",
    "    #                             i+=1\n",
    "    #                     mean_auc/=5\n",
    "    #                     if (mean_auc>grand_auc+0.00001):\n",
    "    #                         print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "    #                         grand_auc=mean_auc\n",
    "    #                         newcols.append(name1)\n",
    "    #                     else :\n",
    "    #                         print( \"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "    #                         train_df.drop(name1, inplace=True,axis=1)\n",
    "    #                         test_df.drop(name1, inplace=True,axis=1)\n",
    "    #                         cat_columns_idx = cat_columns_idx[:-1]\n",
    "\n",
    "    #train_df.to_csv(\"trainid.csv\",index=False)\n",
    "    #test_df.to_csv(\"testid.csv\",index=False)\n",
    "    train_df.to_pickle(\"interactions \" + train)\n",
    "    test_df.to_pickle(\"interactions \" + test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Series cannot perform the operation +",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1fa214fad816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m############ code runs here ############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcreate_interractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_preproc.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_preproc.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/preproc/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute 4way interractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcreate_interractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_preproc_rare.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_preproc_rare.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/preproc/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create_likelihoods_with_counts()  # compute likelihoods and counts per fold and print 5 pairs of train/cv files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d1d560fcb013>\u001b[0m in \u001b[0;36mcreate_interractions\u001b[0;34m(train, test, path)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mname1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_plus_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcat_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m                 \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             raise TypeError(\"{typ} cannot perform the operation \"\n\u001b[0;32m-> 1062\u001b[0;31m                             \"{op}\".format(typ=type(left).__name__, op=str_rep))\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Series cannot perform the operation +"
     ]
    }
   ],
   "source": [
    "\n",
    "############ code runs here ############\n",
    "create_interractions(\"train_preproc.pickle\", \"test_preproc.pickle\", path='../data/preproc/') # compute 4way interractions\n",
    "create_interractions(\"train_preproc_rare.pickle\", \"train_preproc_rare.pickle\", path='../data/preproc/')\n",
    "# create_likelihoods_with_counts()  # compute likelihoods and counts per fold and print 5 pairs of train/cv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
