{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WM1158/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created 22/4/2017\n",
    "@author: Marios Michailidis\n",
    "Script that prepares 2 data sets to get a top 11 position in the Amazon Classification Challenge:\n",
    "Link:https://www.kaggle.com/c/amazon-employee-access-challenge\n",
    "First source of data is via selecting the best up for up to 4-way interractions fo all\n",
    "categorical variables ysing a linear model. Then the results are printes as sparse files\n",
    "The scource are counts and likelihood features created per fold for up to 3way interractions \n",
    "( no feature selection ) .This produces 5 pairs of train/cv files that will be used to do \n",
    "the stacking semi-manually . E.g. in the first level no kfolding will take place.       \n",
    "\"\"\"\n",
    "\n",
    "#amazon helper functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csr_matrix,csc_matrix\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts a dataset to weights of evidence (actuall computations) :\n",
    "Good explanation here :http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/\n",
    "These are likelihood type of features\n",
    "\"\"\"    \n",
    "    \n",
    "def convert_dataset_to_woe(xc,yc,xt, rounding=2,cols=None):\n",
    "    xc=xc.tolist()\n",
    "    xt=xt.tolist()\n",
    "    yc=yc.tolist()\n",
    "    if cols==None:\n",
    "        cols=[k for k in range(0,len(xc[0]))]\n",
    "    woe=[ [0.0 for k in range(0,len(cols))] for g in range(0,len(xt))]\n",
    "    good=[]\n",
    "    bads=[]\n",
    "    for col in cols:\n",
    "        dictsgoouds=defaultdict(int)        \n",
    "        dictsbads=defaultdict(int)\n",
    "        good.append(dictsgoouds)\n",
    "        bads.append(dictsbads)        \n",
    "    total_goods=0\n",
    "    total_bads =0\n",
    "\n",
    "    for a in range (0,len(xc)):\n",
    "        target=yc[a]\n",
    "        if target>0.0:\n",
    "            total_goods+=1.0\n",
    "        else :\n",
    "            total_bads+=1.0\n",
    "        for j in range(0,len(cols)):\n",
    "            col=cols[j]\n",
    "            if target>0:\n",
    "                good[j][xc[a][col]]+=1.0\n",
    "            else :\n",
    "                bads[j][xc[a][col]]+=1.0  \n",
    "    #print(total_goods,total_bads)            \n",
    "    \n",
    "    for a in range (0,len(xt)):    \n",
    "        for j in range(0,len(cols)):\n",
    "            col=cols[j]\n",
    "            tempgood=0.0\n",
    "            tempbad=0.0\n",
    "            if xt[a][col] in good[j]:\n",
    "                tempgood=float(good[j][xt[a][col]])\n",
    "            if xt[a][col] in bads[j]:\n",
    "                tempbad=float(bads[j][xt[a][col]])  \n",
    "            if tempgood>0.0 and tempbad>0.0:\n",
    "                #print(tempgood,tempbad)\n",
    "                woe[a][j]=round(np.log((tempgood/total_goods) /(tempbad/total_bads)) , rounding)\n",
    "            elif tempgood>0.0 :\n",
    "                 woe[a][j]=3.0\n",
    "            elif tempbad>0.0:\n",
    "                woe[a][j]=-3.0\n",
    "            else :\n",
    "                 woe[a][j]=round(np.log(0.9421/0.0579))\n",
    "    return woe            \n",
    "    \n",
    "\"\"\"\n",
    "Converts a dataset to weights of evidence (general):\n",
    "Good explanation here :http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/\n",
    "These are likelihood type of features\n",
    "\"\"\"  \n",
    "def convert_to_woe(X,y, Xt, seed=1, cvals=5, roundings=2, columns=None):\n",
    "    \n",
    "    if columns==None:\n",
    "        columns=[k for k in range(0,(X.shape[1]))]    \n",
    "        \n",
    "    X=X.tolist()\n",
    "    Xt=Xt.tolist() \n",
    "    woetrain=[ [0.0 for k in range(0,len(X[0]))] for g in range(0,len(X))]\n",
    "    woetest=[ [0.0 for k in range(0,len(X[0]))] for g in range(0,len(Xt))]    \n",
    "    \n",
    "    kfolder=StratifiedKFold(y, n_folds=cvals,shuffle=True, random_state=seed)\n",
    "    for train_index, test_index in kfolder:\n",
    "        # creaning and validation sets\n",
    "        X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train = np.array(y)[train_index]\n",
    "\n",
    "        woecv=convert_dataset_to_woe(X_train,y_train,X_cv, rounding=roundings,cols=columns)\n",
    "        X_cv=X_cv.tolist()\n",
    "        no=0\n",
    "        for real_index in test_index:\n",
    "            for j in range(0,len(X_cv[0])):\n",
    "                woetrain[real_index][j]=X_cv[no][j]\n",
    "            no+=1\n",
    "        no=0\n",
    "        for real_index in test_index:\n",
    "            for j in range(0,len(columns)):\n",
    "                col=columns[j]\n",
    "                woetrain[real_index][col]=woecv[no][j]\n",
    "            no+=1      \n",
    "    woefinal=convert_dataset_to_woe(np.array(X),np.array(y),np.array(Xt), rounding=roundings,cols=columns) \n",
    "\n",
    "    for real_index in range(0,len(Xt)):\n",
    "        for j in range(0,len(Xt[0])):           \n",
    "            woetest[real_index][j]=Xt[real_index][j]\n",
    "            \n",
    "    for real_index in range(0,len(Xt)):\n",
    "        for j in range(0,len(columns)):\n",
    "            col=columns[j]\n",
    "            woetest[real_index][col]=woefinal[real_index][j]\n",
    "            \n",
    "    return np.array(woetrain), np.array(woetest)\n",
    "\n",
    "\"\"\"\n",
    "converts sparse data to StackNet format\n",
    "Better use this one than standard svmlight.\n",
    "\"\"\"\n",
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n",
    "    zsparse=csr_matrix(csc_matrix(array))\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))\n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "    \n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget is not None:\n",
    "             f.write(str(ytarget[b]) + deli1)     \n",
    "             \n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:    \n",
    "            print(\" row : %d \" % (counter_row))    \n",
    "    f.close()  \n",
    "    \n",
    "\"\"\"\n",
    "Load training and test data. Then create in a brute force way to cerate all possible 4-way \n",
    "categorical interractions and test whether auc improves when adding them. \n",
    "Once it finds the best interractions, it prints them as sparse data\n",
    "as:\n",
    "    train.sparse\n",
    "    test.sparse\n",
    "\"\"\"\n",
    "  \n",
    "def create_4way_interractions(path=\"\"):\n",
    "    \n",
    "    train_df=pd.read_pickle(path + \"train_preproc_1_.pickle\")\n",
    "    test_df=pd.read_pickle(path + \"test_preproc_1_.pickle\")\n",
    "#    train_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "    \n",
    "    y=np.array(train_df['target'])\n",
    "    train_df.drop(\"target\", axis=1, inplace=True)\n",
    "#    test_df.drop(\"id\", axis=1, inplace=True)  \n",
    "    \n",
    "    \n",
    "    cat_columns = train_df.select_dtypes(include=['int8', 'int16', 'int32']).columns\n",
    "    cat_columns_idx = [train_df.columns.get_loc(cat) for cat in cat_columns]\n",
    "    cat_columns = [cat_columns[k] for k in range(0,len(cat_columns))] # ??\n",
    "    \n",
    "    kfolder = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=1) \n",
    "    \n",
    "    grand_auc = 0\n",
    "    \n",
    "    X = np.array(train_df)\n",
    "    i=0 # iterator counter\n",
    "    model = SGDClassifier(loss='log', penalty='l2', alpha=0.0000225, n_iter=50, random_state=1)\n",
    "    \n",
    "    for train_index, test_index in kfolder:    \n",
    "            X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "            y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]     \n",
    "            one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "            one.fit(X_train)\n",
    "            X_train=one.transform(X_train)\n",
    "            X_cv=one.transform(X_cv)\n",
    "            model.fit(X_train,y_train)\n",
    "            preds=model.predict_proba(X_cv)[:,1]\n",
    "            auc=roc_auc_score(y_cv,preds)\n",
    "            print (\" fold %d/%d auc %f \" % (i+1, 5, auc))\n",
    "            grand_auc+=auc\n",
    "            i+=1\n",
    "    grand_auc/=5\n",
    "    print (\"grand AUC is %f \" % (grand_auc))\n",
    "    \n",
    "    cat_columns = train_df.select_dtypes(include=['category', 'object']).columns\n",
    "    cat_columns_idx = [train_df.columns.get_loc(cat) for cat in cat_columns]\n",
    "    cat_columns = [cat_columns[k] for k in range(0, len(cat_columns))] # ??\n",
    "    \n",
    "    cols=[k for k in cat_columns] #??\n",
    "    newcols=cols[:]\n",
    "    print(cols)\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "                name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]\n",
    "                cols.append(name1)\n",
    "\n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) \n",
    "                cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "                \n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))                \n",
    "                \n",
    "                mean_auc=0\n",
    "                X=np.array(train_df)\n",
    "                i=0 # iterator counter    \n",
    "                for train_index, test_index in kfolder:    \n",
    "                        X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "                        y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]     \n",
    "                        one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "                        one.fit(X_train)\n",
    "                        X_train=one.transform(X_train)\n",
    "                        X_cv=one.transform(X_cv)\n",
    "                        model.fit(X_train, y_train)\n",
    "                        preds=model.predict_proba(X_cv)[:, 1]\n",
    "                        auc=roc_auc_score(y_cv,preds)\n",
    "                        print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "                        mean_auc+=auc\n",
    "                        i+=1\n",
    "                mean_auc/=5  \n",
    "                if (mean_auc > grand_auc + 0.00001):\n",
    "                    print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "                    grand_auc=mean_auc\n",
    "                    newcols.append(name1)\n",
    "                else :\n",
    "                    print( \"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "                    train_df.drop(name1, inplace=True,axis=1) \n",
    "                    test_df.drop(name1, inplace=True,axis=1)\n",
    "                    cat_columns_idx = cat_columns_idx[:-1]\n",
    "                   \n",
    "                \n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):            \n",
    "                name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]\n",
    "                cols.append(name1)\n",
    "                \n",
    "                train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x)) \n",
    "                cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "                \n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))                       \n",
    "                \n",
    "                mean_auc=0\n",
    "                X=np.array(train_df)\n",
    "                i=0 # iterator counter    \n",
    "                for train_index, test_index in kfolder:    \n",
    "                        X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "                        y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]     \n",
    "                        one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "                        one.fit(X_train)\n",
    "                        X_train=one.transform(X_train)\n",
    "                        X_cv=one.transform(X_cv) \n",
    "                        model.fit(X_train,y_train)\n",
    "                        preds=model.predict_proba(X_cv)[:,1]\n",
    "                        auc=roc_auc_score(y_cv,preds)\n",
    "                        print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "                        mean_auc+=auc\n",
    "                        i+=1\n",
    "                mean_auc/=5  \n",
    "                if (mean_auc>grand_auc+0.00001):\n",
    "                    print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "                    grand_auc=mean_auc\n",
    "                    newcols.append(name1)\n",
    "                else :\n",
    "                    print(\"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "                    train_df.drop(name1, inplace=True,axis=1) \n",
    "                    test_df.drop(name1, inplace=True,axis=1) \n",
    "                    cat_columns_idx = cat_columns_idx[:-1]\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):            \n",
    "                for j4 in range(j3+1,len(cat_columns)):                \n",
    "                    name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]+ \"_plus_\" + cat_columns[j4]\n",
    "                    cols.append(name1)\n",
    "\n",
    "                    train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j4]].apply(lambda x:str(x))\n",
    "                    test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j4]].apply(lambda x:str(x)) \n",
    "                    cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "                    \n",
    "                    lbl = LabelEncoder()\n",
    "                    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                    test_df[name1] = lbl.transform(list(test_df[name1].values))                \n",
    "                    \n",
    "                    mean_auc=0\n",
    "                    X=np.array(train_df)\n",
    "                    i=0 # iterator counter    \n",
    "                    for train_index, test_index in kfolder:    \n",
    "                            X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "                            y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]     \n",
    "                            one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "                            one.fit(X_train)\n",
    "                            X_train=one.transform(X_train)\n",
    "                            X_cv=one.transform(X_cv) \n",
    "                            model.fit(X_train,y_train)\n",
    "                            preds=model.predict_proba(X_cv)[:,1]\n",
    "                            auc=roc_auc_score(y_cv,preds)\n",
    "                            print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "                            mean_auc+=auc\n",
    "                            i+=1\n",
    "                    mean_auc/=5  \n",
    "                    if (mean_auc>grand_auc+0.00001):\n",
    "                        print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "                        grand_auc=mean_auc\n",
    "                        newcols.append(name1)\n",
    "                    else :\n",
    "                        print( \"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "                        train_df.drop(name1, inplace=True,axis=1) \n",
    "                        test_df.drop(name1, inplace=True,axis=1)\n",
    "                        cat_columns_idx = cat_columns_idx[:-1]\n",
    "\n",
    "    for j1 in range(0,len(cat_columns)):\n",
    "        for j2 in range(j1+1,len(cat_columns)):\n",
    "            for j3 in range(j2+1,len(cat_columns)):            \n",
    "                for j4 in range(j3+1,len(cat_columns)):     \n",
    "                    for j5 in range(j4+1,len(cat_columns)):                       \n",
    "                        name1=cat_columns[j1] + \"_plus_\" + cat_columns[j2]+ \"_plus_\" + cat_columns[j3]+ \"_plus_\" + cat_columns[j4]+ \"_plus_\" + cat_columns[j5]\n",
    "                        cols.append(name1)\n",
    "                        \n",
    "                        train_df[name1]=train_df[cat_columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[cat_columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j4]].apply(lambda x:str(x))+ \"_\" + train_df[cat_columns[j5]].apply(lambda x:str(x))\n",
    "                        test_df[name1]=test_df[cat_columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[cat_columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j3]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j4]].apply(lambda x:str(x)) + \"_\" + test_df[cat_columns[j5]].apply(lambda x:str(x))\n",
    "                        cat_columns_idx.append(train_df.columns.get_loc(name1))\n",
    "                        \n",
    "                        lbl = LabelEncoder()\n",
    "                        lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                        train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                        test_df[name1] = lbl.transform(list(test_df[name1].values))                             \n",
    "                        \n",
    "                        mean_auc=0\n",
    "                        X=np.array(train_df)\n",
    "                        i=0 # iterator counter    \n",
    "                        for train_index, test_index in kfolder:    \n",
    "                                X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "                                y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index]     \n",
    "                                one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx)\n",
    "                                one.fit(X_train)\n",
    "                                X_train=one.transform(X_train)\n",
    "                                X_cv=one.transform(X_cv) \n",
    "                                model.fit(X_train,y_train)\n",
    "                                preds=model.predict_proba(X_cv)[:,1]\n",
    "                                auc=roc_auc_score(y_cv,preds)\n",
    "                                print (\" %s fold %d/%d auc %f \" % (name1,i+1,5,auc))\n",
    "                                mean_auc+=auc\n",
    "                                i+=1\n",
    "                        mean_auc/=5  \n",
    "                        if (mean_auc>grand_auc+0.00001):\n",
    "                            print (\" %s will remain fold new Auc %f versus old Auc %f \" % (name1,mean_auc,grand_auc))\n",
    "                            grand_auc=mean_auc\n",
    "                            newcols.append(name1)\n",
    "                        else :\n",
    "                            print( \"dropping %s as %f is NOT big enough to %f \" %  (name1,mean_auc,grand_auc))\n",
    "                            train_df.drop(name1, inplace=True,axis=1) \n",
    "                            test_df.drop(name1, inplace=True,axis=1) \n",
    "                            cat_columns_idx = cat_columns_idx[:-1]\n",
    "                       \n",
    "    #train_df.to_csv(\"trainid.csv\",index=False)\n",
    "    #test_df.to_csv(\"testid.csv\",index=False) \n",
    "    train_df.to_pickle(\"trainid.pickle\", index=False)\n",
    "    test_df.to_pickle(\"testid.pickle\", index=False) \n",
    "      \n",
    "    print (\"one hot encoding\")\n",
    "    train=np.array(train_df)\n",
    "    test=np.array(test_df) \n",
    "    \n",
    "    for j in range(0,train.shape[1]):\n",
    "        dicter=defaultdict(lambda:0)\n",
    "        for i in range(0,train.shape[0]):\n",
    "            dicter[str(train[i,j])]+=1 \n",
    "        for i in range(0,test.shape[0]):\n",
    "            dicter[str(test[i,j])]+=1 \n",
    "        for i in range(0,train.shape[0]):\n",
    "            train[i,j]=9999999 if dicter[str(train[i,j])]<=1 else  train[i,j]\n",
    "        for i in range(0,test.shape[0]):\n",
    "            test[i,j]=9999999 if dicter[str(test[i,j])]<=1 else  test[i,j]   \n",
    "          \n",
    "    one=OneHotEncoder(handle_unknown='ignore', categorical_features=cat_columns_idx, sparse=True)\n",
    "    test=one.fit_transform(test)\n",
    "    train=one.transform(train)   \n",
    "    test=csr_matrix(test)\n",
    "    train=csr_matrix(train)\n",
    "    fromsparsetofile(path + \"train.sparse\", train, deli1=\" \", deli2=\":\",ytarget=y)    \n",
    "    fromsparsetofile(path + \"test.sparse\", test, deli1=\" \", deli2=\":\",ytarget=None)      \n",
    "    print (train.shape)\n",
    "    print (test.shape)\n",
    "    \n",
    "  \n",
    "    print (\"counts\")   \n",
    "    result = pd.concat([train_df,test_df])    \n",
    "    for f in newcols:\n",
    "                cases=defaultdict(int)\n",
    "                temp=np.array(result[f]).tolist()\n",
    "                for k in temp:\n",
    "                    cases[k]+=1\n",
    "                print (f, len(cases)) \n",
    "                \n",
    "                train_df[f]=train_df[f].apply(lambda x: cases[x])\n",
    "                test_df[f]=test_df[f].apply(lambda x: cases[x])     \n",
    "    \n",
    "    train_df.to_pickle(\"traincount.pickle\",index=False)\n",
    "    test_df.to_pickle(\"testcount.pickle\",index=False)  \n",
    "       \n",
    "\n",
    "\"\"\"\n",
    "Computes all possible 3-way interractions\n",
    "and finds the counts of each category\n",
    "Then it perfoms k-fold and produces likelihood (woe)\n",
    "values for all features and stacks them next to the counts. Then\n",
    "it prints them in dense format as  :\n",
    "    amazon_counts_train\" + str(fold_number) + \".txt\"\n",
    "    amazon_counts_cv\" + str(fold_number) + \".txt\"\n",
    "It also produces an amazon_counts_train.txt and amazon_counts_test.txt file too.\n",
    "(so 12 in total - 5 pairs of train/cv and a final train and test file)\n",
    "The aim is to prepare StackNet to run stacking with our own folds.\n",
    "The data is also standardized. \n",
    "\"\"\"\n",
    "\n",
    "def create_likelihoods_with_counts(path=\"\"):\n",
    "    \n",
    "    number_of_folds=5\n",
    "    SEED=15\n",
    "    train_df=pd.read_csv(path + \"train.csv\")\n",
    "    test_df=pd.read_csv(path + \"test.csv\")\n",
    "    train_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "    test_df.drop(\"ROLE_CODE\", axis=1, inplace=True)\n",
    "    \n",
    "    y=np.array(train_df['ACTION'])\n",
    "    \n",
    "    train_df.drop(\"ACTION\", axis=1, inplace=True)\n",
    "    test_df.drop(\"id\", axis=1, inplace=True)  \n",
    "    \n",
    "    columns=train_df.columns.values\n",
    "    columns=[columns[k] for k in range(0,len(columns))] # we exclude the first column\n",
    "    cols=[k for k in columns]\n",
    "    print(cols)\n",
    "    for j1 in range(0,len(columns)):\n",
    "        for j2 in range(j1+1,len(columns)):\n",
    "                name1=columns[j1] + \"_plus_\" + columns[j2]\n",
    "                cols.append(name1)\n",
    "                train_df[name1]=train_df[columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[columns[j2]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[columns[j2]].apply(lambda x:str(x)) \n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))                     \n",
    "                \n",
    "\n",
    "                \n",
    "    for j1 in range(0,len(columns)):\n",
    "        for j2 in range(j1+1,len(columns)):\n",
    "            for j3 in range(j2+1,len(columns)):            \n",
    "                name1=columns[j1] + \"_plus_\" + columns[j2]+ \"_plus_\" + columns[j3]\n",
    "                cols.append(name1)\n",
    "\n",
    "                train_df[name1]=train_df[columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[columns[j3]].apply(lambda x:str(x))\n",
    "                test_df[name1]=test_df[columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[columns[j3]].apply(lambda x:str(x))\n",
    "                lbl = LabelEncoder()\n",
    "                lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                test_df[name1] = lbl.transform(list(test_df[name1].values))     \n",
    "    \n",
    " \n",
    "    for j1 in range(0,len(columns)):\n",
    "        for j2 in range(j1+1,len(columns)):\n",
    "            for j3 in range(j2+1,len(columns)):           \n",
    "                for j4 in range(j3+1,len(columns)):                   \n",
    "                    name1=columns[j1] + \"_plus_\" + columns[j2]+ \"_plus_\" + columns[j3]+ \"_plus_\" + columns[j4]\n",
    "                    cols.append(name1)\n",
    "    \n",
    "                    train_df[name1]=train_df[columns[j1]].apply(lambda x:str(x)) + \"_\" + train_df[columns[j2]].apply(lambda x:str(x))+ \"_\" + train_df[columns[j3]].apply(lambda x:str(x))+ \"_\" + train_df[columns[j4]].apply(lambda x:str(x))\n",
    "                    test_df[name1]=test_df[columns[j1]].apply(lambda x:str(x))+ \"_\" + test_df[columns[j2]].apply(lambda x:str(x)) + \"_\" + test_df[columns[j3]].apply(lambda x:str(x))+ \"_\" + test_df[columns[j4]].apply(lambda x:str(x))\n",
    "                    lbl = LabelEncoder()\n",
    "                    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "                    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "                    test_df[name1] = lbl.transform(list(test_df[name1].values))    \n",
    "                \n",
    "    X=np.array(train_df)\n",
    "    X_Test=np.array(test_df)\n",
    "    \n",
    "    print (\"counts\")   \n",
    "    result = pd.concat([train_df,test_df])    \n",
    "    for f in cols:\n",
    "                cases=defaultdict(int)\n",
    "                temp=np.array(result[f]).tolist()\n",
    "                for k in temp:\n",
    "                    cases[k]+=1\n",
    "                print (f, len(cases)) \n",
    "                \n",
    "                train_df[f]=train_df[f].apply(lambda x: cases[x])\n",
    "                test_df[f]=test_df[f].apply(lambda x: cases[x])  \n",
    "                \n",
    "    X_count=np.array(train_df)\n",
    "    X_Test_count=np.array(test_df)  \n",
    "    \n",
    "    X_count[X_count<=1]=0\n",
    "    X_Test_count[X_Test_count<=1]=0  \n",
    "                \n",
    "               \n",
    "    bigy=None     \n",
    "    print(\" creating likelihoods \")\n",
    "    kfolder=StratifiedKFold(y, n_folds=number_of_folds,shuffle=True, random_state=SEED)\n",
    "    #number_of_folds=0\n",
    "    #X,y=shuffle(X,y, random_state=SEED) # Shuffle since the data is ordered by time\n",
    "    i=0 # iterator counter\n",
    "    print (\"printing files for %d kfolds \" % (number_of_folds))\n",
    "    if number_of_folds>0:\n",
    "        for train_index, test_index in kfolder:    \n",
    "            X_train, X_cv = np.array(X)[train_index], np.array(X)[test_index]\n",
    "            X_train_count, X_cv_count = np.array(X_count)[train_index], np.array(X_count)[test_index]            \n",
    "            y_train, y_cv = np.array(y)[train_index], np.array(y)[test_index] \n",
    "            \n",
    "            if bigy==None      :\n",
    "                bigy=y_cv\n",
    "            else :\n",
    "                bigy=np.concatenate((bigy,y_cv))\n",
    "                \n",
    "\n",
    "            X_train,X_cv= convert_to_woe(X_train,y_train , X_cv, seed=1, cvals=5, roundings=2)\n",
    "\n",
    "\n",
    "            \n",
    "            temp_array_train=X_train\n",
    "            temp_array_cv=X_cv    \n",
    "            temp_array_train=np.column_stack((temp_array_train,X_train_count))\n",
    "            temp_array_cv=np.column_stack((temp_array_cv,X_cv_count))\n",
    "            \n",
    "            for a in range(0,temp_array_train.shape[0]):\n",
    "                for b in range(0,temp_array_train.shape[1]):\n",
    "                    if temp_array_train[a,b]>0:\n",
    "                        temp_array_train[a,b]=np.log1p(temp_array_train[a,b])\n",
    "                    else :\n",
    "                        temp_array_train[a,b]=-np.log1p(-temp_array_train[a,b])                        \n",
    "                        \n",
    "            for a in range(0,temp_array_cv.shape[0]):\n",
    "                for b in range(0,temp_array_cv.shape[1]):\n",
    "                    if temp_array_cv[a,b]>0:\n",
    "                        temp_array_cv[a,b]=np.log1p(temp_array_cv[a,b])               \n",
    "                    else :\n",
    "                        temp_array_cv[a,b]= -np.log1p(-temp_array_cv[a,b])          \n",
    "             \n",
    "            stda=StandardScaler()\n",
    "            stda.fit(temp_array_train)\n",
    "            temp_array_train=stda.transform(temp_array_train)\n",
    "            temp_array_cv=stda.transform(temp_array_cv)             \n",
    "            \n",
    "            #temp_array_train=csr_matrix(temp_array_train)\n",
    "            #temp_array_cv=csr_matrix(temp_array_cv)      \n",
    "            np.savetxt (\"amazon_counts_train\" + str(i) + \".txt\",np.column_stack((y_train,temp_array_train)),delimiter=\",\")\n",
    "            np.savetxt (\"amazon_counts_cv\" + str(i) + \".txt\",np.column_stack((y_cv,temp_array_cv)),delimiter=\",\")      \n",
    "            \n",
    "            #fromsparsetofile(\"amazon_count_train\" + str(i) + \".txt\", temp_array_train, deli1=\" \", deli2=\":\",ytarget=y_train)\n",
    "            #fromsparsetofile(\"amazon_count_cv\" + str(i) + \".txt\", temp_array_cv,deli1=\" \", deli2=\":\",ytarget=y_cv)  \n",
    "           \n",
    "            i+=1              \n",
    "    \n",
    "    np.savetxt(\"labcv.txt\",bigy)\n",
    "  \n",
    "    X,X_Test= convert_to_woe(X,y , X_Test, seed=1, cvals=5, roundings=2)        \n",
    "    temp_array_train=X\n",
    "    temp_array_cv=X_Test\n",
    "    \n",
    "    temp_array_train=np.column_stack((temp_array_train,X_count))\n",
    "    temp_array_cv=np.column_stack((temp_array_cv,X_Test_count))\n",
    "    \n",
    "\n",
    "    for a in range(0,temp_array_train.shape[0]):\n",
    "        for b in range(0,temp_array_train.shape[1]):\n",
    "            if temp_array_train[a,b]>0:\n",
    "                temp_array_train[a,b]=np.log1p(temp_array_train[a,b])\n",
    "            else :\n",
    "                temp_array_train[a,b]=-np.log1p(-temp_array_train[a,b])                        \n",
    "                \n",
    "    for a in range(0,temp_array_cv.shape[0]):\n",
    "        for b in range(0,temp_array_cv.shape[1]):\n",
    "            if temp_array_cv[a,b]>0:\n",
    "                temp_array_cv[a,b]=np.log1p(temp_array_cv[a,b])               \n",
    "            else :\n",
    "                temp_array_cv[a,b]= -np.log1p(-temp_array_cv[a,b])          \n",
    "     \n",
    "    stda=StandardScaler()\n",
    "    stda.fit(temp_array_train)\n",
    "    temp_array_train=stda.transform(temp_array_train)\n",
    "    temp_array_cv=stda.transform(temp_array_cv) \n",
    "\n",
    "    #temp_array_train=csr_matrix(temp_array_train)\n",
    "    #temp_array_cv=csr_matrix(temp_array_cv)  \n",
    "\n",
    "    np.savetxt (\"amazon_counts_train.txt\",np.column_stack((y,temp_array_train)),delimiter=\",\")\n",
    "    np.savetxt (\"amazon_counts_test.txt\",temp_array_cv,delimiter=\",\")     \n",
    "    \n",
    "\n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WM1158/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/Users/WM1158/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fold 1/5 auc 0.500542 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WM1158/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fold 2/5 auc 0.585656 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WM1158/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fold 3/5 auc 0.570651 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WM1158/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fold 4/5 auc 0.497467 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WM1158/anaconda2/envs/dsc_hs/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fold 5/5 auc 0.500793 \n",
      "grand AUC is 0.531022 \n",
      "[]\n",
      "one hot encoding\n",
      " data lenth 47759863\n",
      " indices lenth 47759863\n",
      " indptr lenth 1048031\n",
      " row : 10000 \n",
      " row : 20000 \n",
      " row : 30000 \n",
      " row : 40000 \n",
      " row : 50000 \n",
      " row : 60000 \n",
      " row : 70000 \n",
      " row : 80000 \n",
      " row : 90000 \n",
      " row : 100000 \n",
      " row : 110000 \n",
      " row : 120000 \n",
      " row : 130000 \n",
      " row : 140000 \n",
      " row : 150000 \n",
      " row : 160000 \n",
      " row : 170000 \n",
      " row : 180000 \n",
      " row : 190000 \n",
      " row : 200000 \n",
      " row : 210000 \n",
      " row : 220000 \n",
      " row : 230000 \n",
      " row : 240000 \n",
      " row : 250000 \n",
      " row : 260000 \n",
      " row : 270000 \n",
      " row : 280000 \n",
      " row : 290000 \n",
      " row : 300000 \n",
      " row : 310000 \n",
      " row : 320000 \n",
      " row : 330000 \n",
      " row : 340000 \n",
      " row : 350000 \n",
      " row : 360000 \n",
      " row : 370000 \n",
      " row : 380000 \n",
      " row : 390000 \n",
      " row : 400000 \n",
      " row : 410000 \n",
      " row : 420000 \n",
      " row : 430000 \n",
      " row : 440000 \n",
      " row : 450000 \n",
      " row : 460000 \n",
      " row : 470000 \n",
      " row : 480000 \n",
      " row : 490000 \n",
      " row : 500000 \n",
      " row : 510000 \n",
      " row : 520000 \n",
      " row : 530000 \n",
      " row : 540000 \n",
      " row : 550000 \n",
      " row : 560000 \n",
      " row : 570000 \n",
      " row : 580000 \n",
      " row : 590000 \n",
      " row : 600000 \n",
      " row : 610000 \n",
      " row : 620000 \n",
      " row : 630000 \n",
      " row : 640000 \n",
      " row : 650000 \n",
      " row : 660000 \n",
      " row : 670000 \n",
      " row : 680000 \n",
      " row : 690000 \n",
      " row : 700000 \n",
      " row : 710000 \n",
      " row : 720000 \n",
      " row : 730000 \n",
      " row : 740000 \n",
      " row : 750000 \n",
      " row : 760000 \n",
      " row : 770000 \n",
      " row : 780000 \n",
      " row : 790000 \n",
      " row : 800000 \n",
      " row : 810000 \n",
      " row : 820000 \n",
      " row : 830000 \n",
      " row : 840000 \n",
      " row : 850000 \n",
      " row : 860000 \n",
      " row : 870000 \n",
      " row : 880000 \n",
      " row : 890000 \n",
      " row : 900000 \n",
      " row : 910000 \n",
      " row : 920000 \n",
      " row : 930000 \n",
      " row : 940000 \n",
      " row : 950000 \n",
      " row : 960000 \n",
      " row : 970000 \n",
      " row : 980000 \n",
      " row : 990000 \n",
      " row : 1000000 \n",
      " row : 1010000 \n",
      " row : 1020000 \n",
      " row : 1030000 \n",
      " row : 1040000 \n",
      " data lenth 10337963\n",
      " indices lenth 10337963\n",
      " indptr lenth 226771\n",
      " row : 10000 \n",
      " row : 20000 \n",
      " row : 30000 \n",
      " row : 40000 \n",
      " row : 50000 \n",
      " row : 60000 \n",
      " row : 70000 \n",
      " row : 80000 \n",
      " row : 90000 \n",
      " row : 100000 \n",
      " row : 110000 \n",
      " row : 120000 \n",
      " row : 130000 \n",
      " row : 140000 \n",
      " row : 150000 \n",
      " row : 160000 \n",
      " row : 170000 \n",
      " row : 180000 \n",
      " row : 190000 \n",
      " row : 200000 \n",
      " row : 210000 \n",
      " row : 220000 \n",
      "(1048030, 54)\n",
      "(226770, 54)\n",
      "counts\n"
     ]
    }
   ],
   "source": [
    "############ code runs here############\n",
    "create_4way_interractions() # compute 4way interractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_likelihoods_with_counts()  # compute likelihoods and counts per fold and print 5 pairs of train/cv files \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_pickle(\"train_preproc_1_.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['INCIDENT_TYPE_ID', 'TYPE_BI', 'MILLESIME', 'PROBLEM_CODE',\n",
       "       'AUTEUR_INCIDENT', 'ORIGINE_INCIDENT', 'COMMENTAIRE_BI', 'GRAVITE',\n",
       "       'RESOURCE_ID', 'target', 'TYPE_OCC', 'RACHAT_CODE', 'NATURE_CODE',\n",
       "       'MARQUE_LIB', 'MODELE_CODE', 'USAGE_LOCAL', 'PAYS', 'STOP_PHONING',\n",
       "       'CODE_GEN_EQUIPEMENT', 'CODE_FONCTION', 'CODE_ENERGIE',\n",
       "       'CODE_INSTALLATION', 'CODE_SPECIFICATION', 'CODE_EAU_CHAUDE',\n",
       "       'L1_ORGANISATION_ID', 'L2_ORGANISATION_ID', 'STS_CODE', 'FORMULE',\n",
       "       'OPTION', 'CONTRAT_TARIF', 'PRIX_FACTURE', 'nc_1', 'nc_2', 'nc_3',\n",
       "       'nc_4', 'nc_5', 'joursemaine_appel', 'joursemaine_intervention'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select_dtypes(include=['int8', 'int16', 'int32']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsc_hs]",
   "language": "python",
   "name": "conda-env-dsc_hs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
